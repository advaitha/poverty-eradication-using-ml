[
  {
    "objectID": "spatial_data_processing/visualization.html",
    "href": "spatial_data_processing/visualization.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Data Visualization using Folium\n\nimport folium\n\n\nfrom pyproj import crs\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n\nCreate a simple Interactive Map\n\nm = folium.Map(location=[20.59,78.96],zoom_start=5,control_scale=True)\n\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nChange the Style of the Map\n\nm = folium.Map(\n    location = [20.59,78.96],\n    tiles = \"Stamen Toner\",\n    zoom_start = 5,\n    control_scale = True,\n    prefer_canvas = True\n)\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nAdding layers to the Map - Pin location\n\n#18.5786832,73.7666697\nm = folium.Map(location=[20.59,78.96],\n                zoom_start=5,control_scale=True)\nfolium.Marker(\n    location = [18.5786832,73.7666697],\n    popup='Sai Eshnaya Apartments',\n    icon = folium.Icon(color='green',icon='ok-sign')\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nMark good resedential areas in Pune\n\npoints_fp = '../data/addresses.shp'\npoints = gpd.read_file(points_fp)\n\n\npoints.head()\n\n\n\n\n\n  \n    \n      \n      id\n      addr\n      geometry\n    \n  \n  \n    \n      0\n      1000\n      Boat Club Road, 411001, Pune, Maharastra\n      POINT (73.87826 18.53937)\n    \n    \n      1\n      1001\n      Koregaon, 415501, Pune, Maharastra\n      POINT (73.89299 18.53772)\n    \n    \n      2\n      1002\n      Kothrud, 411038, Pune, Maharastra\n      POINT (73.80767 18.50389)\n    \n    \n      3\n      1003\n      Balewadi, 411045, Pune, Maharastra\n      POINT (73.76912 18.57767)\n    \n    \n      4\n      1004\n      Baner, 411047, Pune, Maharastra\n      POINT (73.77686 18.56424)\n    \n  \n\n\n\n\n\npoints_gjson = folium.features.GeoJson(points, name='Good Residential Areas')\n\n\nm = folium.Map(location=[18.5786832,73.7666697], tiles=\"cartodbpositron\",\n                zoom_start=8,\n                control_scale=True)\npoints_gjson.add_to(m)\nfolium.LayerControl().add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate a Heatmap of the locations\n\npoints[\"x\"] = points[\"geometry\"].apply(lambda geom: geom.x)\npoints[\"y\"] = points[\"geometry\"].apply(lambda geom: geom.y)\n\n# Create a list of coordinate pairs\nlocations = list(zip(points[\"y\"], points[\"x\"]))\n\n\nfrom folium.plugins import HeatMap\n\n# Create a Map instance\nm = folium.Map(\n    location=[18.5786832,73.7666697], tiles=\"stamentoner\", zoom_start=10, control_scale=True\n)\n\n# Add heatmap to map instance\n# Available parameters: HeatMap(data, name=None, min_opacity=0.5, max_zoom=18, max_val=1.0, radius=25, blur=15, gradient=None, overlay=True, control=True, show=True)\nHeatMap(locations).add_to(m)\n\n# Alternative syntax:\n# m.add_child(HeatMap(points_array, radius=15))\n\n# Show map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate a Clustered point Map\n\nfrom folium.plugins import MarkerCluster\n\n\n# Create a Map instance\nm = folium.Map(\n    location=[18.5786832,73.7666697], tiles=\"cartodbpositron\", zoom_start=12, control_scale=True\n)\n\n\n# Get x and y coordinates for each point\npoints[\"x\"] = points[\"geometry\"].apply(lambda geom: geom.x)\npoints[\"y\"] = points[\"geometry\"].apply(lambda geom: geom.y)\n\n# Create a list of coordinate pairs\nlocations = list(zip(points[\"y\"], points[\"x\"]))\n\n\n# Create a folium marker cluster\nmarker_cluster = MarkerCluster(locations)\n\n# Add marker cluster to map\nmarker_cluster.add_to(m)\n\n# Show map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate a Choropleth Map\n\nimport geopandas as gpd\nfrom pyproj import CRS\nimport requests\nimport geojson\n\n# Specify the url for web feature service\nurl = \"https://kartta.hsy.fi/geoserver/wfs\"\n\n# Specify parameters (read data in json format).\n# Available feature types in this particular data source: http://geo.stat.fi/geoserver/vaestoruutu/wfs?service=wfs&version=2.0.0&request=describeFeatureType\nparams = dict(\n    service=\"WFS\",\n    version=\"2.0.0\",\n    request=\"GetFeature\",\n    typeName=\"asuminen_ja_maankaytto:Vaestotietoruudukko_2018\",\n    outputFormat=\"json\",\n)\n\n# Fetch data from WFS using requests\nr = requests.get(url, params=params)\n\n# Create GeoDataFrame from geojson\ndata = gpd.GeoDataFrame.from_features(geojson.loads(r.content))\n\n# Check the data\ndata.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      index\n      asukkaita\n      asvaljyys\n      ika0_9\n      ika10_19\n      ika20_29\n      ika30_39\n      ika40_49\n      ika50_59\n      ika60_69\n      ika70_79\n      ika_yli80\n    \n  \n  \n    \n      0\n      POLYGON ((25472499.995 6689749.005, 25472499.9...\n      688\n      9\n      28.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      1\n      POLYGON ((25472499.995 6685998.998, 25472499.9...\n      703\n      5\n      51.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      2\n      POLYGON ((25472499.995 6684249.004, 25472499.9...\n      710\n      8\n      44.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      3\n      POLYGON ((25472499.995 6683999.005, 25472499.9...\n      711\n      5\n      90.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      4\n      POLYGON ((25472499.995 6682998.998, 25472499.9...\n      715\n      11\n      41.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n  \n\n\n\n\n\nfrom pyproj import CRS\n\n# Define crs\ndata.crs = CRS.from_epsg(3879)\n\n\n# Re-project to WGS84\ndata = data.to_crs(epsg=4326)\n\n# Check layer crs definition\nprint(data.crs)\n\nEPSG:4326\n\n\n\n# Change the name of a column\ndata = data.rename(columns={\"asukkaita\": \"pop18\"})\n\n\ndata[\"geoid\"] = data.index.astype(str)\n\n\n# Select only needed columns\ndata = data[[\"geoid\", \"pop18\", \"geometry\"]]\n\n# Convert to geojson (not needed for the simple coropleth map!)\n# pop_json = data.to_json()\n\n# check data\ndata.head()\n\n\n\n\n\n  \n    \n      \n      geoid\n      pop18\n      geometry\n    \n  \n  \n    \n      0\n      0\n      9\n      POLYGON ((24.50236 60.31928, 24.50233 60.32152...\n    \n    \n      1\n      1\n      5\n      POLYGON ((24.50287 60.28562, 24.50284 60.28787...\n    \n    \n      2\n      2\n      8\n      POLYGON ((24.50311 60.26992, 24.50308 60.27216...\n    \n    \n      3\n      3\n      5\n      POLYGON ((24.50315 60.26767, 24.50311 60.26992...\n    \n    \n      4\n      4\n      11\n      POLYGON ((24.50328 60.25870, 24.50325 60.26094...\n    \n  \n\n\n\n\n\nm = folium.Map(\n    location=[60.25, 24.8], tiles=\"cartodbpositron\", zoom_start=10, control_scale=True\n)\n\n# Plot a choropleth map\n# Notice: 'geoid' column that we created earlier needs to be assigned always as the first column\nfolium.Choropleth(\n    geo_data=data,\n    name=\"Population in 2018\",\n    data=data,\n    columns=[\"geoid\", \"pop18\"],\n    key_on=\"feature.id\",\n    fill_color=\"YlOrRd\",\n    fill_opacity=0.7,\n    line_opacity=0.2,\n    line_color=\"white\",\n    line_weight=0,\n    highlight=False,\n    smooth_factor=1.0,\n    # threshold_scale=[100, 250, 500, 1000, 2000],\n    legend_name=\"Population in Helsinki\",\n).add_to(m)\n\n# Show map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate Choropleth Map with Interaction\n\n# Convert points to GeoJson\nfolium.features.GeoJson(\n    data,\n    name=\"Labels\",\n    style_function=lambda x: {\n        \"color\": \"transparent\",\n        \"fillColor\": \"transparent\",\n        \"weight\": 0,\n    },\n    tooltip=folium.features.GeoJsonTooltip(\n        fields=[\"pop18\"], aliases=[\"Population\"], labels=True, sticky=False\n    ),\n).add_to(m)\n\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "spatial_data_processing/plot_buildings_with_area.html",
    "href": "spatial_data_processing/plot_buildings_with_area.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Visualizing Buildings in a location along with its Area\n\nImport the required libraries\n\nimport osmnx as ox\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nimport keplergl\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\n\nPlot the map for Pune\n\npune = ox.geocode_to_gdf(\"Pune, India\")\npune.plot(edgecolor=\"0.2\")\nplt.title(\"Pune\")\n\nText(0.5, 1.0, 'Pune')\n\n\n\n\n\n\n\nPlot your location on the Map\n\nmy_location = pd.DataFrame(\n    {\"location\":[\"Baner\"],\n    \"Longitude\":[73.7747862],\n    \"Latitude\":[18.578686]}\n)\n\n\nmy_location\n\n\n\n\n\n  \n    \n      \n      location\n      Longitude\n      Latitude\n    \n  \n  \n    \n      0\n      Baner\n      73.774786\n      18.578686\n    \n  \n\n\n\n\n\nmy_location = gpd.GeoDataFrame(my_location,\n                    crs = \"EPSG:4326\",\n                    geometry=gpd.points_from_xy(my_location[\"Longitude\"],my_location[\"Latitude\"]))\n\n\nmy_location\n\n\n\n\n\n  \n    \n      \n      location\n      Longitude\n      Latitude\n      geometry\n    \n  \n  \n    \n      0\n      Baner\n      73.774786\n      18.578686\n      POINT (73.77479 18.57869)\n    \n  \n\n\n\n\n\nax = pune.plot(edgecolor=\"0.2\")\nmy_location.plot(ax=ax,markersize=60,edgecolor=\"0.2\",color='red')\nplt.title(\"My Location in Pune\")\n\nText(0.5, 1.0, 'My Location in Pune')\n\n\n\n\n\n\n\nGet the Bike Routes for your location\n\nbike_network = ox.graph_from_point(center_point=(18.5584546,73.7852182),dist=400,network_type='bike')\nbike_network\n\n<networkx.classes.multidigraph.MultiDiGraph at 0x7f3d9ae77f40>\n\n\n\nbike_network = (ox.graph_to_gdfs(bike_network, nodes=False)\n                  .reset_index(drop=True)\n                  .loc[:, [\"name\", \"length\", \"geometry\"]]\n               )\nbike_network\n\n\n\n\n\n  \n    \n      \n      name\n      length\n      geometry\n    \n  \n  \n    \n      0\n      Gopal Hari Deshmukh Marg\n      52.331\n      LINESTRING (73.78688 18.56143, 73.78638 18.56145)\n    \n    \n      1\n      NaN\n      127.050\n      LINESTRING (73.78688 18.56143, 73.78683 18.560...\n    \n    \n      2\n      Pancard Clubs Road\n      8.998\n      LINESTRING (73.78638 18.56153, 73.78638 18.56145)\n    \n    \n      3\n      Gopal Hari Deshmukh Marg\n      58.852\n      LINESTRING (73.78638 18.56153, 73.78689 18.561...\n    \n    \n      4\n      Pancard Clubs Road\n      8.998\n      LINESTRING (73.78638 18.56145, 73.78638 18.56153)\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      201\n      NaN\n      32.554\n      LINESTRING (73.78206 18.55943, 73.78175 18.55946)\n    \n    \n      202\n      NaN\n      47.738\n      LINESTRING (73.78206 18.55943, 73.78204 18.55986)\n    \n    \n      203\n      NaN\n      19.034\n      LINESTRING (73.78206 18.55943, 73.78224 18.55944)\n    \n    \n      204\n      NaN\n      32.554\n      LINESTRING (73.78175 18.55946, 73.78206 18.55943)\n    \n    \n      205\n      NaN\n      19.034\n      LINESTRING (73.78224 18.55944, 73.78206 18.55943)\n    \n  \n\n206 rows × 3 columns\n\n\n\n\nbike_network.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\ntotal_length = bike_network[\"length\"].sum()\nprint(f\"Total bike lane length: {total_length / 1000:.0f}km\")\n\nTotal bike lane length: 16km\n\n\n\n\nPlot Bike routes on the Map\n\nimport contextily as ctx\n\nax = (bike_network.to_crs(\"EPSG:3857\")\n         .plot(figsize=(10, 8), legend=True,\n               edgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\n     )\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)  # I'm using OSM as the source. See all provides with ctx.providers\nplt.axis(\"off\")\nplt.title(\"Baner\")\n\nText(0.5, 1.0, 'Baner')\n\n\n\n\n\n\n\nGet the building details in your area\n\ntags = {'building':True}\n\n\nbaner_buildings = ox.geometries_from_point(center_point=(18.5584546,73.7852182),dist=400,tags=tags)\n\n\nbaner_buildings = baner_buildings.assign(label='Building Footprints').reset_index()\n\n\n(baner_buildings.head(10).T)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      element_type\n      node\n      way\n      way\n      way\n      way\n      way\n      way\n      way\n      way\n      way\n    \n    \n      osmid\n      1432130601\n      264286363\n      359568513\n      359568520\n      359568545\n      359568551\n      359568562\n      359684077\n      359684097\n      359684099\n    \n    \n      building\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n    \n    \n      name\n      UBICS\n      Baneshwar Temple\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      geometry\n      POINT (73.7860015 18.5612441)\n      POLYGON ((73.7869388 18.5589314, 73.7869469 18...\n      POLYGON ((73.7889321 18.5605767, 73.7890453 18...\n      POLYGON ((73.7876847 18.56149, 73.7877554 18.5...\n      POLYGON ((73.78786 18.5613415, 73.7880387 18.5...\n      POLYGON ((73.7881551 18.559907, 73.7882733 18....\n      POLYGON ((73.7828324 18.5618882, 73.782974 18....\n      POLYGON ((73.7813494 18.5612504, 73.7814597 18...\n      POLYGON ((73.781994 18.5613356, 73.7821481 18....\n      POLYGON ((73.7814951 18.5606079, 73.7817434 18...\n    \n    \n      nodes\n      NaN\n      [2699768401, 2699768402, 2699768403, 269976840...\n      [3642483644, 3642483643, 3642483641, 364248364...\n      [3642483654, 3642483653, 3642483648, 364248365...\n      [3642483649, 3642483647, 3642483645, 364248364...\n      [3642483640, 3642483639, 3642483637, 364248363...\n      [3642483660, 3642483659, 3642483657, 364248365...\n      [3643589874, 3643589877, 3643589867, 364358986...\n      [3643589882, 3643589883, 3643589873, 364358987...\n      [3643589826, 3643589830, 3643589824, 364358981...\n    \n    \n      amenity\n      NaN\n      place_of_worship\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      religion\n      NaN\n      hindu\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      addr:city\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      addr:postcode\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      addr:street\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ways\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      type\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      label\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n    \n  \n\n\n\n\n\nbaner_buildings.name.fillna(value='not_known',inplace=True)\n\n\nbaner_buildings.shape\n\n(255, 14)\n\n\n\n\nVisualize the buildings on the map\n\nax = (baner_buildings.to_crs(\"EPSG:3857\")\n         .plot(figsize=(10, 12),column=\"name\",legend=True,\n               edgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\n     )\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)  # I'm using OSM as the source. See all provides with ctx.providers\nplt.axis(\"off\")\nplt.title(\"Baner\")\n\nText(0.5, 1.0, 'Baner')\n\n\n\n\n\n\nbaner_buildings = baner_buildings.to_crs(epsg=3347)\nbaner_buildings = baner_buildings.assign(area=baner_buildings.area)\n\n\nbaner_buildings= baner_buildings[['geometry','area']]\n\n\n\nPlot the buildings on the Map along with Area\n\nbaner_map = keplergl.KeplerGl(height=500)\nbaner_map.add_data(data=baner_buildings.copy(), name=\"Building area\")\n#baner_map.add_data(data=baner_buildings.copy(), name=\"height\")\n#baner_map\n\nUser Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n\n\n\nbaner_map.save_to_html(file_name='first_map.html')\n\nMap saved to first_map.html!\n\n\n\n%%html\n<iframe src=\"first_map.html\" width=\"80%\" height=\"500\"></iframe>\n\n\n\n\n\n#from IPython.display import IFrame\n#IFrame(src='first_map.html', width=700, height=600)"
  },
  {
    "objectID": "spatial_data_processing/osm_processing copy.html",
    "href": "spatial_data_processing/osm_processing copy.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "OpenStreetMap\n\nIt is an crowd-sourced dataset\nIt contains data about streets, buildings, services, landuse etc.\nOSMnx is a package used to retrieve, construct, analyze and visualize street networks from OpenStreetMap and also retrieve data about points of interest such as restaurants, schools and lots of different kind of services.\nIt is also easy to conduct network routing based on walking, cycling or driving by combining OSMnx functionalities with a package called NetworkX\n\n\nimport osmnx as ox\nimport matplotlib.pyplot as plt\n\n\n#place_name = \"Togo, Africa\"\nplace_name = {18.5786832,73.7666697}\n\n\ngraph = ox.graph_from_point(place_name,dist=750,dist_type='bbox',network_type=\"drive\")\n\nEmptyOverpassResponse: There are no data elements in the response JSON\n\n\n\ntype(graph)\n\nnetworkx.classes.multidigraph.MultiDiGraph\n\n\n\nfig, ax = ox.plot_graph(graph)\n\n\n\n\n\nnodes, edges = ox.graph_to_gdfs(graph)\n\n\nnodes.head()\n\n\n\n\n\n  \n    \n      \n      y\n      x\n      street_count\n      geometry\n    \n    \n      osmid\n      \n      \n      \n      \n    \n  \n  \n    \n      652724178\n      18.574935\n      73.763832\n      4\n      POINT (73.76383 18.57493)\n    \n    \n      652724182\n      18.574981\n      73.764610\n      3\n      POINT (73.76461 18.57498)\n    \n    \n      763423062\n      18.571967\n      73.764768\n      3\n      POINT (73.76477 18.57197)\n    \n    \n      871491336\n      18.574828\n      73.770821\n      4\n      POINT (73.77082 18.57483)\n    \n    \n      1377773005\n      18.574932\n      73.763664\n      3\n      POINT (73.76366 18.57493)\n    \n  \n\n\n\n\n\nedges.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      osmid\n      oneway\n      highway\n      reversed\n      length\n      name\n      geometry\n      access\n      lanes\n      ref\n      maxspeed\n    \n    \n      u\n      v\n      key\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      652724178\n      7984103956\n      0\n      669050753\n      False\n      primary\n      False\n      13.448\n      NaN\n      LINESTRING (73.76383 18.57493, 73.76387 18.57482)\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6262990166\n      0\n      669050753\n      False\n      primary\n      True\n      60.618\n      NaN\n      LINESTRING (73.76383 18.57493, 73.76364 18.57545)\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6305085563\n      0\n      73533877\n      True\n      secondary\n      False\n      24.596\n      Moze College Road\n      LINESTRING (73.76383 18.57493, 73.76402 18.574...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      652724182\n      4676484316\n      0\n      73533877\n      True\n      secondary\n      False\n      69.380\n      Moze College Road\n      LINESTRING (73.76461 18.57498, 73.76493 18.575...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7983557257\n      0\n      [250171874, 223437750]\n      False\n      residential\n      [False, True]\n      306.812\n      Echinus Court Road\n      LINESTRING (73.76461 18.57498, 73.76460 18.575...\n      yes\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\narea = ox.geocode_to_gdf(place_name)\n\nValueError: each query must be a dict or a string\n\n\n\ntype(area)\n\nNameError: name 'area' is not defined\n\n\n\narea\n\n\n\n\n\n  \n    \n      \n      geometry\n      bbox_north\n      bbox_south\n      bbox_east\n      bbox_west\n      place_id\n      osm_type\n      osm_id\n      lat\n      lon\n      display_name\n      class\n      type\n      importance\n    \n  \n  \n    \n      0\n      POLYGON ((74.91434 26.83563, 74.91534 26.83465...\n      27.860562\n      26.440461\n      76.285428\n      74.914344\n      298175590\n      relation\n      1950062\n      27.150677\n      75.747016\n      Jaipur, Rajasthan, India\n      boundary\n      administrative\n      0.671968\n    \n  \n\n\n\n\n\narea.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\ntags = {\"building\":True}\n\n\nbuildings = ox.geometries_from_place(place_name,tags)\n\nTypeError: query must be dict, string, or list of strings\n\n\n\nlen(buildings)\n\n32708\n\n\n\nbuildings.head()\n\n\n\n\n\n  \n    \n      \n      \n      nodes\n      building\n      geometry\n      area\n      barrier\n      currency:INR\n      layer\n      name\n      payment:cash\n      payment:fasttag\n      ...\n      name:tg\n      name:fr\n      motor_vehicle\n      architect\n      historic:civilization\n      outdoor_seating\n      location\n      parking\n      changing_table\n      toilets:disposal\n    \n    \n      element_type\n      osmid\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      way\n      383032803\n      [3862350688, 3862350689, 3862350690, 386235069...\n      residential\n      POLYGON ((75.82021 26.78322, 75.82020 26.78289...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032804\n      [3862350692, 3862350693, 3862350694, 386235069...\n      residential\n      POLYGON ((75.82045 26.78350, 75.82042 26.78332...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032805\n      [3862350696, 3862350697, 3862350698, 386235069...\n      residential\n      POLYGON ((75.82068 26.78322, 75.82084 26.78320...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032806\n      [3862350700, 3862350701, 3862350702, 386235070...\n      residential\n      POLYGON ((75.82069 26.78321, 75.82085 26.78319...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032807\n      [3862350704, 3862350705, 3862350706, 386235070...\n      residential\n      POLYGON ((75.82068 26.78297, 75.82079 26.78295...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 132 columns\n\n\n\n\nbuildings.shape\n\n(32708, 132)\n\n\n\n# List key-value pairs for tags\ntags = {\"amenity\":\"restaurant\"}\n\n\n# Retrieve restaurants\nrestaurants = ox.geometries_from_place(place_name, tags)\n\n# How many restaurants do we have?\nlen(restaurants)\n\n127\n\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the footprint\narea.plot(ax=ax, facecolor=\"black\")\n\n# Plot street edges\nedges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\",alpha=0.9)\n\n# Plot buildings\nbuildings.plot(ax=ax, facecolor=\"yellow\", markersize=20)\n\n# Plot restaurants\nrestaurants.plot(ax=ax, color=\"red\", markersize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "spatial_data_processing/spatial_modelling.html",
    "href": "spatial_data_processing/spatial_modelling.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "import warnings\nimport keplergl\nimport numpy as np\nimport osmnx as ox\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nfrom skgstat import Variogram\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import Point\nfrom pykrige.ok import OrdinaryKriging\nfrom scipy.interpolate import NearestNDInterpolator\nfrom tobler.area_weighted import area_interpolate\n# Custom functions\nfrom scripts.utils import pixel2poly\n# Plotting defaults\nplt.style.use('ggplot')\npx.defaults.height = 400; px.defaults.width = 620\nplt.rcParams.update({'font.size': 16, 'axes.labelweight': 'bold', 'figure.figsize': (6, 6), 'axes.grid': False})"
  },
  {
    "objectID": "spatial_data_processing/geographic_data_formats.html",
    "href": "spatial_data_processing/geographic_data_formats.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Geographic information can be represented in two forms vector or raster\nVector representations are constructed from points in geographical space which are connected to each other forming lines and polygons\nRasters are constructed from rectangular cells that form a uniform grid. Each cell of the grid contains a value representing some information such as elevation, temperature or presence / absence\nSpatio-temporal data incorporates time as additional dimension to the geographic dimension\n\n\n\n\nGeometric objects like points, lines and polygons are used\n\n\n\n\nVector data representation\n\n\n\n\n\nAttribute data is typically attached to the geometries that describe the given entity with various possible characteristics. Attributes are always linked to the geometries in one way or another.\n\n\n\n\n\nGDAL (Geospatial Data Abstraction Library) is a library for reading and writing raster and vector data formats which is used by most of the software libraries\n\n\n\n\nIntroduced by ESRI\nFilename extension is .shp\nIt is made of multiple separate files\nA valid shapefile dataset consist of:\n\n.shp - Feature geometries\n.shx - Positional index for the feature geometries\n.dbf - Attribute information\n.prj - Information about CRS of the dataset\n\n\n\n\n\n\nOpen format for encoding variety of geographic data structures along with their attribute information\nFilename extension is .geojson\nFile is not compressed\nAn example of GeoJSON data\n\n{\"type\": \"FeatureCollection\", \n    \"features\": [\n        {\"type\": \"Feature\", \"properties\": {\"id\": 75553155, \"timestamp\": 1494181812},\n        \"geometry\": {\"type\": \"MultiLineString\", \"coordinates\": [[[26.938, 60.520], [26.938, 60.520]], [[26.937, 60.521], [26.937, 60.521]], [[26.937, 60.521], [26.936, 60.522]]]}\n        }, \n        {\"type\": \"Feature\", \"properties\": {\"id\": 424099695, \"timestamp\": 1465572910}, \n        \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[26.935, 60.521], [26.935, 60.521], [26.935, 60.521], [26.935, 60.521], [26.935, 60.521]]]}\n        }\n    ]\n}\n\n\n\n\nIt uses SQLite database container to store the data\nFilename extension is .gpkg\n\n\n\n\n\nGeography Markup Language (GML) is an XML based format\nIt serves as a modeling language for geographic systems as well as an open interchange format for geographic transactions on the Internet\nFile extension is .gml\n\n\n\n\n\n\n\nData is represented as arrays of cells (called pixels) to represent real-world objects or continuous phenomena Ex- Digital photos with RGB channels\nWe can store other information to pixels, such as elevation or temperature data or more detailed spectral information that capture how the light reflects from objects on earth at different wave-lengths\n\n\n\n\nRaster data representation\n\n\n\n\n\nRaster Bit Depth\n\n\n\n\n\nxarray Data Format\n\n\n\n\n\n\n\nBased on TIFF format developed by NASA\nFile extension is .tif\n\n\n\n\n\nCloud Optimized GeoTIFF (COG)\nFile extension is .tif\n\n\n\n\n\nNetwork Common Data Form\nVariables stored in NetCDF are often measured multiple times per day over large (e.g. continental) areas\nThe file extension of NetCDF is .nc4\n\n\n\n\n\nUsed to transfer Raster files between applications\nThe file extension of ASCII Raster File is .asc\n\n\n\n\n\nThe ERDAS Imagine file format (IMG) is proprietary file format that was originally created by an image processing software company called ERDAS. The file can be accompanied with an .xml file which stores metadata information about the raster layer\nThe file extension of Imagine file format is .img\n\n\n\n\n\n\n\nNetworkx is used to store graph objects\npysal rely on sparse adjacency matrix"
  },
  {
    "objectID": "spatial_data_processing/osm_processing.html",
    "href": "spatial_data_processing/osm_processing.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "OpenStreetMap\n\nIt is an crowd-sourced dataset\nIt contains data about streets, buildings, services, landuse etc.\nOSMnx is a package used to retrieve, construct, analyze and visualize street networks from OpenStreetMap and also retrieve data about points of interest such as restaurants, schools and lots of different kind of services.\nIt is also easy to conduct network routing based on walking, cycling or driving by combining OSMnx functionalities with a package called NetworkX\n\n\nGet Street Network Graph for Tirupathi\n\nimport osmnx as ox\nimport matplotlib.pyplot as plt\n\n\nplace_name = \"Tirupathi, Andhra Pradesh, India\"\n\n\ngraph = ox.graph_from_place(place_name)\n\n\ntype(graph)\n\nnetworkx.classes.multidigraph.MultiDiGraph\n\n\n\nfig, ax = ox.plot_graph(graph)\n\n\n\n\n\nnodes, edges = ox.graph_to_gdfs(graph)\n\n\nnodes.head()\n\n\n\n\n\n  \n    \n      \n      y\n      x\n      street_count\n      geometry\n    \n    \n      osmid\n      \n      \n      \n      \n    \n  \n  \n    \n      3726004217\n      13.626082\n      79.391887\n      3\n      POINT (79.39189 13.62608)\n    \n    \n      3726082024\n      13.624080\n      79.381771\n      3\n      POINT (79.38177 13.62408)\n    \n    \n      3726082625\n      13.624315\n      79.383015\n      3\n      POINT (79.38302 13.62431)\n    \n    \n      3726082626\n      13.624330\n      79.383098\n      3\n      POINT (79.38310 13.62433)\n    \n    \n      3726082627\n      13.624499\n      79.393360\n      3\n      POINT (79.39336 13.62450)\n    \n  \n\n\n\n\n\nedges.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      osmid\n      highway\n      oneway\n      reversed\n      length\n      geometry\n      tunnel\n      bridge\n    \n    \n      u\n      v\n      key\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      3726004217\n      3727759169\n      0\n      368755785\n      service\n      False\n      True\n      102.448\n      LINESTRING (79.39189 13.62608, 79.39096 13.62589)\n      NaN\n      NaN\n    \n    \n      3726082692\n      0\n      368765668\n      service\n      True\n      False\n      11.398\n      LINESTRING (79.39189 13.62608, 79.39189 13.626...\n      NaN\n      NaN\n    \n    \n      3726082024\n      3726082625\n      0\n      368755785\n      service\n      False\n      False\n      136.948\n      LINESTRING (79.38177 13.62408, 79.38302 13.62431)\n      NaN\n      NaN\n    \n    \n      3726082653\n      0\n      368765683\n      service\n      False\n      False\n      247.024\n      LINESTRING (79.38177 13.62408, 79.38146 13.625...\n      NaN\n      NaN\n    \n    \n      3726082625\n      3726082626\n      0\n      368755785\n      service\n      False\n      False\n      9.070\n      LINESTRING (79.38302 13.62431, 79.38310 13.62433)\n      NaN\n      NaN\n    \n  \n\n\n\n\n\narea = ox.geocode_to_gdf(place_name)\n\n\ntype(area)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\narea\n\n\n\n\n\n  \n    \n      \n      geometry\n      bbox_north\n      bbox_south\n      bbox_east\n      bbox_west\n      place_id\n      osm_type\n      osm_id\n      lat\n      lon\n      display_name\n      class\n      type\n      importance\n    \n  \n  \n    \n      0\n      POLYGON ((79.37901 13.62928, 79.38167 13.62409...\n      13.634066\n      13.623569\n      79.39373\n      79.379014\n      191306545\n      way\n      369041142\n      13.626914\n      79.386643\n      Sri Venkateshwara Veterinary University, Tirup...\n      amenity\n      university\n      0.718072\n    \n  \n\n\n\n\n\narea.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\n\nGet Building information\n\ntags = {\"building\":True}\n\n\nbuildings = ox.geometries_from_place(place_name,tags)\n\n\nlen(buildings)\n\n103\n\n\n\nbuildings.head()\n\n\n\n\n\n  \n    \n      \n      \n      nodes\n      building\n      geometry\n      layer\n      name\n      ways\n      type\n    \n    \n      element_type\n      osmid\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      way\n      368754138\n      [3725992120, 3725992118, 3725992325, 372599232...\n      yes\n      POLYGON ((79.38556 13.62655, 79.38560 13.62650...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      368765639\n      [3727711369, 3727711372, 3727711373, 372771137...\n      yes\n      POLYGON ((79.38762 13.62865, 79.38763 13.62865...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      368765640\n      [3726082631, 3726082641, 3726082638, 372608263...\n      yes\n      POLYGON ((79.38655 13.62537, 79.38662 13.62548...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      368765641\n      [3726082662, 3726082681, 3726082697, 372608269...\n      yes\n      POLYGON ((79.38255 13.62598, 79.38244 13.62610...\n      1\n      Admin Office (Dr. Y.S.R. Bhavan)\n      NaN\n      NaN\n    \n    \n      368765644\n      [3726083018, 3726083020, 3726082991, 372608298...\n      yes\n      POLYGON ((79.38190 13.62782, 79.38196 13.62783...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nbuildings.shape\n\n(103, 7)\n\n\n\n# List key-value pairs for tags\ntags = {\"railway\":True}\n\n\n# Retrieve restaurants\nrailway = ox.geometries_from_place(place_name, tags)\n\n# How many restaurants do we have?\nlen(railway)\n\n1\n\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the footprint\narea.plot(ax=ax, facecolor=\"black\")\n\n# Plot street edges\nedges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\")\n\n# Plot buildings\nbuildings.plot(ax=ax, facecolor=\"silver\", alpha=0.7)\n\n# Plot restaurants\nrailway.plot(ax=ax, color=\"red\", alpha=0.7, markersize=20)\nplt.tight_layout()\n\n\n\n\n\n\nGet Park Information\n\ntags = {\"leisure\": \"park\", \"landuse\": \"grass\"}\n\n\nparks = ox.geometries_from_place(place_name, tags)\nprint(\"Retrieved\", len(parks), \"objects\")\n\nRetrieved 5 objects\n\n\n\nparks.head(3)\n\n\n\n\n\n  \n    \n      \n      \n      nodes\n      landuse\n      geometry\n    \n    \n      element_type\n      osmid\n      \n      \n      \n    \n  \n  \n    \n      way\n      368765686\n      [3726082943, 3726082942, 3726082954, 372608295...\n      grass\n      POLYGON ((79.38144 13.62756, 79.38141 13.62755...\n    \n    \n      368765687\n      [3726082995, 3726082981, 3726082971, 372608299...\n      grass\n      POLYGON ((79.38149 13.62773, 79.38150 13.62770...\n    \n    \n      368765688\n      [3726083023, 3726083010, 3726082983, 372608300...\n      grass\n      POLYGON ((79.38150 13.62785, 79.38152 13.62778...\n    \n  \n\n\n\n\n\nparks.plot(color='green')\n\n<AxesSubplot: >\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the footprint\narea.plot(ax=ax, facecolor=\"black\")\n\n# Plot the parks\nparks.plot(ax=ax, facecolor=\"green\")\n\n# Plot street edges\nedges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\")\n\n# Plot buildings\nbuildings.plot(ax=ax, facecolor=\"silver\", alpha=0.7)\n\n# Plot restaurants\nrailway.plot(ax=ax, color=\"red\", alpha=0.7, markersize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "spatial_data_processing/spatial_analysis.html",
    "href": "spatial_data_processing/spatial_analysis.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "import geopandas as gpd\nfrom pathlib import Path\n\n\ninput_path = '/home/thulasiram/personal/going_deep_and_wide/togo/togo-targeting-replication/data/shapefiles/cantons.geojson'\ndata = gpd.read_file(input_path)\n\n\ntype(data)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      canton\n      poverty\n      geometry\n    \n  \n  \n    \n      0\n      1\n      3.738084\n      MULTIPOLYGON (((0.75228 6.83786, 0.75137 6.840...\n    \n    \n      1\n      2\n      7.096286\n      MULTIPOLYGON (((0.69026 6.80602, 0.69627 6.806...\n    \n    \n      2\n      3\n      0.824586\n      MULTIPOLYGON (((0.63102 6.74430, 0.63295 6.747...\n    \n    \n      3\n      4\n      3.983729\n      MULTIPOLYGON (((0.67259 6.85123, 0.67714 6.849...\n    \n    \n      4\n      5\n      7.708810\n      MULTIPOLYGON (((0.75269 6.84116, 0.75137 6.840...\n    \n  \n\n\n\n\n\nprint(\"Number of rows\",len(data[\"canton\"]))\nprint(\"Number of classes\",data[\"canton\"].nunique())\n\nNumber of rows 387\nNumber of classes 387\n\n\n\n\n\n\ndata.plot()\n\n<AxesSubplot: >\n\n\n\n\n\nChecking the shape and area of the first Multipolygon in the data\n\ndata.at[0,\"geometry\"]\n\n\n\n\n\n# Calculating area with lat and long is wrong, to calculate \n# area correctly we need to change the co-ordinate reference system\nround(data.at[0,\"geometry\"].area,3)\n\n0.014\n\n\n\n\n\n\ndata.plot(\"poverty\",legend=True)\n\n<AxesSubplot: >\n\n\n\n\n\n\ndata.explore(\"poverty\",legend=True)\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n\nGeocoding is the process of transforming place names or addresses into coordinates\n\n# Import necessary modules\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n\n# Filepath\nfp = '../data/addresses.txt'\n\n# Read the data\ndata = pd.read_csv(fp,sep=\";\")\n\n\nlen(data)\n\n5\n\n\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      id\n      addr\n    \n  \n  \n    \n      0\n      1000\n      Boat Club Road, 411001, Pune, Maharastra\n    \n    \n      1\n      1001\n      Koregaon, 415501, Pune, Maharastra\n    \n    \n      2\n      1002\n      Kothrud, 411038, Pune, Maharastra\n    \n    \n      3\n      1003\n      Balewadi, 411045, Pune, Maharastra\n    \n    \n      4\n      1004\n      Baner, 411047, Pune, Maharastra\n    \n  \n\n\n\n\n\n# Import the geocoding tool\nfrom geopandas.tools import geocode\n\n# Geocode addresses using Nominatim. Remember to provide a custom \"application name\" in the user_agent parameter!\ngeo = geocode(data[\"addr\"], provider=\"nominatim\", user_agent=\"autogis_xx\", timeout=4)\n\n\ngeo.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      address\n    \n  \n  \n    \n      0\n      POINT (73.87826 18.53937)\n      Boat Club Road, Pune City, Pune, Maharashtra, ...\n    \n    \n      1\n      POINT (73.89299 18.53772)\n      Koregaon Park, Suyojan Society, Ghorpuri, Pune...\n    \n    \n      2\n      POINT (73.80767 18.50389)\n      Kothrud, Pune City, Maharashtra, 411038, India\n    \n    \n      3\n      POINT (73.76912 18.57767)\n      Prakashgad Society, Balewadi, Perfect 10 Inter...\n    \n    \n      4\n      POINT (73.77686 18.56424)\n      Baner, Pune City, Maharashtra, 511045, India\n    \n  \n\n\n\n\n\n# Joining the original dataframe with geoencoded dataframe\njoin = geo.join(data)\njoin = join.drop(columns=['address'])\n\n\njoin.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      id\n      addr\n    \n  \n  \n    \n      0\n      POINT (73.87826 18.53937)\n      1000\n      Boat Club Road, 411001, Pune, Maharastra\n    \n    \n      1\n      POINT (73.89299 18.53772)\n      1001\n      Koregaon, 415501, Pune, Maharastra\n    \n    \n      2\n      POINT (73.80767 18.50389)\n      1002\n      Kothrud, 411038, Pune, Maharastra\n    \n    \n      3\n      POINT (73.76912 18.57767)\n      1003\n      Balewadi, 411045, Pune, Maharastra\n    \n    \n      4\n      POINT (73.77686 18.56424)\n      1004\n      Baner, 411047, Pune, Maharastra\n    \n  \n\n\n\n\n\n\n\n\nfrom shapely.geometry import box\n\nminx = 73.76\nminy = 18.537\nmaxx = 73.89\nmaxy = 18.56\ngeom = box(minx, miny, maxx, maxy)\nclipping_gdf = gpd.GeoDataFrame({\"geometry\": [geom]}, index=[0], crs=\"epsg:4326\")\n\n# Explore the extent on a map\nclipping_gdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Write the data to shape file\noutfp = r\"../data/addresses.shp\"\njoin.to_file(outfp)"
  },
  {
    "objectID": "spatial_data_processing/crs.html",
    "href": "spatial_data_processing/crs.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Coordinate Reference Systems (CRS)\nA CRS tells python how coordinates are related to places on the Earth. A map projection (or a projected coordinate system) is a systematic transformation of the latitudes and longitudes into a plain surface where units are quite commonly represented as meters (instead of decimal degrees). This transformation is used to represent the three dimensional earth on a flat, two dimensional map.\nThere is no perfect projection and we should know the strength and weaknesses of projection systems and choose a projection system that best fits our purpose.\nWe can reproject the geometries from crs to another using to_crs() function from GeoPandas.\nWe can define the coordinate system in different formats using pyproj CRS\n\nImport and view the data\n\nimport geopandas as gpd\n\n\n# Read the data\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n\nworld.head(4)\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      0\n      889953.0\n      Oceania\n      Fiji\n      FJI\n      5496\n      MULTIPOLYGON (((180.00000 -16.06713, 180.00000...\n    \n    \n      1\n      58005463.0\n      Africa\n      Tanzania\n      TZA\n      63177\n      POLYGON ((33.90371 -0.95000, 34.07262 -1.05982...\n    \n    \n      2\n      603253.0\n      Africa\n      W. Sahara\n      ESH\n      907\n      POLYGON ((-8.66559 27.65643, -8.66512 27.58948...\n    \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n  \n\n\n\n\n\n\nView the CRS of the data\n\n# Check the CRS of the data.\n# Lat Long data should have EPSG 4326 and WGS 84\nworld.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n\nChange the CRS and visualize the data\n\nax = world.plot()\nax.set_title(\"WGS84 (lat/lon)\")\nworld = world[(world.name != \"Antarctica\") & (world.name != \"Fr. S. Antarctic Lands\")]\n# Data in Mercator Projection\nworld = world.to_crs(\"EPSG:3395\")\nax = world.plot()\nax.set_title(\"Mercator\")\n\nText(0.5, 1.0, 'Mercator')\n\n\n\n\n\n\n\n\n\n\nOrthographic Projection\n\n# Orthographic projection\nfrom pyproj import CRS\n\n\n# Define an orthographic projection, from: http://www.statsmapsnpix.com/2019/09/globe-projections-and-insets-in-qgis.html\northo = CRS.from_proj4(\n    \"+proj=ortho +lat_0=60.00 +lon_0=23.0000 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\"\n)\n\n# Re-project and plot\nax = world.to_crs(ortho).plot()\n\n# Remove x and y axis\nax.axis(\"off\")\nax.set_title(\"Orthographic\")\n\nText(0.5, 1.0, 'Orthographic')"
  },
  {
    "objectID": "spatial_data_processing/gee_timelapse.html",
    "href": "spatial_data_processing/gee_timelapse.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Steps to create a Landsat timelapse:\n\nPan and zoom to your area of interest, or click the globe icon at the upper left corner to search for a location.\nUse the drawing tool to draw a rectangle anywhere on the map.\nAdjust the parameters (e.g., start year, end year, title) if needed.\nClick the Create timelapse button to create a timelapse.\nOnce the timelapse has been added to the map, click the hyperlink at the end if you want to download the GIF.\n\n\nimport os\nimport ee\nimport geemap\nimport ipywidgets as widgets\n\n\nMap = geemap.Map()\nMap.add_basemap('HYBRID')\nMap\n\n\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 10.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 1984.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 2020.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 5.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 10.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 30.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 0.\n\n\n\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-0e20ead6bdef0b1d973c2259323cb508:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map.\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-0ed6f7095071ae8dea998b8fcb895851:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map.\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-be14a4d9b69711ca1550db81d8fda1a8:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map."
  },
  {
    "objectID": "spatial_data_processing/raster_data_processing.html",
    "href": "spatial_data_processing/raster_data_processing.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Converting Data from Raster to Tabular (Geometry) format\n\nImport the libraries\n\nimport pandas\nimport osmnx\nimport geopandas \nimport rioxarray\nimport xarray\nimport datashader as ds\nimport contextily as cx\nfrom shapely import geometry\nimport matplotlib.pyplot as plt\nimport folium\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\n\nDownload Geopackage\n\n# URL for the geopackage\nurl = (\"https://jeodpp.jrc.ec.europa.eu/ftp/\"\\\n       \"jrc-opendata/GHSL/\"\\\n       \"GHS_FUA_UCDB2015_GLOBE_R2019A/V1-0/\"\\\n       \"GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.zip\"\n      )\nurl\n\n'https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL/GHS_FUA_UCDB2015_GLOBE_R2019A/V1-0/GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.zip'\n\n\n\n\nVisualize the map\n\n# Visualize the Map for Sao Paulo\np = f\"zip+{url}!GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.gpkg\"\nfuas = geopandas.read_file(p)\nsao_paulo = fuas.query(\"eFUA_name == 'São Paulo'\").to_crs(\"EPSG:4326\")\n\n\nax = sao_paulo.plot(alpha=0.5, figsize=(9, 9))\ncx.add_basemap(ax, crs=sao_paulo.crs);\n\n\n\n\n\n\nDownload the population data\n\nurl = (\"https://cidportal.jrc.ec.europa.eu/ftp/\"\\\n       \"jrc-opendata/GHSL/GHS_POP_MT_GLOBE_R2019A/\"\\\n       \"GHS_POP_E2015_GLOBE_R2019A_54009_250/V1-0/\"\\\n       \"tiles/\"\\\n       \"GHS_POP_E2015_GLOBE_R2019A_54009_250_V1_0_13_11.zip\"\n      )\nurl\n\n'https://cidportal.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL/GHS_POP_MT_GLOBE_R2019A/GHS_POP_E2015_GLOBE_R2019A_54009_250/V1-0/tiles/GHS_POP_E2015_GLOBE_R2019A_54009_250_V1_0_13_11.zip'\n\n\n\n# Population data in raster format\n%%time\np = f\"zip+{url}!GHS_POP_E2015_GLOBE_R2019A_54009_250_V1_0_13_11.tif\"\nghsl = rioxarray.open_rasterio(p)\nghsl\n\nCPU times: user 35.6 ms, sys: 4.12 ms, total: 39.8 ms\nWall time: 7.12 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 4000, x: 4000)>\n[16000000 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -5.041e+06 -5.041e+06 ... -4.041e+06 -4.041e+06\n  * y            (y) float64 -2e+06 -2e+06 -2.001e+06 ... -3e+06 -3e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    _FillValue:     -200.0\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 4000x: 4000...[16000000 values with dtype=float32]Coordinates: (4)band(band)int641array([1])x(x)float64-5.041e+06 ... -4.041e+06array([-5040875., -5040625., -5040375., ..., -4041625., -4041375., -4041125.])y(y)float64-2e+06 -2e+06 ... -3e+06 -3e+06array([-2000125., -2000375., -2000625., ..., -2999375., -2999625., -2999875.])spatial_ref()int640crs_wkt :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]spatial_ref :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :-5041000.0 250.0 0.0 -2000000.0 0.0 -250.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([-5040875.0, -5040625.0, -5040375.0, -5040125.0, -5039875.0,\n              -5039625.0, -5039375.0, -5039125.0, -5038875.0, -5038625.0,\n              ...\n              -4043375.0, -4043125.0, -4042875.0, -4042625.0, -4042375.0,\n              -4042125.0, -4041875.0, -4041625.0, -4041375.0, -4041125.0],\n             dtype='float64', name='x', length=4000))yPandasIndexPandasIndex(Float64Index([-2000125.0, -2000375.0, -2000625.0, -2000875.0, -2001125.0,\n              -2001375.0, -2001625.0, -2001875.0, -2002125.0, -2002375.0,\n              ...\n              -2997625.0, -2997875.0, -2998125.0, -2998375.0, -2998625.0,\n              -2998875.0, -2999125.0, -2999375.0, -2999625.0, -2999875.0],\n             dtype='float64', name='y', length=4000))Attributes: (4)AREA_OR_POINT :Area_FillValue :-200.0scale_factor :1.0add_offset :0.0\n\n\n\n\nVisualize the population on raster data\n\ncvs = ds.Canvas(plot_width=600, plot_height=600)\nagg = cvs.raster(ghsl.where(ghsl>0).sel(band=1))\n\n\nf, ax = plt.subplots(1, figsize=(9, 7))\nagg.plot.imshow(ax=ax, alpha=0.5, cmap=\"cividis_r\")\ncx.add_basemap(\n    ax, \n    crs=ghsl.rio.crs, \n    zorder=-1, \n    source=cx.providers.CartoDB.Voyager\n)\n\n\n\n\n\n# Clip the data for Sao Paulo\nghsl_sp = ghsl.rio.clip(sao_paulo.to_crs(ghsl.rio.crs).geometry.iloc[0])\nghsl_sp\n\n/home/thulasiram/miniconda3/envs/geopy/lib/python3.9/site-packages/rasterio/features.py:290: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n  for index, item in enumerate(shapes):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 416, x: 468)>\narray([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -4.482e+06 -4.482e+06 ... -4.365e+06 -4.365e+06\n  * y            (y) float64 -2.822e+06 -2.822e+06 ... -2.926e+06 -2.926e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0\n    _FillValue:     -200.0xarray.DataArrayband: 1y: 416x: 468-200.0 -200.0 -200.0 -200.0 -200.0 ... -200.0 -200.0 -200.0 -200.0array([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)Coordinates: (4)band(band)int641array([1])x(x)float64-4.482e+06 ... -4.365e+06axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([-4481875., -4481625., -4481375., ..., -4365625., -4365375., -4365125.])y(y)float64-2.822e+06 ... -2.926e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([-2822125., -2822375., -2822625., ..., -2925375., -2925625., -2925875.])spatial_ref()int640crs_wkt :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]spatial_ref :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :-4482000.0 250.0 0.0 -2822000.0 0.0 -250.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([-4481875.0, -4481625.0, -4481375.0, -4481125.0, -4480875.0,\n              -4480625.0, -4480375.0, -4480125.0, -4479875.0, -4479625.0,\n              ...\n              -4367375.0, -4367125.0, -4366875.0, -4366625.0, -4366375.0,\n              -4366125.0, -4365875.0, -4365625.0, -4365375.0, -4365125.0],\n             dtype='float64', name='x', length=468))yPandasIndexPandasIndex(Float64Index([-2822125.0, -2822375.0, -2822625.0, -2822875.0, -2823125.0,\n              -2823375.0, -2823625.0, -2823875.0, -2824125.0, -2824375.0,\n              ...\n              -2923625.0, -2923875.0, -2924125.0, -2924375.0, -2924625.0,\n              -2924875.0, -2925125.0, -2925375.0, -2925625.0, -2925875.0],\n             dtype='float64', name='y', length=416))Attributes: (4)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0_FillValue :-200.0\n\n\nout_p = “../data/ghsl_sao_paulo.tif” ! rm $out_p ghsl_sp.rio.to_raster(out_p)\n\n\nConvert Raster to geometry\n\n# Read the raster data\nsurface = xarray.open_rasterio(\"../data/ghsl_sao_paulo.tif\")\n\n\n# Convert raster to geometry\nt_surface = surface.to_series()\n\n\nt_surface.head()\n\nband  y           x         \n1     -2822125.0  -4481875.0   -200.0\n                  -4481625.0   -200.0\n                  -4481375.0   -200.0\n                  -4481125.0   -200.0\n                  -4480875.0   -200.0\ndtype: float32\n\n\n\nt_surface = t_surface.reset_index().rename(columns={0: \"Value\"})\n\n\nt_surface.query(\"Value > 1000\").info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 7734 entries, 3785 to 181296\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   band    7734 non-null   int64  \n 1   y       7734 non-null   float64\n 2   x       7734 non-null   float64\n 3   Value   7734 non-null   float32\ndtypes: float32(1), float64(2), int64(1)\nmemory usage: 271.9 KB\n\n\n\ntype(t_surface)\n\npandas.core.frame.DataFrame\n\n\n\n# Calculate the polygon based on resolution values\ndef row2cell(row, res_xy):\n    res_x, res_y = res_xy  # Extract resolution for each dimension\n    # XY Coordinates are centered on the pixel\n    minX = row[\"x\"] - (res_x / 2)\n    maxX = row[\"x\"] + (res_x / 2)\n    minY = row[\"y\"] + (res_y / 2)\n    maxY = row[\"y\"] - (res_y / 2)\n    poly = geometry.box(\n        minX, minY, maxX, maxY\n    )  # Build squared polygon\n    return poly\n\n\n# Get the polygons\nmax_polys = (\n    t_surface.query(\n        \"Value > 1000\"\n    )  # Keep only cells with more than 1k people\n    .apply(  # Build polygons for selected cells\n        row2cell, res_xy=surface.attrs[\"res\"], axis=1\n    )\n    .pipe(  # Pipe result from apply to convert into a GeoSeries\n        geopandas.GeoSeries, crs=surface.attrs[\"crs\"]\n    )\n)\n\n\n# Plot polygons on the map\nax = max_polys.plot(edgecolor=\"red\", figsize=(9, 9))\n# Add basemap\ncx.add_basemap(\n    ax, crs=surface.attrs[\"crs\"], source=cx.providers.CartoDB.Voyager\n)\n\n\n\n\n\n\nConvert Geometry to Raster\n\nnew_da = xarray.DataArray.from_series(\n    t_surface.set_index([\"band\", \"y\", \"x\"])[\"Value\"]\n)\nnew_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'Value' (band: 1, y: 416, x: 468)>\narray([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)\nCoordinates:\n  * band     (band) int64 1\n  * y        (y) float64 -2.926e+06 -2.926e+06 ... -2.822e+06 -2.822e+06\n  * x        (x) float64 -4.482e+06 -4.482e+06 ... -4.365e+06 -4.365e+06xarray.DataArray'Value'band: 1y: 416x: 468-200.0 -200.0 -200.0 -200.0 -200.0 ... -200.0 -200.0 -200.0 -200.0array([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)Coordinates: (3)band(band)int641array([1])y(y)float64-2.926e+06 ... -2.822e+06array([-2925875., -2925625., -2925375., ..., -2822625., -2822375., -2822125.])x(x)float64-4.482e+06 ... -4.365e+06array([-4481875., -4481625., -4481375., ..., -4365625., -4365375., -4365125.])Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))yPandasIndexPandasIndex(Float64Index([-2925875.0, -2925625.0, -2925375.0, -2925125.0, -2924875.0,\n              -2924625.0, -2924375.0, -2924125.0, -2923875.0, -2923625.0,\n              ...\n              -2824375.0, -2824125.0, -2823875.0, -2823625.0, -2823375.0,\n              -2823125.0, -2822875.0, -2822625.0, -2822375.0, -2822125.0],\n             dtype='float64', name='y', length=416))xPandasIndexPandasIndex(Float64Index([-4481875.0, -4481625.0, -4481375.0, -4481125.0, -4480875.0,\n              -4480625.0, -4480375.0, -4480125.0, -4479875.0, -4479625.0,\n              ...\n              -4367375.0, -4367125.0, -4366875.0, -4366625.0, -4366375.0,\n              -4366125.0, -4365875.0, -4365625.0, -4365375.0, -4365125.0],\n             dtype='float64', name='x', length=468))Attributes: (0)"
  },
  {
    "objectID": "research_methods/literature_review.html",
    "href": "research_methods/literature_review.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Use Nighttime light values (NLV) as a proxy for poverty\nDataset provided by Earth Observation Group and Images from Google Static Map at 2.5 Meters of resolution\nUsing nightlights as a proxy for development. Nightlights data cannot be directly used because there is less difference in luminosity between rich and poor regions in Africa\nUse Day images which capture more information and use night images as label data.\nUse transfer learning to learn features from day satellite images and NLV labels\nTrain the model and learn important features. These features are calculated for a new image.\nUse these learned features along with survey data at cluster level to train the model.\nAs the cluster level data is very less, use simple models\nUse the trained model for classifying new clusters or areas\n\n\n\n\nUsing CNN on Nightlight Images to learn features\n\n\n\n\n\n\n\n\n\nPublicly available satellite-based estimates of poverty are available\nThe estimation methods use deep learning models trained on Demographic and Health Surveys (DHS) data from neighbouring countries to estimate the average relative wealth of each 2.4km tile in Togo\nThis is used to do cluster level predictions\nIdentify the clusters\nUse phone CDR as independent variables and survey data as dependent variable to build models for each household\n\n\n\n\nUsing Phone CDR and Survey data for prediction\n\n\n\n\n\n\n\n\n\nLand use and the manufactured objects observed in a satellite image emphasize the wealthiness of an area\nCNN was trained on a land use detection and classification task\nThey used xView data consisting of very high resolution images annotated with bounding boxes defined over 10 main classes (building, fixed-wing aircraft, passenger vehicle, truck, railway vehicle, maritime vessel, engineering vehicle, helipad, vehicle lot, construction site) and 60 sub-classes.\nYolo V3 was used for object detection\n\n\n\n\n\n\n\n\nBased on Unsupervised learning\nThis method emphasizes the difference between two satellite images\nCluster homogeneous-looking areas and assume that some clusters will be specific to poor areas\n\n\n\n\nFields are often surronded by other fields\n\n\n\n\n\nContrastive learning between Anchor Neighbor and Distant tiles\n\n\n\n\n\n\n\n\n\ncomparison of different approaches"
  },
  {
    "objectID": "research_methods/nlp_wiki_wealth_pred.html",
    "href": "research_methods/nlp_wiki_wealth_pred.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Estimate socioeconomic indicators using open-source, geolocated textual information from wikipedia articles\nNLP techniques are used to predict community level asset wealth and education outcomes using nearby geolocated Wikipedia articles\nMany wikipedia articles are geolocated. Many developing regions of the world contain high concentrations of geolocated articles. These articles contain a rich textual information about locations and entities in an area\n\n\n\n\nAn Example of a geolocated wikipedia article\n\n\n\n\n\nGeolocated articles are mapped to a vector representation using Doc2vec method\nUse spatial distribution of the embeddings to predict socioeconomic indicators of poverty, as measured by ground-truth survey data collected by the world bank.\nThe model is further extended to include information about nightime light intensity as measured by satellites\nThis method is able to provide reliable predictions\n\n\n\n\n\nAsset ownership from DHS\nCorpus of geolocated wikipedia articles. For Africa there were roughly 50,000 such articles.\nNightlights Imagery from VIIRS\n\n\n\n\n\nWikipedia articles consist of a lot of bias in terms of information present, length of articles etc\nDoc2vec model is used to train the embeddings from the documents\n\n\n\n\nDoc2vec Model\n\n\n\n\n\nMulti-Modal architecture with Images and Text\n\n\n\n\n\n\nWikipedia embedding model outperformed the Nightlight-only model (train and tested within the same country)\nWikipedia embedding contributes positively towards the predictions\nMulti-modal model performs best in all the different situations\nResults suggest that wikipedia embeddings and nightlight images provide highly complementary information about poverty\n\n\n\n\nResearch Article"
  },
  {
    "objectID": "research_methods/Tile2Vec.html",
    "href": "research_methods/Tile2Vec.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Learning a lower-dimensional representation of the data that can be used for any number of downstream ML tasks\nLandscapes in remote sensing datasets are highly spatially correlated. The idea is to extract enough learning signal to reliably train deep neural networks\n\n\n\n\nThe distributional hypothesis in linguistics is the idea that a word is characterized by the company it keeps. words that appear in similar contexts should have similar meanings.\nIn natural language processing (NLP), this assumption that meaning can be derived from context is leveraged to learn continuous word vector representations like Word2vec and GloVe\n\n\n\n\n\nEverything is related to everything else, but near things are more related than distant things.\n\n\n\n\n\nTo extend the Word2vec analogy from NLP, an image tiles is used similar to be our “words” and spatial neighborhoods (defined by some radius) to define the context. The image tiles that are geographic neighbors (i.e. close spatially) should have similar semantics and therefore representations, while tiles far apart are likely to have dissimilar semantics and should therefore have dissimilar representations.\nTo learn a mapping from image tiles to low-dimensional embeddings, train a CNN on triplets of tiles, where each triplet consists of an anchor tile, neighbor tile and a distant tile.\nTiles from the same neighborhood are more likely to be similar than their more distant counterparts.\nA CNN is trained to minimize the distance between the anchor and neighbor embeddings, while maximizing the distance between the anchor and distant embeddings.\n\n\n\n\nContrastive Spatial Analysis\n\n\n\n\n\nA CNN is used to learn the embeddings\n\n\n\n\n\n\n\nTile2Vec CNN is a ResNet-18 architecture without the classification layer\n\n\n\n\n\nFor each Uganda cluster, the team extracted a median composite through Google Earth Engine of roughly 75 × 75 pixels (5 km2) centered at its location. They randomly sample 10 tiles from this patch and average their Tile2Vec embeddings. These embeddings are then input to a ridge regression to predict log consumption expenditures.\n\n\n\nBlog post on Tile2Vec Research Paper Github Repo"
  },
  {
    "objectID": "research_methods/new_directions.html",
    "href": "research_methods/new_directions.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Use Internet speeds for a location to estimate the development in the area (Ookla provides open source datasets for the same)\n\n\n\n\n\nFor a given location / area using Openstreet Map data it is possible to derive the following information, which can be used to estimate wealth\n\nNo. of buildings\nNo. of roads\nNo. of primary roads\nNo. of trunk roads\ncount of market places\nNumber of charging stations\nNo. of post offices\nSupermarket counts\nCar repair counts\nDepartment stores\nComputers\nPlaygrounds\nMonuments\n\n\n\n\n\n\n\n\n\nUsing explainable models - How much a variable is influencing the predictions\nExplainability along with good prediction accuracy. Example - Explainable boosting machines (EBMs)\nWe can build editable models using EBMs\n\n\n\n\n\nUse libraries like snorkel to generate labels\nLow confidence images can be routed to humans for labeling\n\n\n\n\n\nThere is correlation between wealth and spatial location\nA Gaussian process can be implemented on top of the model. Use the prior observations to get the posterior distribution from the priors (tried at stanford in 2016)\n\n\n\n\n\nUsing time series analysis to forecast the wealth from past surveys"
  },
  {
    "objectID": "research_methods/image_detection.html",
    "href": "research_methods/image_detection.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Poverty prediction using night lights and other methods are not interpretable\nThis method uses object detection on high resolution satellite images\nUse weighted counts of objects as features for predicting local-level consumption\nA satellite imagery object detector was trained on a publicly available, global scale object detection dataset, called xView, which avoids location specific training and provides a more general object detection model. The model is then applied to high resolution images taken over hundreds of villages across Uganda that were measured in an existing georeferenced household survey, and use extracted counts of detected objects as features in a final prediction of consumption expenditure.\nA linear regression model is used to predict consumption\n\n\n\n\nMethodology\n\n\n\n\n\n\nSurvey data from Living Standards Measurement Study (LSMS) for Uganda.\nUganda satellite imagery - High resolution images from DigitalGlobe satellites with three bands (RGB) and 30cm pixel resolution.\nAs object annotations were not available for Uganda, transfer learning was done by training an object detector on a different but related source dataset (xView dataset)\n\n\n\n\n\nYOLOv3 with single stage object detector with a DarkNet53 backbone architecture\nMean average precision and recall per class was used for evaluation\nxView dataset consist of parent and child classes. Two object detectors were trained using parent and child level classes\nFour types of features which was explored for this research paper\n\nCount of objects\nConfidence x counts - objects detected in Uganda was weighted by the confidence score\nEach detected object is weighted by its bounding box area\n(Confidence, size) X counts\n\nGiven the cluster level categorical feature vector, the poverty index is estimated with a regression model\nCount of objects was performing better than other methods\n\n\n\n\nFeature importance of the final model\n\n\nReference\n\nResearch Paper"
  },
  {
    "objectID": "research_methods/ph_social_media.html",
    "href": "research_methods/ph_social_media.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Satellite images are costly to acquire and training a deep learning model requires costly GPU resources\nDeep learning models do not provide interpretability\nIn this study these challenges are overcome by combining social media and geospatial data sources with cost efficient ML methods as an interpretable and inexpensive approach to poverty estimation\n\n\n\n\nGround truth data - DHS data for Philippines\nGeospatial covariates\n\nSocial Media advertising data\n\nFacebook users per DHS household cluster with a breakdown of user segments such as users with 4G access, 3G access, 2G access, WiFi access, Apple devices and mid-to-high valued goods consumer preferences\n\nRemote sensing data\n\nUsing Google Earth Engine (GEE) features were extracted from publicly accessible low-resolution saellite images\nNighttime luminosity data taken from the Visible Infrared Imaging Radiometer Suite provided by NASA\nDaytime and nighttime land surface temperature derived from MODIS Satellite 2017 data\nNormalized Difference Vegetation Index (NDVI) derived from Landsat 2017\nFor each satellite image, summary statistics was computed, i.e. the mean, maximum, minimum,skewness, variance, and kurtosis of all cloudless pixel values within each DHS cluster\n\nPoint of interest data\n\nUsing OSM volunteered geographic information was obtained for various points of interest like banks, hotels, convenience stores within each DHS household cluster.\n\nHealth data from Department of health\nPublic school information\n\n\n\n\n\n\nLinear Regression, Lasso Regression, Ridge Regression, Random Forest, and LightGBM. The models were trained on social media data, remote sensing data, and point of interest data, first separately then combined, with the hypothesis that integrating multiple data sources will lead to improved model performance over using any one data source alone.\n\n\n\n\n\nUsing multiple data sources provided better results than using a single data source\nImportant features in this study to predict wealth are night time light values, proportion of population with 4G access, presence of public schools\n\n\n\n\nResearch Paper"
  },
  {
    "objectID": "research_methods/poverty_mapping_deep_reinforcement.html",
    "href": "research_methods/poverty_mapping_deep_reinforcement.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Reinforcement learning approach in which free low-resolution imagery is used to dynamically identify where to acquire costly high-resolution images, prior to performing a deep learning task on high-resolution images\n\n\n\n\nLSMS survey conducted in Uganda\nHigh resolution images from DigitalGlobe satellites with 3 bands (RGB) and 30cm pixel resolution\nLow resolution satellite imagery from Sentinel-2 with 3 bands (RGB) with 10m pixel resolution\n\n\n\n\n\n\n\nDeep reinforcement learning method used\n\n\n\nIn the first step, High Resolution (HR) tiles are adaptively sampled and in the second step, pre-trained detector is used on the images\n\n\n\n\nThis framework finds tiles to sample, conditioned on the low spatial resolution image covering a cluster.\nA policy network is modelled to only choose tiles where there is desirable number of object counts\nThe reward function encourages dropping as many subtiles as possible while successfully approximating the classwise object counts (object detection was used)\n\n\n\n\n\n\nThe model achieved an R-squared of 0.62 and substantially outperforms results published from other studies, while using around 80% fewer satellite images.\nThe model is performing well when images of wet season is used instead of dry season\n\n\n\n\nDifference in image acquisition for dry and wet seasons\n\n\n\n\n\nResearch Paper"
  },
  {
    "objectID": "data_sources_detecting_poverty/multispectral_remote_sensing.html",
    "href": "data_sources_detecting_poverty/multispectral_remote_sensing.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Multispectral remote sensing is a passive remote sensing type. This means that the sensor is measuring light energy from an existing source - in this case the sun.\n\n\nThe electromagnetic spectrum is composed of a range of different wavelengths or “colors” of light energy. A spectral remote sensing instrument collects light energy within specific regions of the electromagnetic spectrum. Each region in the spectrum is referred to as a band.\n\n\n\nElectromagnetic Spectrum\n\n\n\n\n\nSpectral remote sensing data are collected by powerful camera-like instruments known as imaging spectrometers. Imaging spectrometers collect reflected light energy in “bands.”\nIn Multispectral dataset, the band information is reported as the center wavelength value. This value represents the center point value of the wavelengths represented in that band. Thus in a band spanning 800-850 nm, the center would be 825 nm.\n\n\n\nThe spectral resolution of a dataset that has more than one band, refers to the spectral width of each band in the dataset.\n\n\n\nThe spatial resolution of a raster represents the area on the ground that each pixel covers. If you have smaller pixels in a raster the data will appear more “detailed.” If you have large pixels in a raster, the data will appear more coarse or “fuzzy.”"
  },
  {
    "objectID": "data_sources_detecting_poverty/survey_data.html",
    "href": "data_sources_detecting_poverty/survey_data.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Demographic and Health Survey (DHS)\nHousehold expenditure and Income survery\nLiving standards measurement study (LSMS)\nSurveys conducted by National Statistics Office (NSO) of various countries\n\n\n\n\n\nVerifiability of the variable\nCorrelation betweent the variable and household consumption levels\nWe should not depend on any single or very few variables\nThe variables may differ based on the region. For example for a rural area, livestock is important but not for urban area\n\n\n\n\nHuman capital variables\n\nEducation of household head\nHighest level of education in household\nFemale literacy\nNumber of childern in school\n\nDemographic characteristics\n\nHousehold size\nNumber of children\nGender / Marital status\nAge of Household head\nDependency ratio\n\nHousehold assets\n\nOwn home\nType of wall construction\nType of roofing material\nType of latrine\nNumber of rooms per capita\nType of cooking fuel\nRadio, television and other forms of electronic or communication devices\nBicycle, car, motorcycle or other means of owned transport\nFurniture\nAccess to electricity\nCooker, heater, fan, air-conditioning\n\nProductive assets\n\nLandholding size\nLivestock\nuse of fertiliser\n\nLivelihood options\n\nAgricultural or nonfarm wage labour\nNon-farm independent business\nAgricultural production of cash or staple crops\nReceipt of foreign remittances\nSector of work\n\nCommunity variables\n\nPresence of midwife\nPopulation density\nAsphalt road\nBank in Community\nDivisional Secretariat in community\n\n\n\n\n\n\n\nProxies used in a PMT model\n\n\nReference:-\n\nWorld bank Document on PMT\nTargeting the Poorest\nExclusion by Design\nconsiderations in using PMT"
  },
  {
    "objectID": "data_sources_detecting_poverty/satellite_imagery.html",
    "href": "data_sources_detecting_poverty/satellite_imagery.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Satellite images are obtained from earth-observing systems. In general, the three main types of these systems, based on the altitude of their orbit, are geostationary (GEO) satellites, low Earth orbit (LEO) satellites, and medium Earth orbit satellites.\nGEO satellites stay positioned over the same spot on the Earth, with a highest altitude of about 36,000 km. This enables them to have greater Earth surface coverage, but with an increasingly skewed pixel towards the edge of the sensor coverage. GEO satellites were originally designed for meteorological use. An example of such a satellite is the HIMAWARI-8, which is positioned over Indonesia and can cover half the globe, having the highest spatial resolution of 500 meters (m) with images taken at 10-minute intervals.\nLEO satellites are positioned relatively close to the Earth’s surface at an altitude of 400 km to 800 km. These satellites can complete their rotation around the earth in about 90 minutes as they travel through a fixed orbit at around 28,000 km per hour. LEO satellites have wider coverage toward the poles, instead of at the equator. Being closer to earth allows these satellites to have higher spatial resolution. The resolution of LEO satellites can be as high as 30 centimeters per pixel for captured images in black and white or panchromatic, while commercially available images in color or multispectral bands can have about 1 m per pixel. Some popular publicly available LEO sensors are the Moderate Resolution Imaging Spectroradiometer (MODIS) and Landsat; with spatial resolutions of 250 m, 500 m, and 1000 m for MODIS, and 30 m for Landsat. These sensors have data applications that are well documented and have been covered by peer-reviewed literature. Meanwhile, the Sentinel-2A and 2B satellites, operated by the European Satellite Agency, have spatial resolutions of 10 m to 60 m, depending on the band.\nMedium earth orbit satellites are commonly used on navigation, communication, and geodetic or space satellites. They are positioned at approximately 20,000 km above the Earth, between GEO and LEO satellites.\n\n\n\n\nLandsat 8 Spectral bands and their purpose\n\n\n\n\n\n\nPrevious studies have used publicly available images from Landsat 8 (with 15 m resolution after pansharpening) and Sentinel 2 (with 10 m resolution). In identifying which specific area an image belonged to, its center was used as reference point.\n\n\n\n\n\nCollect cloud-free daytime images or with least amount of cloud cover\nPanshrapen the images to enhance the resolution of the images. Pansharpening produces a single high-resolution, color, multiband RGB image by combining high-resolution panchromatic images (black and white but sensitive to colors) with lower-resolution, multispectral band images. This is achieved by increasing the pixel-per-unit area of the multispectral band RGB image, transforming the RGB color scheme into a hue saturation value, and changing the value to the pixel intensity of the panchromatic image. The original Landsat images with 30 m resolution were converted to 15 m resolution after pansharpening\nWe should isolate the images that render highest loss, to prevent contamination of the input dataset. These images are very cloudy, with no recognizable land or urban areas, which could render the model inaccurate in predicting class and training incorrect features. Such images were caused either by weather disturbance or technical problems with the sensor’s camera, and should be isolated from further training.\nData augmentation methods should be used to prevent overfitting. Methods suitable to remote-sensing images are:-\n\nVertical and horizontal flipping\nRandom lighting\ncontrast change within a 10% probability\nDihedral and symmetric warping\n\n\n\n\n\n\nEarth Engine\nTranslator library for raster and vector geospatial data formats called Geospatial Data Abstraction Library (GDAL)\n\n\n\n\n\n\nImages from Visible Infrared Imaging Radiometer Suite (VIIRS) which provides publicly accessible earth observation images at night for the entire globe.\nData processing is required to ensure consistency of the resolution of night light data with the daytime satellite imagery in preparation for the CNN modelling\nFor a more effective training of the CNN model, actual values of intensity of lights were batched into discrete groups. Similarly, a Gaussian mixture model (GMM) for clustering the values of night light intensity was applied. The GMM assumes that the night light intensity distribution comes from the mixture of k underlying Gaussian or Normal distributions. A histogram of the radiance values was evaluated to arrive at the set of Normal distributions that best fit the data"
  },
  {
    "objectID": "about_me/why_me.html",
    "href": "about_me/why_me.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "I deeply care for eradication of poverty\nI am passionate about eradicating poverty from a long time\n\n\n\n\nThe challenges of Global Poverty\n\n\n\nThe International Social Service (ISS) is an international NGO founded in 1924 which assists children and families confronted with complex social problems as a result of migration. This project was undertaken in collaboration with 30 data scientists around the world.\n\n\n\n\nA Goodwill project I did for The International Social Service\n\n\n\n\n\n\nI love the values of the company like Recipients first, team next, create positive energy, Think rigorously and act quickly, Be productively ambitious\nI care about ethics while handling data and doing predictions. A blog post I wrote on Ethics and Bias in Data Science and How to Mitigate Bias\n\n\n\n\n\nI have the requisite skills\nI can pick up new skills fast in case a gap exists\n\n\n\n\n\n\n\nI am a team player\n\n\n\nAn example - I initiated and built a data discovery platform understanding the difficulties faced by other data scientists in the team\nI initiated and built a automated system for video moderation and tagging after knowing about the challenges faced by a different team in the company\n\n\n\n\n\nAfter serving in military for 20 years, I pivoted to a corporate environment and a different industry\nI love making small upgrades on a daily basis\n\n\n\nI upgrade my version everyday\n\n\n\n\n\n\n\nBooks on ML I read the previous year \nCourse I am doing currently\n\n\n\n\n\n\n\n\n\nIf you get lost, call HR"
  },
  {
    "objectID": "about_me/project_portfolio.html",
    "href": "about_me/project_portfolio.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Why\n\nAt ZEE5, on average 20% of paid subscribers were churning every month and the company wanted to reduce the churn and retain the customers.\n\nRole\n\nAs a Data Science scientist, my target was to reduce the churn of customers from 20% to 17% in six months.\n\nAction\n\nWe started with talking with different teams and understanding their perspective on churn, gathering as much domain knowledge as possible.\nAnalyzing the patterns of churn and searching for the various data sources available. Talking to different data owners and understanding the data\n\nExploratory data analysis\nFeature engineering\nCreating a model to predict churn\nEvaluating results\nModel deployment\nMeasuring the impact of the predictions and making changes for improvement\n\nResult\n\nWe are able to reduce churn by 4%, there by increasing the customer rentention and revenue for the organisation.\n\nTools Used\n\nAWS S3, Athena, AWS Glue, EC2, ECR\nDatabricks\nSisense / Tableau for Dashboards\n\nRelavance\n\nModelling using Machine learning\nData regarding customers communication with help desk and location was considered for modelling\n\nModel monitoring and Dashboarding\n\n\n\n\nwhy\n\nAt Lomotif, we wanted to keep the platform clean and safe to increase the satisfaction of all our customers. We wanted to increase their enjoyment by recommending videos which users will prefer and are difficult to discover on the platform due to plethora of choices.\n\nRole\n\nAs a Data Science manager, my target was to blacklist the NSFW content and reduce the workload of manual moderators by 60%. Tag the videos and improve the number of video views and watch time by 5%\n\nAction\n\nBrainstorming what is meant by NSFW, reading the content moderation guidelines\nUnderstanding what should not be published on the platform.\nWe created computer vision models and also used open source models to detect NSFW and category of the content.\n\nEvaluate and deploy the models\nUsed an open source tool Fiftyone to automate samping videos, getting the ground truth, calculating the ML metrics and publishing on the dashboard.\n\n\n\n\nVisualizing Results in Fiftyone\n\n\nResult\n\nWe reduced the effort of moderation and tagging by more than 80%. Increased the watchtime and video views on average by 3%\n\nTools Used\n\nSnowflake, EC2, Ray Serve, Grafana, Fiftyone\n\nRelavance\n\nUsing my computer vision skills for analyzing and classifying satellite images\nUsing Fiftyone skills to automate the operations of analyzing images\nModel deployment and monitoring\n\n\n\n\nWhy\n\nAt Eyeota, partner companies have the data for survey participants. The number of survey participants are very less. Find the customers in our company’s database who are similar to survery participants for ad targeting and monetisation.\n\nRole\n\nAs a Data science manager, develop a framework for data integration with partners and automation the pipeline for building look-alike models, segment the customers and send the data for campaigning.\n\nAction\n\nCo-ordination with different teams for data integration\nDesigning a framework for data ingestion, feature engineering, model creation, segmenting the customers and evaluation of results\n\nResult\n\nCreating a framework to automate data ingestion, creation of hundereds of look-alike models and segmenting the customers. Using the framework for integration with different partners. Increasing the campaign revenue by more than 2%\n\nTools Used\n\nAWS S3, Glue, Sagemaker, EMR, EC2, Mlflow\nSnowflake\nAirflow\nApache Superset for Dashboarding\n\nRelavance\n\nAutomating deployment of hundreds of models\nCreating Datapipelines\n\n\n\n\nWhy\n\nAt Upgrad, student enrollment for a course is high, if he is informed in advance the placement outcomes of the course.\n\nRole\n\nAs a Data scientist, I need to create a model which will help increase the enrollment by 10%\n\nAction\n\nCollecting the data on student’s past educational performance, work experience etc.\nCreating and evaluating the model\n\nResult\n\nStudent enrollment increased 12% when they were informed in advance their placement outcomes\n\nTools Used\n\nAWS & Streamlit\n\nRelavance\n\nCreating UI for the end-users to interact with the model\n\n\n\n\nWhy\n\nAt Lomotif, enabling content discovery and providing informed choices for content consumption as per the user preferences\n\nRole\n\nAs a Data Science manager, my role is to design a framework for recommendation engines, executing the project and evaluating the results. Recommendation engines should increase the video views and watch time by 5%\n\nAction\n\nDesigning a recommendation system based on candidate retrieval and ranking system\nCreating the pipelines for data processing\nExecution of candidte retrieval using elasticsearch\nModeling and ranking the videos for the user\nEvaluation of the recommendation system\n\nResult\n\nWe are currently deployment stage\n\nTools Used\n\nElasticsearch, FastAPI, AWS, Snowflake\n\nRelavance\n\nImplementation of recommendation engines\nCreating and providing APIs\n\n\n\n\n\n\n\nwhy\n\nAt ZEE5, data is generated from numerous tools which is used by different departments in an organisation. Accessing data generated by different tools was becoming difficult and different numbers were being reported at different levels.\n\nRole\n\nAs a Data Science manager my role was to create a single point of truth for the data. Automate the data processing requirements of the organization by 50% and reduce the infrastructure cost by 30%\n\nAction\n\nCollecting all the data generated by different tools at a central location.\nDesigning a framework for automation, cleaning and processing of the data.\nScheduling of workflows and creating alerts for failures\nCreating different marts for the data\n\nBronze tables - Cleaned and transformed data\nSilver tables - Joining different tables, calculating and storing the features required for analysis and different machine learning models\nGold tables - Calculating and storing metrics for business reporting and dashboards\n\nSetting up of expectations for data quality, monitoring of validations and alerts in case of any discrepancy.\n\n\n\n\nEnsuring Data Quality with Great Expectations\n\n\nResult\n\nAchieved more than 50% of organisations data processing requirements and automation\n\nTools Used\n\nAWS S3, EMR, Athena, Glue, Databricks, Pyspark, Hive metastore, Airflow etc\nGreat Expectations for monitoring Data quality\nDashboards for monitoring the data processed and results of quality checks\n\nRelavance\n\nParallel processing huge amounts of data using Databricks\nFramework for measuring and monitoring data quality\n\n\n\n\nwhy\n\nData Scientists in the organization was facing challenges to know about existing data sources, understanding their definition and meaning. Knowledge existed in silos as the information generated by one data scientist was not shared with others\n\nRole\n\nTo create a interactive platform where information about data sources are searchable, definitions of features available. Creating a knowledge repository to share all the analysis and reports.\n\nAction\n\nSearch for a data discovery platform which can easily integrate with AWS\nStarting and executing a POC for a data discovery platform\nProviding access to the platform to all data scientists in the organization and collecting feedback.\nImplementation in production\n\nResult\n\nIncrease in work satisfaction and productivity of all the data scientists in the organization.\n\n\n\n\nData discovery using Quiltdata\n\n\nTools Used\n\nQuilt, Elasticsearch\n\nRelavance\n\nCreation of a data discovery platform to know and understand all data sources\nKnowledge repository for the organization\n\n\n\n\n\n\n\nWhy\nAt ZEE5, Business teams wanted to see a few metrics with dynamic date ranges which were extremely difficult to show on the dashboards. An example, unique users on the platform with a dynamic data range.\nRole\nDo a POC on how to show these metrics on the dashboard which can be rendered very fast and are computationally cheap.\nAction\n\nConducting a study on how to process the data at scale to calculate these metrics and provide the information to the dashboard quickly\nReading about sketching algorithms which can provide approximate results very quickly\nUndertaking a POC\nCreating a dashboard and presenting the results\n\nResult\n\nAble to provide metrics required by the business team on the dashboard for a dynamic date range\n\n\n\n\nCreating Dashboard on raw data using Dremio\n\n\nTools Used\n\nDremio, Druid, Apache Superset\n\nRelavance\n\nBuilding dashboards\nLarge scale data processing and sketching algorithms for processing huge amount of data and showing the metrics on the dashboards"
  },
  {
    "objectID": "about_me/general_info.html",
    "href": "about_me/general_info.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "I got 7 years experience as a Data Science Manager and Data Scientist. In my career I worked on different data science projects like Building a Recommendation engine for a social media app, building look-a-like models and deploying it on scale (100’s of them), Content moderation and tagging using computer vision, building churn prediction models, creating a data lake, building a data discovery platform, automating dashboards etc. I have skills in various data science topics like Computer vision, NLP, Interpretable Machine learning, Causal Inference, Probabilistic Programming, Privacy preserving ML among other things.\nAttributes and qualities * Perception about the business challenges and the requirements of the clients * Team player - Striving for the success of the team as a whole * Mentoring the junior members of the team * Open communication to get ideas from all the members on the team and implementation based on rational evaluation * Willingness to learn whatever I don’t know quickly * Updating my beliefs in life based on evidence"
  },
  {
    "objectID": "private/questions.html",
    "href": "private/questions.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Difficult project previously managed\nDetails on how you’ve project-managed multiple, competing priorities\nWhat do I think “poverty” means\nWhat is your experience with poverty\nDealt with outliers?\nLasso regression\nData Encoding\nFeature selection\nGeospatial data\nHow to handle Datelines ?? There seems to be a typo here\n\n\n\n\nQuestions"
  },
  {
    "objectID": "telecom_churn_prediction/telecom_churn_prediction.html",
    "href": "telecom_churn_prediction/telecom_churn_prediction.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Telecom Churn Prediction\nThe objectives of this project are:-\n1. Perform exploratory analysis and extract insights from the dataset.\n2. Split the dataset into train/test sets and explain your reasoning.\n3. Build a predictive model to predict which customers are going to churn and discuss the reason why you choose a particular algorithm.\n4. Establish metrics to evaluate model performance.\n5. Discuss the potential issues with deploying the model into production\n\nImport the required libraries\n\n# python version # 3.8.2\nimport pandas as pd \nimport numpy as np \nfrom pandas_profiling import ProfileReport \nfrom pycaret.classification import * \nfrom sklearn import metrics \nimport os \nfrom sklearn.model_selection import train_test_split \n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# option to display all columns\npd.set_option('display.max_columns', None)\n\n\n# Read the data\ntelecom_churn = pd.read_csv('data science challenge.csv')\n\n\ntelecom_churn.head(10)\n\n\n\n\n\n  \n    \n      \n      state\n      account length\n      area code\n      phone number\n      international plan\n      voice mail plan\n      number vmail messages\n      total day minutes\n      total day calls\n      total day charge\n      total eve minutes\n      total eve calls\n      total eve charge\n      total night minutes\n      total night calls\n      total night charge\n      total intl minutes\n      total intl calls\n      total intl charge\n      customer service calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      5\n      AL\n      118\n      510\n      391-8027\n      yes\n      no\n      0\n      223.4\n      98\n      37.98\n      220.6\n      101\n      18.75\n      203.9\n      118\n      9.18\n      6.3\n      6\n      1.70\n      0\n      False\n    \n    \n      6\n      MA\n      121\n      510\n      355-9993\n      no\n      yes\n      24\n      218.2\n      88\n      37.09\n      348.5\n      108\n      29.62\n      212.6\n      118\n      9.57\n      7.5\n      7\n      2.03\n      3\n      False\n    \n    \n      7\n      MO\n      147\n      415\n      329-9001\n      yes\n      no\n      0\n      157.0\n      79\n      26.69\n      103.1\n      94\n      8.76\n      211.8\n      96\n      9.53\n      7.1\n      6\n      1.92\n      0\n      False\n    \n    \n      8\n      LA\n      117\n      408\n      335-4719\n      no\n      no\n      0\n      184.5\n      97\n      31.37\n      351.6\n      80\n      29.89\n      215.8\n      90\n      9.71\n      8.7\n      4\n      2.35\n      1\n      False\n    \n    \n      9\n      WV\n      141\n      415\n      330-8173\n      yes\n      yes\n      37\n      258.6\n      84\n      43.96\n      222.0\n      111\n      18.87\n      326.4\n      97\n      14.69\n      11.2\n      5\n      3.02\n      0\n      False\n    \n  \n\n\n\n\n\n\nCheck the Shape and Column types of the Dataframe\n\ntelecom_churn.shape\n\n(3333, 21)\n\n\n\ntelecom_churn.dtypes\n\nstate                      object\naccount length              int64\narea code                   int64\nphone number               object\ninternational plan         object\nvoice mail plan            object\nnumber vmail messages       int64\ntotal day minutes         float64\ntotal day calls             int64\ntotal day charge          float64\ntotal eve minutes         float64\ntotal eve calls             int64\ntotal eve charge          float64\ntotal night minutes       float64\ntotal night calls           int64\ntotal night charge        float64\ntotal intl minutes        float64\ntotal intl calls            int64\ntotal intl charge         float64\ncustomer service calls      int64\nchurn                        bool\ndtype: object\n\n\n\n\nExploratory Analysis\n\n# No missing values in the data.\n# Scaling of numeric columns is required\ntelecom_churn.describe()\n\n\n\n\n\n  \n    \n      \n      account length\n      area code\n      number vmail messages\n      total day minutes\n      total day calls\n      total day charge\n      total eve minutes\n      total eve calls\n      total eve charge\n      total night minutes\n      total night calls\n      total night charge\n      total intl minutes\n      total intl calls\n      total intl charge\n      customer service calls\n    \n  \n  \n    \n      count\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n    \n    \n      mean\n      101.064806\n      437.182418\n      8.099010\n      179.775098\n      100.435644\n      30.562307\n      200.980348\n      100.114311\n      17.083540\n      200.872037\n      100.107711\n      9.039325\n      10.237294\n      4.479448\n      2.764581\n      1.562856\n    \n    \n      std\n      39.822106\n      42.371290\n      13.688365\n      54.467389\n      20.069084\n      9.259435\n      50.713844\n      19.922625\n      4.310668\n      50.573847\n      19.568609\n      2.275873\n      2.791840\n      2.461214\n      0.753773\n      1.315491\n    \n    \n      min\n      1.000000\n      408.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      23.200000\n      33.000000\n      1.040000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      74.000000\n      408.000000\n      0.000000\n      143.700000\n      87.000000\n      24.430000\n      166.600000\n      87.000000\n      14.160000\n      167.000000\n      87.000000\n      7.520000\n      8.500000\n      3.000000\n      2.300000\n      1.000000\n    \n    \n      50%\n      101.000000\n      415.000000\n      0.000000\n      179.400000\n      101.000000\n      30.500000\n      201.400000\n      100.000000\n      17.120000\n      201.200000\n      100.000000\n      9.050000\n      10.300000\n      4.000000\n      2.780000\n      1.000000\n    \n    \n      75%\n      127.000000\n      510.000000\n      20.000000\n      216.400000\n      114.000000\n      36.790000\n      235.300000\n      114.000000\n      20.000000\n      235.300000\n      113.000000\n      10.590000\n      12.100000\n      6.000000\n      3.270000\n      2.000000\n    \n    \n      max\n      243.000000\n      510.000000\n      51.000000\n      350.800000\n      165.000000\n      59.640000\n      363.700000\n      170.000000\n      30.910000\n      395.000000\n      175.000000\n      17.770000\n      20.000000\n      20.000000\n      5.400000\n      9.000000\n    \n  \n\n\n\n\n\n# Format the column names, remove space and special characters in column names\ntelecom_churn.columns =  telecom_churn.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n\n\ntelecom_churn\n\n\n\n\n\n  \n    \n      \n      state\n      account_length\n      area_code\n      phone_number\n      international_plan\n      voice_mail_plan\n      number_vmail_messages\n      total_day_minutes\n      total_day_calls\n      total_day_charge\n      total_eve_minutes\n      total_eve_calls\n      total_eve_charge\n      total_night_minutes\n      total_night_calls\n      total_night_charge\n      total_intl_minutes\n      total_intl_calls\n      total_intl_charge\n      customer_service_calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3328\n      AZ\n      192\n      415\n      414-4276\n      no\n      yes\n      36\n      156.2\n      77\n      26.55\n      215.5\n      126\n      18.32\n      279.1\n      83\n      12.56\n      9.9\n      6\n      2.67\n      2\n      False\n    \n    \n      3329\n      WV\n      68\n      415\n      370-3271\n      no\n      no\n      0\n      231.1\n      57\n      39.29\n      153.4\n      55\n      13.04\n      191.3\n      123\n      8.61\n      9.6\n      4\n      2.59\n      3\n      False\n    \n    \n      3330\n      RI\n      28\n      510\n      328-8230\n      no\n      no\n      0\n      180.8\n      109\n      30.74\n      288.8\n      58\n      24.55\n      191.9\n      91\n      8.64\n      14.1\n      6\n      3.81\n      2\n      False\n    \n    \n      3331\n      CT\n      184\n      510\n      364-6381\n      yes\n      no\n      0\n      213.8\n      105\n      36.35\n      159.6\n      84\n      13.57\n      139.2\n      137\n      6.26\n      5.0\n      10\n      1.35\n      2\n      False\n    \n    \n      3332\n      TN\n      74\n      415\n      400-4344\n      no\n      yes\n      25\n      234.4\n      113\n      39.85\n      265.9\n      82\n      22.60\n      241.4\n      77\n      10.86\n      13.7\n      4\n      3.70\n      0\n      False\n    \n  \n\n3333 rows × 21 columns\n\n\n\n\n#telecom_churn[\"area_code\"] = telecom_churn[\"area_code\"].astype('category')\n\n\nprofile = ProfileReport(telecom_churn, title = \"Telecom Churn Report\")\n\n\n# create report for EDA\nprofile.to_widgets()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#save the profile report\nprofile.to_file(\"telecom_churn_eda.html\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[telecom_churn.churn.value_counts()]\n\n[False    2850\n True      483\n Name: churn, dtype: int64]\n\n\n\npd.crosstab(telecom_churn.churn, telecom_churn. customer_service_calls,margins=True, margins_name=\"Total\")\n\n\n\n\n\n  \n    \n      customer_service_calls\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      Total\n    \n    \n      churn\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      False\n      605\n      1059\n      672\n      385\n      90\n      26\n      8\n      4\n      1\n      0\n      2850\n    \n    \n      True\n      92\n      122\n      87\n      44\n      76\n      40\n      14\n      5\n      1\n      2\n      483\n    \n    \n      Total\n      697\n      1181\n      759\n      429\n      166\n      66\n      22\n      9\n      2\n      2\n      3333\n    \n  \n\n\n\n\n\nimport matplotlib.pyplot as plt\nct = pd.crosstab(telecom_churn.churn, telecom_churn.customer_service_calls)\nct.plot.bar(stacked=True)\nplt.legend(title='churn vs Number of calls')\nplt.show()\n\n\n\n\n\ntelecom_churn['area_code'].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntelecom_churn['state'].value_counts().head(10).plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntelecom_churn['international_plan'].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntelecom_churn['voice_mail_plan'].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nObservations from EDA are:-\n\nDataset is imbalanced - 85.5% customers did not churn and 14.5% customers churned\nState consist of 51 distinct values with high cardinality\nNumeric variables are in different ranges and needs to be scaled\nThree distinct area codes - Area code ‘415’ is 49.7%, rest two area codes are equally distributed\nDistinct values of phone number is equal to the length of the dataset. This will be equivalent to primary key of the dataset. Not be included in modelling\n72.3% customers did not activate their voicemail plan. This is verified by equal number of customers with zero number of voice messages\nTotal international calls and customer service calls data is skewed. This is verified by high kurtosis and skewness.\nAll the other numeric variables are following normal distribution as verified by kurtosis / skewness values and histogram\n\n\n\nSplit the Data for Training and Testing\nThe Machine learning algorithm should not be exposed to test data. The performance of the learning algorithm can only be measured by testing on unseen data. To achieve the same a train and test split with 95% and 5% is created. It is ensured that the sampling is stratified so that the proportion of churn and not churn customers are equal in train and test data. As the amount of data is very less, only 5% of the data is kept aside for testing.Further the train data is further split into train and validation set with 90% and 10%. Validation set is required for hyperparameter tuning.As validation set is also exposed to training algorithm, it is also should not be used for model validation. Model validation is done on test set only.\n\n# convert the target value to integers. \ntelecom_churn['churn'] = telecom_churn['churn'] * 1\n\n\ntrain, test = train_test_split(telecom_churn, test_size = 0.05, stratify = telecom_churn['churn']) \nprint('Data for Modeling: ' + str(train.shape))\nprint('Unseen Data For Predictions: ' + str(test.shape))\n\nData for Modeling: (3166, 21)\nUnseen Data For Predictions: (167, 21)\n\n\n\n# Test the proportion of churn in train and test sets\ntrain.churn.value_counts()\n\n0    2707\n1     459\nName: churn, dtype: int64\n\n\n\n# 16.5% of the customers churned in train data\n(459/2707)*100\n\n16.956039896564462\n\n\n\ntest.churn.value_counts()\n\n0    143\n1     24\nName: churn, dtype: int64\n\n\n\n# 16.7% of the customers churned in test data\n(24/143)*100\n# customers churned proportionally from train and test data\n\n16.783216783216783\n\n\n\n\nModelling with Pycaret\n\nTrain and validation sets are created with 90 % and 10 % data.\nThe random seed selected for the modeling is 786\nIn this step we are normalizing the data, ignoring the variable ‘phone number’ for analysis\nFixing the imbalance in the data using SMOTE method\nWe are transforming the features - Changing the distribution of variables to a normal or approximate normal distribution\nIgnorning features with low variance - This will ignore variables (multi level categorical) where a single level dominates and there is not much variation in the information provided by the feature\nThe setup is inferring the customer_service_calls as numeric (as there are only ten distinct values). Hence explicitly mentioning it as numeric\n\n\nexp_clf =    setup(data = train, target = 'churn', session_id = 786, \n                   train_size = 0.9,\n                   normalize = True,\n                   transformation = True,\n                   ignore_low_variance = True,               \n                   ignore_features = ['phone_number'],\n                   fix_imbalance = True,\n                   high_cardinality_features = ['state'],\n                   numeric_features = ['customer_service_calls'])               \n\nSetup Succesfully Completed!\n\n\n\n                    Description        Value    \n                \n                        0\n                        session_id\n                        786\n            \n            \n                        1\n                        Target Type\n                        Binary\n            \n            \n                        2\n                        Label Encoded\n                        None\n            \n            \n                        3\n                        Original Data\n                        (3166, 21)\n            \n            \n                        4\n                        Missing Values \n                        False\n            \n            \n                        5\n                        Numeric Features \n                        15\n            \n            \n                        6\n                        Categorical Features \n                        5\n            \n            \n                        7\n                        Ordinal Features \n                        False\n            \n            \n                        8\n                        High Cardinality Features \n                        True\n            \n            \n                        9\n                        High Cardinality Method \n                        frequency\n            \n            \n                        10\n                        Sampled Data\n                        (3166, 21)\n            \n            \n                        11\n                        Transformed Train Set\n                        (2849, 22)\n            \n            \n                        12\n                        Transformed Test Set\n                        (317, 22)\n            \n            \n                        13\n                        Numeric Imputer \n                        mean\n            \n            \n                        14\n                        Categorical Imputer \n                        constant\n            \n            \n                        15\n                        Normalize \n                        True\n            \n            \n                        16\n                        Normalize Method \n                        zscore\n            \n            \n                        17\n                        Transformation \n                        True\n            \n            \n                        18\n                        Transformation Method \n                        yeo-johnson\n            \n            \n                        19\n                        PCA \n                        False\n            \n            \n                        20\n                        PCA Method \n                        None\n            \n            \n                        21\n                        PCA Components \n                        None\n            \n            \n                        22\n                        Ignore Low Variance \n                        True\n            \n            \n                        23\n                        Combine Rare Levels \n                        False\n            \n            \n                        24\n                        Rare Level Threshold \n                        None\n            \n            \n                        25\n                        Numeric Binning \n                        False\n            \n            \n                        26\n                        Remove Outliers \n                        False\n            \n            \n                        27\n                        Outliers Threshold \n                        None\n            \n            \n                        28\n                        Remove Multicollinearity \n                        False\n            \n            \n                        29\n                        Multicollinearity Threshold \n                        None\n            \n            \n                        30\n                        Clustering \n                        False\n            \n            \n                        31\n                        Clustering Iteration \n                        None\n            \n            \n                        32\n                        Polynomial Features \n                        False\n            \n            \n                        33\n                        Polynomial Degree \n                        None\n            \n            \n                        34\n                        Trignometry Features \n                        False\n            \n            \n                        35\n                        Polynomial Threshold \n                        None\n            \n            \n                        36\n                        Group Features \n                        False\n            \n            \n                        37\n                        Feature Selection \n                        False\n            \n            \n                        38\n                        Features Selection Threshold \n                        None\n            \n            \n                        39\n                        Feature Interaction \n                        False\n            \n            \n                        40\n                        Feature Ratio \n                        False\n            \n            \n                        41\n                        Interaction Threshold \n                        None\n            \n            \n                        42\n                        Fix Imbalance\n                        True\n            \n            \n                        43\n                        Fix Imbalance Method\n                        SMOTE\n            \n    \n\n\n\ncompare_models(fold = 5)\n\n\n                    Model        Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC        TT (Sec)    \n                \n                        0\n                        Light Gradient Boosting Machine\n                        0.9512\n                        0.9127\n                        0.7771\n                        0.8732\n                        0.8213\n                        0.7932\n                        0.7957\n                        0.1908\n            \n            \n                        1\n                        Extreme Gradient Boosting\n                        0.9488\n                        0.9145\n                        0.7675\n                        0.8652\n                        0.8124\n                        0.7829\n                        0.7854\n                        0.2130\n            \n            \n                        2\n                        CatBoost Classifier\n                        0.9466\n                        0.9140\n                        0.7577\n                        0.8583\n                        0.8041\n                        0.7734\n                        0.7759\n                        4.3314\n            \n            \n                        3\n                        Extra Trees Classifier\n                        0.9333\n                        0.9032\n                        0.6535\n                        0.8544\n                        0.7392\n                        0.7018\n                        0.7109\n                        0.1456\n            \n            \n                        4\n                        Random Forest Classifier\n                        0.9330\n                        0.9082\n                        0.7045\n                        0.8147\n                        0.7529\n                        0.7145\n                        0.7186\n                        0.0366\n            \n            \n                        5\n                        Gradient Boosting Classifier\n                        0.9308\n                        0.9078\n                        0.7674\n                        0.7618\n                        0.7635\n                        0.7230\n                        0.7238\n                        1.1674\n            \n            \n                        6\n                        Decision Tree Classifier\n                        0.8929\n                        0.8227\n                        0.7238\n                        0.6106\n                        0.6616\n                        0.5986\n                        0.6022\n                        0.0356\n            \n            \n                        7\n                        Ada Boost Classifier\n                        0.8645\n                        0.8464\n                        0.6003\n                        0.5325\n                        0.5625\n                        0.4829\n                        0.4853\n                        0.2996\n            \n            \n                        8\n                        Naive Bayes\n                        0.8284\n                        0.7894\n                        0.6439\n                        0.4430\n                        0.5233\n                        0.4237\n                        0.4355\n                        0.0028\n            \n            \n                        9\n                        K Neighbors Classifier\n                        0.7715\n                        0.7897\n                        0.6878\n                        0.3531\n                        0.4664\n                        0.3398\n                        0.3705\n                        0.0160\n            \n            \n                        10\n                        Logistic Regression\n                        0.7375\n                        0.7886\n                        0.6852\n                        0.3152\n                        0.4314\n                        0.2904\n                        0.3273\n                        0.0316\n            \n            \n                        11\n                        Ridge Classifier\n                        0.7371\n                        0.0000\n                        0.6852\n                        0.3149\n                        0.4311\n                        0.2899\n                        0.3270\n                        0.0082\n            \n            \n                        12\n                        Linear Discriminant Analysis\n                        0.7336\n                        0.7844\n                        0.6779\n                        0.3103\n                        0.4252\n                        0.2824\n                        0.3190\n                        0.0144\n            \n            \n                        13\n                        SVM - Linear Kernel\n                        0.7329\n                        0.0000\n                        0.6079\n                        0.3163\n                        0.4021\n                        0.2621\n                        0.2910\n                        0.0198\n            \n            \n                        14\n                        Quadratic Discriminant Analysis\n                        0.5907\n                        0.6505\n                        0.6373\n                        0.2089\n                        0.3106\n                        0.1201\n                        0.1596\n                        0.0110\n            \n    \n\n\nLGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=786, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n\n\n\n\nCreating the models for top performing algorithms based on Precision and AUC. Tree based models are performing well on this dataset\n\nlightgbm = create_model('lightgbm', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9526\n                        0.9008\n                        0.8171\n                        0.8481\n                        0.8323\n                        0.8047\n                        0.8049\n            \n            \n                        1\n                        0.9561\n                        0.9271\n                        0.8193\n                        0.8718\n                        0.8447\n                        0.8192\n                        0.8197\n            \n            \n                        2\n                        0.9544\n                        0.9241\n                        0.8072\n                        0.8701\n                        0.8375\n                        0.8110\n                        0.8118\n            \n            \n                        3\n                        0.9544\n                        0.9278\n                        0.7470\n                        0.9254\n                        0.8267\n                        0.8007\n                        0.8068\n            \n            \n                        4\n                        0.9385\n                        0.8836\n                        0.6951\n                        0.8507\n                        0.7651\n                        0.7301\n                        0.7351\n            \n            \n                        Mean\n                        0.9512\n                        0.9127\n                        0.7771\n                        0.8732\n                        0.8213\n                        0.7932\n                        0.7957\n            \n            \n                        SD\n                        0.0065\n                        0.0176\n                        0.0488\n                        0.0278\n                        0.0287\n                        0.0321\n                        0.0307\n            \n    \n\n\n\ncatboost = create_model('catboost', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9421\n                        0.9161\n                        0.7439\n                        0.8356\n                        0.7871\n                        0.7537\n                        0.7554\n            \n            \n                        1\n                        0.9456\n                        0.9254\n                        0.7711\n                        0.8421\n                        0.8050\n                        0.7735\n                        0.7745\n            \n            \n                        2\n                        0.9561\n                        0.9198\n                        0.8313\n                        0.8625\n                        0.8466\n                        0.8210\n                        0.8212\n            \n            \n                        3\n                        0.9544\n                        0.9314\n                        0.7470\n                        0.9254\n                        0.8267\n                        0.8007\n                        0.8068\n            \n            \n                        4\n                        0.9350\n                        0.8775\n                        0.6951\n                        0.8261\n                        0.7550\n                        0.7178\n                        0.7214\n            \n            \n                        Mean\n                        0.9466\n                        0.9140\n                        0.7577\n                        0.8583\n                        0.8041\n                        0.7734\n                        0.7759\n            \n            \n                        SD\n                        0.0078\n                        0.0190\n                        0.0443\n                        0.0356\n                        0.0317\n                        0.0360\n                        0.0358\n            \n    \n\n\n\nxgboost = create_model('xgboost', fold = 5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9474\n                        0.9080\n                        0.8171\n                        0.8171\n                        0.8171\n                        0.7863\n                        0.7863\n            \n            \n                        1\n                        0.9561\n                        0.9270\n                        0.8072\n                        0.8816\n                        0.8428\n                        0.8173\n                        0.8184\n            \n            \n                        2\n                        0.9474\n                        0.9231\n                        0.7590\n                        0.8630\n                        0.8077\n                        0.7773\n                        0.7795\n            \n            \n                        3\n                        0.9509\n                        0.9393\n                        0.7470\n                        0.8986\n                        0.8158\n                        0.7877\n                        0.7922\n            \n            \n                        4\n                        0.9420\n                        0.8752\n                        0.7073\n                        0.8657\n                        0.7785\n                        0.7455\n                        0.7506\n            \n            \n                        Mean\n                        0.9488\n                        0.9145\n                        0.7675\n                        0.8652\n                        0.8124\n                        0.7829\n                        0.7854\n            \n            \n                        SD\n                        0.0047\n                        0.0220\n                        0.0404\n                        0.0272\n                        0.0206\n                        0.0230\n                        0.0218\n            \n    \n\n\n\nrf = create_model('rf', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9298\n                        0.8986\n                        0.7317\n                        0.7692\n                        0.7500\n                        0.7092\n                        0.7095\n            \n            \n                        1\n                        0.9263\n                        0.9227\n                        0.7349\n                        0.7531\n                        0.7439\n                        0.7009\n                        0.7009\n            \n            \n                        2\n                        0.9281\n                        0.9091\n                        0.6867\n                        0.7917\n                        0.7355\n                        0.6941\n                        0.6965\n            \n            \n                        3\n                        0.9421\n                        0.9291\n                        0.7349\n                        0.8472\n                        0.7871\n                        0.7538\n                        0.7563\n            \n            \n                        4\n                        0.9385\n                        0.8817\n                        0.6341\n                        0.9123\n                        0.7482\n                        0.7145\n                        0.7298\n            \n            \n                        Mean\n                        0.9330\n                        0.9082\n                        0.7045\n                        0.8147\n                        0.7529\n                        0.7145\n                        0.7186\n            \n            \n                        SD\n                        0.0062\n                        0.0170\n                        0.0396\n                        0.0583\n                        0.0178\n                        0.0208\n                        0.0221\n            \n    \n\n\n\net = create_model('et', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9175\n                        0.8940\n                        0.6463\n                        0.7465\n                        0.6928\n                        0.6455\n                        0.6477\n            \n            \n                        1\n                        0.9404\n                        0.9064\n                        0.6988\n                        0.8657\n                        0.7733\n                        0.7394\n                        0.7451\n            \n            \n                        2\n                        0.9404\n                        0.9096\n                        0.6747\n                        0.8889\n                        0.7671\n                        0.7337\n                        0.7428\n            \n            \n                        3\n                        0.9456\n                        0.9307\n                        0.6867\n                        0.9194\n                        0.7862\n                        0.7558\n                        0.7664\n            \n            \n                        4\n                        0.9227\n                        0.8754\n                        0.5610\n                        0.8519\n                        0.6765\n                        0.6347\n                        0.6525\n            \n            \n                        Mean\n                        0.9333\n                        0.9032\n                        0.6535\n                        0.8544\n                        0.7392\n                        0.7018\n                        0.7109\n            \n            \n                        SD\n                        0.0111\n                        0.0183\n                        0.0494\n                        0.0586\n                        0.0453\n                        0.0510\n                        0.0503\n            \n    \n\n\n\n\nTune the created models for selecting the best hyperparameters\n\ntuned_lightgbm = tune_model(lightgbm, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9404\n                        0.9034\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        1\n                        0.9439\n                        0.8894\n                        0.8049\n                        0.8049\n                        0.8049\n                        0.7721\n                        0.7721\n            \n            \n                        2\n                        0.9579\n                        0.9525\n                        0.8537\n                        0.8537\n                        0.8537\n                        0.8291\n                        0.8291\n            \n            \n                        3\n                        0.9649\n                        0.9000\n                        0.8049\n                        0.9429\n                        0.8684\n                        0.8483\n                        0.8519\n            \n            \n                        4\n                        0.9474\n                        0.9161\n                        0.8049\n                        0.8250\n                        0.8148\n                        0.7841\n                        0.7842\n            \n            \n                        5\n                        0.9614\n                        0.9198\n                        0.8049\n                        0.9167\n                        0.8571\n                        0.8349\n                        0.8373\n            \n            \n                        6\n                        0.9684\n                        0.9594\n                        0.8810\n                        0.9024\n                        0.8916\n                        0.8731\n                        0.8732\n            \n            \n                        7\n                        0.9649\n                        0.9162\n                        0.7619\n                        1.0000\n                        0.8649\n                        0.8451\n                        0.8554\n            \n            \n                        8\n                        0.9333\n                        0.8673\n                        0.7381\n                        0.7949\n                        0.7654\n                        0.7266\n                        0.7273\n            \n            \n                        9\n                        0.9437\n                        0.9092\n                        0.6829\n                        0.9032\n                        0.7778\n                        0.7462\n                        0.7558\n            \n            \n                        Mean\n                        0.9526\n                        0.9134\n                        0.7918\n                        0.8744\n                        0.8289\n                        0.8015\n                        0.8042\n            \n            \n                        SD\n                        0.0117\n                        0.0259\n                        0.0531\n                        0.0659\n                        0.0414\n                        0.0480\n                        0.0484\n            \n    \n\n\n\ntuned_catboost = tune_model(catboost, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9368\n                        0.8919\n                        0.7317\n                        0.8108\n                        0.7692\n                        0.7328\n                        0.7341\n            \n            \n                        1\n                        0.9474\n                        0.9042\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        2\n                        0.9509\n                        0.9510\n                        0.8049\n                        0.8462\n                        0.8250\n                        0.7964\n                        0.7968\n            \n            \n                        3\n                        0.9544\n                        0.8872\n                        0.7561\n                        0.9118\n                        0.8267\n                        0.8007\n                        0.8053\n            \n            \n                        4\n                        0.9474\n                        0.9258\n                        0.8049\n                        0.8250\n                        0.8148\n                        0.7841\n                        0.7842\n            \n            \n                        5\n                        0.9544\n                        0.9058\n                        0.8049\n                        0.8684\n                        0.8354\n                        0.8090\n                        0.8098\n            \n            \n                        6\n                        0.9579\n                        0.9607\n                        0.7619\n                        0.9412\n                        0.8421\n                        0.8181\n                        0.8242\n            \n            \n                        7\n                        0.9439\n                        0.9133\n                        0.6667\n                        0.9333\n                        0.7778\n                        0.7467\n                        0.7605\n            \n            \n                        8\n                        0.9193\n                        0.8823\n                        0.7143\n                        0.7317\n                        0.7229\n                        0.6757\n                        0.6757\n            \n            \n                        9\n                        0.9437\n                        0.8767\n                        0.7073\n                        0.8788\n                        0.7838\n                        0.7518\n                        0.7577\n            \n            \n                        Mean\n                        0.9456\n                        0.9099\n                        0.7533\n                        0.8589\n                        0.8008\n                        0.7695\n                        0.7729\n            \n            \n                        SD\n                        0.0106\n                        0.0270\n                        0.0452\n                        0.0597\n                        0.0351\n                        0.0411\n                        0.0414\n            \n    \n\n\n\ntuned_xgboost = tune_model(xgboost, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9474\n                        0.8786\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        1\n                        0.9474\n                        0.8737\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        2\n                        0.9509\n                        0.9396\n                        0.8293\n                        0.8293\n                        0.8293\n                        0.8006\n                        0.8006\n            \n            \n                        3\n                        0.9684\n                        0.9017\n                        0.8049\n                        0.9706\n                        0.8800\n                        0.8620\n                        0.8670\n            \n            \n                        4\n                        0.9509\n                        0.9386\n                        0.7805\n                        0.8649\n                        0.8205\n                        0.7921\n                        0.7935\n            \n            \n                        5\n                        0.9544\n                        0.9125\n                        0.7561\n                        0.9118\n                        0.8267\n                        0.8007\n                        0.8053\n            \n            \n                        6\n                        0.9439\n                        0.9686\n                        0.7381\n                        0.8611\n                        0.7949\n                        0.7626\n                        0.7656\n            \n            \n                        7\n                        0.9614\n                        0.9216\n                        0.7857\n                        0.9429\n                        0.8571\n                        0.8350\n                        0.8397\n            \n            \n                        8\n                        0.9263\n                        0.8634\n                        0.6905\n                        0.7838\n                        0.7342\n                        0.6916\n                        0.6935\n            \n            \n                        9\n                        0.9366\n                        0.9118\n                        0.6829\n                        0.8485\n                        0.7568\n                        0.7208\n                        0.7264\n            \n            \n                        Mean\n                        0.9487\n                        0.9110\n                        0.7629\n                        0.8697\n                        0.8120\n                        0.7825\n                        0.7852\n            \n            \n                        SD\n                        0.0113\n                        0.0313\n                        0.0446\n                        0.0533\n                        0.0408\n                        0.0472\n                        0.0476\n            \n    \n\n\n\ntuned_rf = tune_model(rf, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9368\n                        0.8803\n                        0.7561\n                        0.7949\n                        0.7750\n                        0.7383\n                        0.7386\n            \n            \n                        1\n                        0.9368\n                        0.9106\n                        0.8049\n                        0.7674\n                        0.7857\n                        0.7487\n                        0.7490\n            \n            \n                        2\n                        0.9439\n                        0.9287\n                        0.8049\n                        0.8049\n                        0.8049\n                        0.7721\n                        0.7721\n            \n            \n                        3\n                        0.9614\n                        0.9113\n                        0.7805\n                        0.9412\n                        0.8533\n                        0.8313\n                        0.8362\n            \n            \n                        4\n                        0.9439\n                        0.9427\n                        0.8049\n                        0.8049\n                        0.8049\n                        0.7721\n                        0.7721\n            \n            \n                        5\n                        0.9368\n                        0.8999\n                        0.7561\n                        0.7949\n                        0.7750\n                        0.7383\n                        0.7386\n            \n            \n                        6\n                        0.9439\n                        0.9418\n                        0.7143\n                        0.8824\n                        0.7895\n                        0.7575\n                        0.7631\n            \n            \n                        7\n                        0.9509\n                        0.9113\n                        0.7143\n                        0.9375\n                        0.8108\n                        0.7832\n                        0.7927\n            \n            \n                        8\n                        0.9263\n                        0.8532\n                        0.6905\n                        0.7838\n                        0.7342\n                        0.6916\n                        0.6935\n            \n            \n                        9\n                        0.9437\n                        0.8886\n                        0.6829\n                        0.9032\n                        0.7778\n                        0.7462\n                        0.7558\n            \n            \n                        Mean\n                        0.9424\n                        0.9068\n                        0.7509\n                        0.8415\n                        0.7911\n                        0.7579\n                        0.7612\n            \n            \n                        SD\n                        0.0089\n                        0.0265\n                        0.0454\n                        0.0636\n                        0.0293\n                        0.0344\n                        0.0356\n            \n    \n\n\n\ntuned_et = tune_model(et, optimize = 'F1' , n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9158\n                        0.8828\n                        0.6341\n                        0.7429\n                        0.6842\n                        0.6360\n                        0.6386\n            \n            \n                        1\n                        0.9298\n                        0.8971\n                        0.6829\n                        0.8000\n                        0.7368\n                        0.6966\n                        0.6996\n            \n            \n                        2\n                        0.9263\n                        0.9369\n                        0.6585\n                        0.7941\n                        0.7200\n                        0.6780\n                        0.6819\n            \n            \n                        3\n                        0.9509\n                        0.8955\n                        0.7317\n                        0.9091\n                        0.8108\n                        0.7830\n                        0.7891\n            \n            \n                        4\n                        0.9368\n                        0.9228\n                        0.6829\n                        0.8485\n                        0.7568\n                        0.7210\n                        0.7266\n            \n            \n                        5\n                        0.9439\n                        0.8852\n                        0.7073\n                        0.8788\n                        0.7838\n                        0.7520\n                        0.7578\n            \n            \n                        6\n                        0.9474\n                        0.9410\n                        0.7381\n                        0.8857\n                        0.8052\n                        0.7751\n                        0.7794\n            \n            \n                        7\n                        0.9439\n                        0.9394\n                        0.6429\n                        0.9643\n                        0.7714\n                        0.7409\n                        0.7607\n            \n            \n                        8\n                        0.9123\n                        0.8558\n                        0.5476\n                        0.7931\n                        0.6479\n                        0.5997\n                        0.6131\n            \n            \n                        9\n                        0.9120\n                        0.8913\n                        0.4878\n                        0.8333\n                        0.6154\n                        0.5695\n                        0.5956\n            \n            \n                        Mean\n                        0.9319\n                        0.9048\n                        0.6514\n                        0.8450\n                        0.7332\n                        0.6952\n                        0.7042\n            \n            \n                        SD\n                        0.0141\n                        0.0273\n                        0.0755\n                        0.0625\n                        0.0629\n                        0.0698\n                        0.0666\n            \n    \n\n\n\n\nCreate an Ensemble, Blended and Stack model to see the performance\n\ndt = create_model('dt' , fold = 5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8895\n                        0.8289\n                        0.7439\n                        0.5922\n                        0.6595\n                        0.5945\n                        0.6000\n            \n            \n                        1\n                        0.8912\n                        0.8314\n                        0.7470\n                        0.6019\n                        0.6667\n                        0.6026\n                        0.6076\n            \n            \n                        2\n                        0.8877\n                        0.8094\n                        0.6988\n                        0.5979\n                        0.6444\n                        0.5783\n                        0.5807\n            \n            \n                        3\n                        0.9035\n                        0.8536\n                        0.7831\n                        0.6373\n                        0.7027\n                        0.6458\n                        0.6507\n            \n            \n                        4\n                        0.8928\n                        0.7903\n                        0.6463\n                        0.6235\n                        0.6347\n                        0.5719\n                        0.5721\n            \n            \n                        Mean\n                        0.8929\n                        0.8227\n                        0.7238\n                        0.6106\n                        0.6616\n                        0.5986\n                        0.6022\n            \n            \n                        SD\n                        0.0055\n                        0.0214\n                        0.0471\n                        0.0170\n                        0.0234\n                        0.0260\n                        0.0274\n            \n    \n\n\n\ntuned_dt = tune_model(dt, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9018\n                        0.8782\n                        0.7805\n                        0.6275\n                        0.6957\n                        0.6379\n                        0.6433\n            \n            \n                        1\n                        0.8842\n                        0.9037\n                        0.8049\n                        0.5690\n                        0.6667\n                        0.5991\n                        0.6123\n            \n            \n                        2\n                        0.8982\n                        0.9072\n                        0.8780\n                        0.6000\n                        0.7129\n                        0.6537\n                        0.6712\n            \n            \n                        3\n                        0.9053\n                        0.8717\n                        0.7805\n                        0.6400\n                        0.7033\n                        0.6476\n                        0.6521\n            \n            \n                        4\n                        0.9193\n                        0.8988\n                        0.8049\n                        0.6875\n                        0.7416\n                        0.6941\n                        0.6971\n            \n            \n                        5\n                        0.9018\n                        0.9168\n                        0.8537\n                        0.6140\n                        0.7143\n                        0.6569\n                        0.6699\n            \n            \n                        6\n                        0.9439\n                        0.9188\n                        0.8333\n                        0.7955\n                        0.8140\n                        0.7809\n                        0.7812\n            \n            \n                        7\n                        0.9298\n                        0.8626\n                        0.7381\n                        0.7750\n                        0.7561\n                        0.7151\n                        0.7154\n            \n            \n                        8\n                        0.9053\n                        0.8125\n                        0.6667\n                        0.6829\n                        0.6747\n                        0.6193\n                        0.6193\n            \n            \n                        9\n                        0.9190\n                        0.8680\n                        0.7073\n                        0.7250\n                        0.7160\n                        0.6688\n                        0.6689\n            \n            \n                        Mean\n                        0.9108\n                        0.8838\n                        0.7848\n                        0.6716\n                        0.7195\n                        0.6673\n                        0.6731\n            \n            \n                        SD\n                        0.0164\n                        0.0307\n                        0.0623\n                        0.0715\n                        0.0406\n                        0.0494\n                        0.0469\n            \n    \n\n\n\nbagged_dt = ensemble_model(tuned_dt, n_estimators = 200, optimize = 'F1')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8807\n                        0.8698\n                        0.7805\n                        0.5614\n                        0.6531\n                        0.5833\n                        0.5949\n            \n            \n                        1\n                        0.9298\n                        0.9043\n                        0.8293\n                        0.7234\n                        0.7727\n                        0.7315\n                        0.7338\n            \n            \n                        2\n                        0.9053\n                        0.9175\n                        0.9024\n                        0.6167\n                        0.7327\n                        0.6776\n                        0.6957\n            \n            \n                        3\n                        0.9404\n                        0.9077\n                        0.8049\n                        0.7857\n                        0.7952\n                        0.7603\n                        0.7604\n            \n            \n                        4\n                        0.9298\n                        0.9073\n                        0.8537\n                        0.7143\n                        0.7778\n                        0.7365\n                        0.7406\n            \n            \n                        5\n                        0.9123\n                        0.9220\n                        0.8293\n                        0.6538\n                        0.7312\n                        0.6796\n                        0.6865\n            \n            \n                        6\n                        0.9509\n                        0.9436\n                        0.8810\n                        0.8043\n                        0.8409\n                        0.8119\n                        0.8131\n            \n            \n                        7\n                        0.9509\n                        0.9128\n                        0.7857\n                        0.8684\n                        0.8250\n                        0.7965\n                        0.7979\n            \n            \n                        8\n                        0.9088\n                        0.8489\n                        0.7381\n                        0.6739\n                        0.7045\n                        0.6507\n                        0.6517\n            \n            \n                        9\n                        0.9401\n                        0.8935\n                        0.7561\n                        0.8158\n                        0.7848\n                        0.7501\n                        0.7508\n            \n            \n                        Mean\n                        0.9249\n                        0.9027\n                        0.8161\n                        0.7218\n                        0.7618\n                        0.7178\n                        0.7225\n            \n            \n                        SD\n                        0.0215\n                        0.0254\n                        0.0504\n                        0.0922\n                        0.0540\n                        0.0664\n                        0.0632\n            \n    \n\n\n\nboosted_dt = ensemble_model(tuned_dt, method = 'Boosting', n_estimators = 50, optimize = 'F1')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8877\n                        0.8717\n                        0.5610\n                        0.6216\n                        0.5897\n                        0.5249\n                        0.5258\n            \n            \n                        1\n                        0.9123\n                        0.8352\n                        0.6341\n                        0.7222\n                        0.6753\n                        0.6249\n                        0.6266\n            \n            \n                        2\n                        0.9333\n                        0.9076\n                        0.7073\n                        0.8056\n                        0.7532\n                        0.7149\n                        0.7169\n            \n            \n                        3\n                        0.9404\n                        0.8308\n                        0.6829\n                        0.8750\n                        0.7671\n                        0.7335\n                        0.7409\n            \n            \n                        4\n                        0.9053\n                        0.8260\n                        0.6098\n                        0.6944\n                        0.6494\n                        0.5949\n                        0.5965\n            \n            \n                        5\n                        0.9333\n                        0.9336\n                        0.7317\n                        0.7895\n                        0.7595\n                        0.7209\n                        0.7216\n            \n            \n                        6\n                        0.9228\n                        0.8915\n                        0.6429\n                        0.7941\n                        0.7105\n                        0.6666\n                        0.6715\n            \n            \n                        7\n                        0.9018\n                        0.8379\n                        0.4524\n                        0.7917\n                        0.5758\n                        0.5248\n                        0.5512\n            \n            \n                        8\n                        0.9263\n                        0.8179\n                        0.6190\n                        0.8387\n                        0.7123\n                        0.6712\n                        0.6814\n            \n            \n                        9\n                        0.9085\n                        0.8018\n                        0.5610\n                        0.7419\n                        0.6389\n                        0.5876\n                        0.5952\n            \n            \n                        Mean\n                        0.9172\n                        0.8554\n                        0.6202\n                        0.7675\n                        0.6832\n                        0.6364\n                        0.6428\n            \n            \n                        SD\n                        0.0159\n                        0.0411\n                        0.0774\n                        0.0701\n                        0.0654\n                        0.0735\n                        0.0710\n            \n    \n\n\n\n# Train a voting classifier with all models in the library\nblender = blend_models()\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8912\n                        0.0000\n                        0.7561\n                        0.5962\n                        0.6667\n                        0.6028\n                        0.6088\n            \n            \n                        1\n                        0.9404\n                        0.0000\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        2\n                        0.9158\n                        0.0000\n                        0.6829\n                        0.7179\n                        0.7000\n                        0.6511\n                        0.6513\n            \n            \n                        3\n                        0.9474\n                        0.0000\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        4\n                        0.9263\n                        0.0000\n                        0.7561\n                        0.7381\n                        0.7470\n                        0.7039\n                        0.7039\n            \n            \n                        5\n                        0.9158\n                        0.0000\n                        0.7073\n                        0.7073\n                        0.7073\n                        0.6581\n                        0.6581\n            \n            \n                        6\n                        0.9404\n                        0.0000\n                        0.7857\n                        0.8049\n                        0.7952\n                        0.7603\n                        0.7604\n            \n            \n                        7\n                        0.9474\n                        0.0000\n                        0.7619\n                        0.8649\n                        0.8101\n                        0.7797\n                        0.7818\n            \n            \n                        8\n                        0.9018\n                        0.0000\n                        0.7143\n                        0.6522\n                        0.6818\n                        0.6239\n                        0.6248\n            \n            \n                        9\n                        0.9190\n                        0.0000\n                        0.7317\n                        0.7143\n                        0.7229\n                        0.6755\n                        0.6755\n            \n            \n                        Mean\n                        0.9245\n                        0.0000\n                        0.7457\n                        0.7438\n                        0.7431\n                        0.6990\n                        0.7001\n            \n            \n                        SD\n                        0.0183\n                        0.0000\n                        0.0333\n                        0.0802\n                        0.0520\n                        0.0628\n                        0.0621\n            \n    \n\n\n\nblender_specific = blend_models(estimator_list = [tuned_lightgbm,tuned_xgboost,\n                                                 tuned_rf, tuned_et, tuned_dt], method = 'soft')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9404\n                        0.8833\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        1\n                        0.9404\n                        0.9079\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        2\n                        0.9474\n                        0.9345\n                        0.8293\n                        0.8095\n                        0.8193\n                        0.7885\n                        0.7886\n            \n            \n                        3\n                        0.9684\n                        0.8996\n                        0.8049\n                        0.9706\n                        0.8800\n                        0.8620\n                        0.8670\n            \n            \n                        4\n                        0.9509\n                        0.9361\n                        0.8049\n                        0.8462\n                        0.8250\n                        0.7964\n                        0.7968\n            \n            \n                        5\n                        0.9474\n                        0.9149\n                        0.8293\n                        0.8095\n                        0.8193\n                        0.7885\n                        0.7886\n            \n            \n                        6\n                        0.9544\n                        0.9541\n                        0.7857\n                        0.8919\n                        0.8354\n                        0.8091\n                        0.8113\n            \n            \n                        7\n                        0.9579\n                        0.9318\n                        0.7381\n                        0.9688\n                        0.8378\n                        0.8142\n                        0.8241\n            \n            \n                        8\n                        0.9298\n                        0.8511\n                        0.6905\n                        0.8056\n                        0.7436\n                        0.7032\n                        0.7060\n            \n            \n                        9\n                        0.9437\n                        0.8926\n                        0.7073\n                        0.8788\n                        0.7838\n                        0.7518\n                        0.7577\n            \n            \n                        Mean\n                        0.9481\n                        0.9106\n                        0.7751\n                        0.8581\n                        0.8124\n                        0.7824\n                        0.7851\n            \n            \n                        SD\n                        0.0102\n                        0.0289\n                        0.0458\n                        0.0639\n                        0.0354\n                        0.0412\n                        0.0422\n            \n    \n\n\n\nstacked_models = stack_models(estimator_list = [tuned_lightgbm,tuned_catboost,tuned_xgboost,\n                                                 tuned_rf, tuned_et, tuned_dt], meta_model = None, optimize = 'F1', fold = 5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9368\n                        0.9035\n                        0.8293\n                        0.7556\n                        0.7907\n                        0.7536\n                        0.7547\n            \n            \n                        1\n                        0.9579\n                        0.9200\n                        0.8434\n                        0.8642\n                        0.8537\n                        0.8291\n                        0.8292\n            \n            \n                        2\n                        0.9509\n                        0.9184\n                        0.8434\n                        0.8235\n                        0.8333\n                        0.8045\n                        0.8046\n            \n            \n                        3\n                        0.9614\n                        0.9347\n                        0.8434\n                        0.8861\n                        0.8642\n                        0.8417\n                        0.8421\n            \n            \n                        4\n                        0.9315\n                        0.8787\n                        0.7561\n                        0.7654\n                        0.7607\n                        0.7207\n                        0.7208\n            \n            \n                        Mean\n                        0.9477\n                        0.9111\n                        0.8231\n                        0.8190\n                        0.8205\n                        0.7899\n                        0.7903\n            \n            \n                        SD\n                        0.0117\n                        0.0189\n                        0.0339\n                        0.0519\n                        0.0391\n                        0.0459\n                        0.0458\n            \n    \n\n\n\n\nOut of all the models created tuned_lightgbm is performing better on validation data\n\nevaluate_model(tuned_lightgbm)\n\n\n\n\n\n\nTest the model on the test data and choose the best performing model\n\n\nEvaluate Model\n\n# create funtion to return evaluation metrics\ndef evaluation_metrics(model):\n    check_model = predict_model(model, data = test)\n    print(metrics.confusion_matrix(check_model.churn,check_model.Label))\n    tn, fp, fn, tp = metrics.confusion_matrix(check_model.churn,check_model.Label).ravel()\n    Accuracy = round((tp+tn)/(tp+tn+fp+fn),3)\n    precision = round(tp/(tp+fp),3)\n    specificity = round(tn/(tn+fp),3)\n    recall = round(tp/(tp+fn),3)\n    print( f\"Accuracy:{Accuracy} , Specificity:{specificity}, Precision:{precision} , Recall:{recall}\")\n\n\ncheck tuned_lightgbm\n\nevaluation_metrics(tuned_lightgbm)\n\n[[142   1]\n [  5  19]]\nAccuracy:0.964 , Specificity:0.993, Precision:0.95 , Recall:0.792\n\n\n\n\ncheck tuned_catboost\n\nevaluation_metrics(tuned_catboost)\n\n[[141   2]\n [  5  19]]\nAccuracy:0.958 , Specificity:0.986, Precision:0.905 , Recall:0.792\n\n\n\n\ncheck tuned_xgboost\n\nevaluation_metrics(tuned_xgboost)\n\n[[141   2]\n [  5  19]]\nAccuracy:0.958 , Specificity:0.986, Precision:0.905 , Recall:0.792\n\n\n\n\ncheck tuned_rf\n\nevaluation_metrics(tuned_rf)\n\n[[141   2]\n [  6  18]]\nAccuracy:0.952 , Specificity:0.986, Precision:0.9 , Recall:0.75\n\n\n\n\ncheck tuned_et\n\nevaluation_metrics(tuned_et)\n\n[[143   0]\n [ 11  13]]\nAccuracy:0.934 , Specificity:1.0, Precision:1.0 , Recall:0.542\n\n\n\n\ncheck tuned_dt\n\nevaluation_metrics(tuned_dt)\n\n[[141   2]\n [  6  18]]\nAccuracy:0.952 , Specificity:0.986, Precision:0.9 , Recall:0.75\n\n\n\n\ncheck boosted_dt\n\nevaluation_metrics(boosted_dt)\n\n[[141   2]\n [ 10  14]]\nAccuracy:0.928 , Specificity:0.986, Precision:0.875 , Recall:0.583\n\n\n\n\ncheck bagged_dt\n\nevaluation_metrics(bagged_dt)\n\n[[139   4]\n [  6  18]]\nAccuracy:0.94 , Specificity:0.972, Precision:0.818 , Recall:0.75\n\n\n\n\ncheck blender\n\nevaluation_metrics(blender)\n\n[[143   0]\n [ 11  13]]\nAccuracy:0.934 , Specificity:1.0, Precision:1.0 , Recall:0.542\n\n\n\n\ncheck blender_specific\n\nevaluation_metrics(blender_specific)\n\n[[142   1]\n [  5  19]]\nAccuracy:0.964 , Specificity:0.993, Precision:0.95 , Recall:0.792\n\n\n\n\ncheck stacked_models\n\nevaluation_metrics(stacked_models)\n\n[[139   4]\n [  4  20]]\nAccuracy:0.952 , Specificity:0.972, Precision:0.833 , Recall:0.833\n\n\n\n\n\nFinalizing the Model and Metrics\n\nCompared multiple models to examine which algorithm is suitable for this dataset\nChose the five best performing algorithms and created models for them\nHyper parameter tuning was done to further improve the model performance\nEnsemble of models were created to check their performance on test data\nAll the tuned and esemble models were tested out on unseen data to finalize a model\n\n\n\nModel finalization\nMy recommendation for the final model is tuned_lightgbm. This is because the models predictions for churned customers is very high. From my experience in media Industry, it was observed that business users usually request for model explainability. This model’s Recall is slightly lower than stacked_models. stacked_model was not selected because it does not provide model interpretation. Also given similar performance it is better to go for simpler model.\n\n# Finalize the model\nfinal_model = finalize_model(tuned_lightgbm)\n\n\n# Feature importance using decision tree models\nplot_model(final_model, plot = 'feature')\n\n\n\n\n\n# Feature importance using Shap\ninterpret_model(final_model, plot = 'summary')\n\n\n\n\n\n# local interpretation\ninterpret_model(final_model, plot = 'reason', observation = 14)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n# save the model\nsave_model(final_model,'tuned_lightgbm_dt_save_20201017')\n\nTransformation Pipeline and Model Succesfully Saved\n\n\n\n# load the model\nloaded_model = load_model('bagged_dt_save_20201017')\n\nTransformation Pipeline and Model Sucessfully Loaded\n\n\n\n\nPotential issues with deploying the model into production are:-\n\nData and model versioning As the amount of data is increasing every day, a mechanism to version data along with code needs to be established. This needs to be done without any cost overhead for storing multiple copies of the same data.\nTracking and storing of experiment results and artifacts efficiently. Data scientist should be able to tell which version of model is presently in production, data used for training, what are the evaluation metrics of the model at any given time.\nMonitoring of models in production for data drift - The behaviour of incoming data may change and will may differ from the data on which it was trained\nTaking care of CI/CD in production - As soon as a better performing model is finalized and commited, it should go to production in an automated fashion\nInstead of full-deployment of the models - Canary or Blue-Green deployment should be done. If model should be exposed to 10% to 15% of the population. If it performs well on a small population, then it should be rolled out for everyone.\nFeedback loops - The data used by the model for prediction going again into the training set\n\nThe following tools which can be used for the production issues mentioned above:- 1. Data and model versioning - Data Version Control (DVC) and MLOps by DVC 2. Tracking experiments - mlflow python package 3. Monitoring of models in production - Bi tools 4. Containerization - Docker and Kubernetes 5. CI/CD - Github, CircleCI, MLops 6. Deployment - Seldon core, Heroku 7. Canary / Bluegreen Deployment - AWS Sagemaker\n\n\nAppendix\nIf the business priorirty is to predict both churners and non-churners accurately then Accuracy. If the business priority is to identify churners then precision and recall. If the business priority is to predict non-churners (which is a very rare scenario) then it is specificity. The final model will be chosen as per business priority. If the business priority is :- 1. Predicting both churn and non-churning customers accurately then the model with highest accuracy will be chosen i.e - tuned_dt 2. Maximizing the proportion of churner identifications which are actually correct, then model with highest precision - tuned_dt 3. Identifying the Maximum proportion of actual churners then model with highest recall - bagged_dt (as this is simpler than blender_specific)\n\n# Data for DOE\ndoe = predict_model(loaded_model, data = telecom_churn)\nprint(metrics.confusion_matrix(doe.churn,doe.Label))\n\n[[2821   29]\n [ 135  348]]\n\n\n\ntn, fp, fn, tp = metrics.confusion_matrix(doe.churn,doe.Label).ravel()\nAccuracy = round((tp+tn)/(tp+tn+fp+fn),3)\nprecision = round(tp/(tp+fp),3)\nspecificity = round(tn/(tn+fp),3)\nrecall = round(tp/(tp+fn),3)\nprint( f\"Accuracy:{Accuracy} , Specificity:{specificity}, Precision:{precision} , Recall:{recall}\")\n\nAccuracy:0.951 , Specificity:0.99, Precision:0.923 , Recall:0.72\n\n\n\ndoe.shape\n\n(3333, 23)\n\n\n\ndoe.churn.value_counts()\n\n0    2850\n1     483\nName: churn, dtype: int64\n\n\n\n93/483\n\n0.19254658385093168"
  },
  {
    "objectID": "telecom_churn_prediction/telecom_churn_eda.html",
    "href": "telecom_churn_prediction/telecom_churn_eda.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "EDA on Telecom Churn Data\nThe objectives of this project are:-\n1. Perform exploratory analysis and extract insights from the dataset.\n2. Split the dataset into train/test sets and explain your reasoning.\n3. Build a predictive model to predict which customers are going to churn and discuss the reason why you choose a particular algorithm.\n4. Establish metrics to evaluate model performance.\n5. Discuss the potential issues with deploying the model into production\n\nImport the required libraries\n\n# python version # 3.8.2\nimport pandas as pd \nimport numpy as np \nimport os \nfrom pandas_profiling import ProfileReport\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# option to display all columns\npd.set_option('display.max_columns', None)\n\n\n# Read the data\ntelecom_churn = pd.read_csv('../data/telecom_data/telecom.csv')\n\n\ntelecom_churn.head(10)\n\n\n\n\n\n  \n    \n      \n      state\n      account length\n      area code\n      phone number\n      international plan\n      voice mail plan\n      number vmail messages\n      total day minutes\n      total day calls\n      total day charge\n      total eve minutes\n      total eve calls\n      total eve charge\n      total night minutes\n      total night calls\n      total night charge\n      total intl minutes\n      total intl calls\n      total intl charge\n      customer service calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      5\n      AL\n      118\n      510\n      391-8027\n      yes\n      no\n      0\n      223.4\n      98\n      37.98\n      220.6\n      101\n      18.75\n      203.9\n      118\n      9.18\n      6.3\n      6\n      1.70\n      0\n      False\n    \n    \n      6\n      MA\n      121\n      510\n      355-9993\n      no\n      yes\n      24\n      218.2\n      88\n      37.09\n      348.5\n      108\n      29.62\n      212.6\n      118\n      9.57\n      7.5\n      7\n      2.03\n      3\n      False\n    \n    \n      7\n      MO\n      147\n      415\n      329-9001\n      yes\n      no\n      0\n      157.0\n      79\n      26.69\n      103.1\n      94\n      8.76\n      211.8\n      96\n      9.53\n      7.1\n      6\n      1.92\n      0\n      False\n    \n    \n      8\n      LA\n      117\n      408\n      335-4719\n      no\n      no\n      0\n      184.5\n      97\n      31.37\n      351.6\n      80\n      29.89\n      215.8\n      90\n      9.71\n      8.7\n      4\n      2.35\n      1\n      False\n    \n    \n      9\n      WV\n      141\n      415\n      330-8173\n      yes\n      yes\n      37\n      258.6\n      84\n      43.96\n      222.0\n      111\n      18.87\n      326.4\n      97\n      14.69\n      11.2\n      5\n      3.02\n      0\n      False\n    \n  \n\n\n\n\n\n\nCheck the Shape and Column types of the Dataframe\n\ntelecom_churn.shape\n\n(3333, 21)\n\n\n\ntelecom_churn.dtypes\n\nstate                      object\naccount length              int64\narea code                   int64\nphone number               object\ninternational plan         object\nvoice mail plan            object\nnumber vmail messages       int64\ntotal day minutes         float64\ntotal day calls             int64\ntotal day charge          float64\ntotal eve minutes         float64\ntotal eve calls             int64\ntotal eve charge          float64\ntotal night minutes       float64\ntotal night calls           int64\ntotal night charge        float64\ntotal intl minutes        float64\ntotal intl calls            int64\ntotal intl charge         float64\ncustomer service calls      int64\nchurn                        bool\ndtype: object\n\n\n\n\nExploratory Analysis\n\n# Format the column names, remove space and special characters in column names\ntelecom_churn.columns =  telecom_churn.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n\n\ntelecom_churn\n\n\n\n\n\n  \n    \n      \n      state\n      account_length\n      area_code\n      phone_number\n      international_plan\n      voice_mail_plan\n      number_vmail_messages\n      total_day_minutes\n      total_day_calls\n      total_day_charge\n      total_eve_minutes\n      total_eve_calls\n      total_eve_charge\n      total_night_minutes\n      total_night_calls\n      total_night_charge\n      total_intl_minutes\n      total_intl_calls\n      total_intl_charge\n      customer_service_calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3328\n      AZ\n      192\n      415\n      414-4276\n      no\n      yes\n      36\n      156.2\n      77\n      26.55\n      215.5\n      126\n      18.32\n      279.1\n      83\n      12.56\n      9.9\n      6\n      2.67\n      2\n      False\n    \n    \n      3329\n      WV\n      68\n      415\n      370-3271\n      no\n      no\n      0\n      231.1\n      57\n      39.29\n      153.4\n      55\n      13.04\n      191.3\n      123\n      8.61\n      9.6\n      4\n      2.59\n      3\n      False\n    \n    \n      3330\n      RI\n      28\n      510\n      328-8230\n      no\n      no\n      0\n      180.8\n      109\n      30.74\n      288.8\n      58\n      24.55\n      191.9\n      91\n      8.64\n      14.1\n      6\n      3.81\n      2\n      False\n    \n    \n      3331\n      CT\n      184\n      510\n      364-6381\n      yes\n      no\n      0\n      213.8\n      105\n      36.35\n      159.6\n      84\n      13.57\n      139.2\n      137\n      6.26\n      5.0\n      10\n      1.35\n      2\n      False\n    \n    \n      3332\n      TN\n      74\n      415\n      400-4344\n      no\n      yes\n      25\n      234.4\n      113\n      39.85\n      265.9\n      82\n      22.60\n      241.4\n      77\n      10.86\n      13.7\n      4\n      3.70\n      0\n      False\n    \n  \n\n3333 rows × 21 columns\n\n\n\n\nprofile = ProfileReport(telecom_churn, title = \"Telecom Churn Report\")\n\n\nprofile.to_notebook_iframe()"
  },
  {
    "objectID": "data_privacy/basic_privacy_approaches.html",
    "href": "data_privacy/basic_privacy_approaches.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Data which could be used separately or in combination with other information to identify a person or small group of persons\n\nPersonally Identifiable Information (PII)\nPerson-Related data\nProprietary and Confidential data\n\n\n\n\nIdentifying PII in images and text\n\n\n\n\nWe can have a classification system for the data.\nUse classification as an initial step for documentation\nToolkit for documenting Data - Data Cards\nFramework for documenting Models - Model Cards\nTool for Data Management\n\nDocumenting data Collection\nDocumenting Data Quality\nDocumenting Data Security\nDocumenting Data Privacy\nDocumenting Data Descriptions\nDocumenting Data Statistics\nDocumenting consent\n\nTrack Data Lineage\nData version control\n\n\n\n\n\n\n\nIt is a technique that allows us to use “Pseudonyms” instead of real names and data\n\n\n\n\npseudonymization approaches\n\n\n\n\n\nHow a linkage attack works\n\n\n\nLinking is a primary attack vector to determine the identity of an individual.\n\n\n\n\nAdvantages and Disadvantages of Pseudonymization\n\n\n\nIf the data will be only used intenally by a small group of individuals who may require privileged access, then pseudonymization might be a good fit for the needs\nTools for pseudonymization\n\nKIProtect’s Kodex\nFormat preserving library by Mysto\nMicrosoft’s Presidio\nPrivate Input Masked output based on GO"
  },
  {
    "objectID": "data_privacy/differential_privacy.html",
    "href": "data_privacy/differential_privacy.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Anonymize is to remove identifying information from data so that original source cannot be known\nPrivacy needs to be understood as a gradient and not a “on” or “off” thing \n\n\n\n\n\n\nHow Differential Privacy works\n\n\nA process A is epsilon-differentially private if for all databases D1 and D2 which differ in only one individual:\n\n\n\nDefinition\n\n\nThis must be true for all possible outputs O. If epsilon is very close to 0, then exponential of epsilon is very close to 1, so the probabilities are very similar. The bigger epsilon is, the more the probabilities can differ.\n\nIt is a rigorous and scientific definition of privacy-sensitive information release - that defines a limit or bounds on the amount of privacy loss you can have when you release information\nThis method focuses on the process rather than the result\nDifferential privacy shifts to thinking about what guarantees a particular algorithm can provide by measuring the information that is being continuously released via the algorithm itself.\nWhy is differential privacy special:-\n\nNo longer need attack modeling\nWe can quantify the privacy loss\nWe can compose multiple mechanisms - We can add the epsilon of multiple queries to arrive at the privacy loss for all the queries together.we can allocate budget for the user queries\n\nSensitivity measures the maximum change in the query result based on change in the underlying dataset.\n\n\n\n\n\n\nFree Udacity course\n\n\n\npython package pydp Tutorials using pydp python package opendp spark package PipelineDP Tensorflow Privacy\n\n\n\nBeautiful book with lot of plots and code\n\n\n\nDifferential Privacy blog by Damien Desfontaines\n\n\n\nMicrosoft session on privacy preserving ML"
  },
  {
    "objectID": "gx_tutorials/getting_started_tutorial_final_v3_api/great_expectations/notebooks/spark/validation_playground.html",
    "href": "gx_tutorials/getting_started_tutorial_final_v3_api/great_expectations/notebooks/spark/validation_playground.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Watch a short tutorial video or read the written tutorial\n\n\n\n\n\nWe’d love it if you reach out for help on the Great Expectations Slack Channel\n\nimport json\nimport great_expectations as ge\nimport great_expectations.jupyter_ux\nfrom great_expectations.datasource.types import BatchKwargs\nimport datetime\n\n\n\n\nThis represents your project that you just created using great_expectations init.\n\ncontext = ge.data_context.DataContext()\n\n\n\n\nList expectation suites that you created in your project\n\ncontext.list_expectation_suite_names()\n\n\nexpectation_suite_name =  # TODO: set to a name from the list above\n\n\n\n\nTo learn more about get_batch, see this tutorial\n\n# list datasources of the type SparkDFDatasource in your project\n[datasource['name'] for datasource in context.list_datasources() if datasource['class_name'] == 'SparkDFDatasource']\n\n\ndatasource_name = # TODO: set to a datasource name from above\n\n\n# If you would like to validate a file on a filesystem:\nbatch_kwargs = {'path': \"YOUR_FILE_PATH\", 'datasource': datasource_name}\n# To customize how Spark reads the file, you can add options under reader_options key in batch_kwargs (e.g., header='true')\n\n# If you already loaded the data into a PySpark Data Frame:\nbatch_kwargs = {'dataset': \"YOUR_DATAFRAME\", 'datasource': datasource_name}\n\n\nbatch = context.get_batch(batch_kwargs, expectation_suite_name)\nbatch.head()\n\n\n\n\nValidation Operators provide a convenient way to bundle the validation of multiple expectation suites and the actions that should be taken after validation.\nWhen deploying Great Expectations in a real data pipeline, you will typically discover these needs:\n\nvalidating a group of batches that are logically related\nvalidating a batch against several expectation suites such as using a tiered pattern like warning and failure\ndoing something with the validation results (e.g., saving them for a later review, sending notifications in case of failures, etc.).\n\nRead more about Validation Operators in the tutorial\n\n# This is an example of invoking a validation operator that is configured by default in the great_expectations.yml file\n\n\"\"\"\nCreate a run_id. The run_id must be of type RunIdentifier, with optional run_name and run_time instantiation\narguments (or a dictionary with these keys). The run_name can be any string (this could come from your pipeline\nrunner, e.g. Airflow run id). The run_time can be either a dateutil parsable string or a datetime object.\nNote - any provided datetime will be assumed to be a UTC time. If no instantiation arguments are given, run_name will\nbe None and run_time will default to the current UTC datetime.\n\"\"\"\n\nrun_id = {\n  \"run_name\": \"some_string_that_uniquely_identifies_this_run\",  # insert your own run_name here\n  \"run_time\": datetime.datetime.now(datetime.timezone.utc)\n}\n\nresults = context.run_validation_operator(\n    \"action_list_operator\",\n    assets_to_validate=[batch],\n    run_id=run_id)\n\n\n\n\nLet’s now build and look at your Data Docs. These will now include an data quality report built from the ValidationResults you just created that helps you communicate about your data with both machines and humans.\nRead more about Data Docs in the tutorial\n\ncontext.open_data_docs()\n\n\n\n\n\n\n\n\n\ntypical workflow\n\n\n\nYou are now among the elite data professionals who know how to build robust descriptions of your data and protections for pipelines and machine learning models. Join the Great Expectations Slack Channel to see how others are wielding these superpowers."
  },
  {
    "objectID": "gx_tutorials/getting_started_tutorial_final_v3_api/great_expectations/notebooks/sql/validation_playground.html",
    "href": "gx_tutorials/getting_started_tutorial_final_v3_api/great_expectations/notebooks/sql/validation_playground.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Watch a short tutorial video or read the written tutorial\n\n\n\n\n\nWe’d love it if you reach out for help on the Great Expectations Slack Channel\n\nimport json\nimport great_expectations as ge\nimport great_expectations.jupyter_ux\nfrom great_expectations.datasource.types import BatchKwargs\nimport datetime\n\n\n\n\nThis represents your project that you just created using great_expectations init.\n\ncontext = ge.data_context.DataContext()\n\n\n\n\nList expectation suites that you created in your project\n\ncontext.list_expectation_suite_names()\n\n\nexpectation_suite_name =  # TODO: set to a name from the list above\n\n\n\n\nTo learn more about get_batch, see this tutorial\n\n# list datasources of the type SqlAlchemyDatasource in your project\n[datasource['name'] for datasource in context.list_datasources() if datasource['class_name'] == 'SqlAlchemyDatasource']\n\n\ndatasource_name = # TODO: set to a datasource name from above\n\n\n# If you would like to validate an entire table or view in your database's default schema:\nbatch_kwargs = {'table': \"YOUR_TABLE\", 'datasource': datasource_name}\n\n# If you would like to validate an entire table or view from a non-default schema in your database:\nbatch_kwargs = {'table': \"YOUR_TABLE\", \"schema\": \"YOUR_SCHEMA\", 'datasource': datasource_name}\n\n# If you would like to validate the result set of a query:\n# batch_kwargs = {'query': 'SELECT YOUR_ROWS FROM YOUR_TABLE', 'datasource': datasource_name}\n\n\n\nbatch = context.get_batch(batch_kwargs, expectation_suite_name)\nbatch.head()\n\n\n\n\nValidation Operators provide a convenient way to bundle the validation of multiple expectation suites and the actions that should be taken after validation.\nWhen deploying Great Expectations in a real data pipeline, you will typically discover these needs:\n\nvalidating a group of batches that are logically related\nvalidating a batch against several expectation suites such as using a tiered pattern like warning and failure\ndoing something with the validation results (e.g., saving them for a later review, sending notifications in case of failures, etc.).\n\nRead more about Validation Operators in the tutorial\n\n# This is an example of invoking a validation operator that is configured by default in the great_expectations.yml file\n\n\"\"\"\nCreate a run_id. The run_id must be of type RunIdentifier, with optional run_name and run_time instantiation\narguments (or a dictionary with these keys). The run_name can be any string (this could come from your pipeline\nrunner, e.g. Airflow run id). The run_time can be either a dateutil parsable string or a datetime object.\nNote - any provided datetime will be assumed to be a UTC time. If no instantiation arguments are given, run_name will\nbe None and run_time will default to the current UTC datetime.\n\"\"\"\n\nrun_id = {\n  \"run_name\": \"some_string_that_uniquely_identifies_this_run\",  # insert your own run_name here\n  \"run_time\": datetime.datetime.now(datetime.timezone.utc)\n}\n\nresults = context.run_validation_operator(\n    \"action_list_operator\",\n    assets_to_validate=[batch],\n    run_id=run_id)\n\n\n\n\nLet’s now build and look at your Data Docs. These will now include an data quality report built from the ValidationResults you just created that helps you communicate about your data with both machines and humans.\nRead more about Data Docs in the tutorial\n\ncontext.open_data_docs()\n\n\n\n\n\n\n\n\n\ntypical workflow\n\n\n\nYou are now among the elite data professionals who know how to build robust descriptions of your data and protections for pipelines and machine learning models. Join the Great Expectations Slack Channel to see how others are wielding these superpowers."
  },
  {
    "objectID": "gx_tutorials/getting_started_tutorial_final_v3_api/great_expectations/notebooks/pandas/validation_playground.html",
    "href": "gx_tutorials/getting_started_tutorial_final_v3_api/great_expectations/notebooks/pandas/validation_playground.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Watch a short tutorial video or read the written tutorial\n\n\n\n\n\nWe’d love it if you reach out for help on the Great Expectations Slack Channel\n\nimport json\nimport great_expectations as ge\nimport great_expectations.jupyter_ux\nfrom great_expectations.datasource.types import BatchKwargs\nimport datetime\n\n\n\n\nThis represents your project that you just created using great_expectations init.\n\ncontext = ge.data_context.DataContext()\n\n\n\n\nList expectation suites that you created in your project\n\ncontext.list_expectation_suite_names()\n\n\nexpectation_suite_name =  # TODO: set to a name from the list above\n\n\n\n\nTo learn more about get_batch, see this tutorial\n\n# list datasources of the type PandasDatasource in your project\n[datasource['name'] for datasource in context.list_datasources() if datasource['class_name'] == 'PandasDatasource']\n\n\ndatasource_name = # TODO: set to a datasource name from above\n\n\n# If you would like to validate a file on a filesystem:\nbatch_kwargs = {'path': \"YOUR_FILE_PATH\", 'datasource': datasource_name}\n\n# If you already loaded the data into a Pandas Data Frame:\nbatch_kwargs = {'dataset': \"YOUR_DATAFRAME\", 'datasource': datasource_name}\n\n\nbatch = context.get_batch(batch_kwargs, expectation_suite_name)\nbatch.head()\n\n\n\n\nValidation Operators provide a convenient way to bundle the validation of multiple expectation suites and the actions that should be taken after validation.\nWhen deploying Great Expectations in a real data pipeline, you will typically discover these needs:\n\nvalidating a group of batches that are logically related\nvalidating a batch against several expectation suites such as using a tiered pattern like warning and failure\ndoing something with the validation results (e.g., saving them for a later review, sending notifications in case of failures, etc.).\n\nRead more about Validation Operators in the tutorial\n\n# This is an example of invoking a validation operator that is configured by default in the great_expectations.yml file\n\n\"\"\"\nCreate a run_id. The run_id must be of type RunIdentifier, with optional run_name and run_time instantiation\narguments (or a dictionary with these keys). The run_name can be any string (this could come from your pipeline\nrunner, e.g. Airflow run id). The run_time can be either a dateutil parsable string or a datetime object.\nNote - any provided datetime will be assumed to be a UTC time. If no instantiation arguments are given, run_name will\nbe None and run_time will default to the current UTC datetime.\n\"\"\"\n\nrun_id = {\n  \"run_name\": \"some_string_that_uniquely_identifies_this_run\",  # insert your own run_name here\n  \"run_time\": datetime.datetime.now(datetime.timezone.utc)\n}\n\nresults = context.run_validation_operator(\n    \"action_list_operator\",\n    assets_to_validate=[batch],\n    run_id=run_id)\n\n\n\n\nLet’s now build and look at your Data Docs. These will now include an data quality report built from the ValidationResults you just created that helps you communicate about your data with both machines and humans.\nRead more about Data Docs in the tutorial\n\ncontext.open_data_docs()\n\n\n\n\n\n\n\n\n\ntypical workflow\n\n\n\nYou are now among the elite data professionals who know how to build robust descriptions of your data and protections for pipelines and machine learning models. Join the Great Expectations Slack Channel to see how others are wielding these superpowers."
  },
  {
    "objectID": "gx_tutorials/getting_started_tutorial_final_v2_api/great_expectations/notebooks/spark/validation_playground.html",
    "href": "gx_tutorials/getting_started_tutorial_final_v2_api/great_expectations/notebooks/spark/validation_playground.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Watch a short tutorial video or read the written tutorial\n\n\n\n\n\nWe’d love it if you reach out for help on the Great Expectations Slack Channel\n\nimport json\nimport great_expectations as ge\nimport great_expectations.jupyter_ux\nfrom great_expectations.datasource.types import BatchKwargs\nimport datetime\n\n\n\n\nThis represents your project that you just created using great_expectations init.\n\ncontext = ge.data_context.DataContext()\n\n\n\n\nList expectation suites that you created in your project\n\ncontext.list_expectation_suite_names()\n\n\nexpectation_suite_name =  # TODO: set to a name from the list above\n\n\n\n\nTo learn more about get_batch, see this tutorial\n\n# list datasources of the type SparkDFDatasource in your project\n[datasource['name'] for datasource in context.list_datasources() if datasource['class_name'] == 'SparkDFDatasource']\n\n\ndatasource_name = # TODO: set to a datasource name from above\n\n\n# If you would like to validate a file on a filesystem:\nbatch_kwargs = {'path': \"YOUR_FILE_PATH\", 'datasource': datasource_name}\n# To customize how Spark reads the file, you can add options under reader_options key in batch_kwargs (e.g., header='true')\n\n# If you already loaded the data into a PySpark Data Frame:\nbatch_kwargs = {'dataset': \"YOUR_DATAFRAME\", 'datasource': datasource_name}\n\n\nbatch = context.get_batch(batch_kwargs, expectation_suite_name)\nbatch.head()\n\n\n\n\nValidation Operators provide a convenient way to bundle the validation of multiple expectation suites and the actions that should be taken after validation.\nWhen deploying Great Expectations in a real data pipeline, you will typically discover these needs:\n\nvalidating a group of batches that are logically related\nvalidating a batch against several expectation suites such as using a tiered pattern like warning and failure\ndoing something with the validation results (e.g., saving them for a later review, sending notifications in case of failures, etc.).\n\nRead more about Validation Operators in the tutorial\n\n# This is an example of invoking a validation operator that is configured by default in the great_expectations.yml file\n\n\"\"\"\nCreate a run_id. The run_id must be of type RunIdentifier, with optional run_name and run_time instantiation\narguments (or a dictionary with these keys). The run_name can be any string (this could come from your pipeline\nrunner, e.g. Airflow run id). The run_time can be either a dateutil parsable string or a datetime object.\nNote - any provided datetime will be assumed to be a UTC time. If no instantiation arguments are given, run_name will\nbe None and run_time will default to the current UTC datetime.\n\"\"\"\n\nrun_id = {\n  \"run_name\": \"some_string_that_uniquely_identifies_this_run\",  # insert your own run_name here\n  \"run_time\": datetime.datetime.now(datetime.timezone.utc)\n}\n\nresults = context.run_validation_operator(\n    \"action_list_operator\",\n    assets_to_validate=[batch],\n    run_id=run_id)\n\n\n\n\nLet’s now build and look at your Data Docs. These will now include an data quality report built from the ValidationResults you just created that helps you communicate about your data with both machines and humans.\nRead more about Data Docs in the tutorial\n\ncontext.open_data_docs()\n\n\n\n\n\n\n\n\n\ntypical workflow\n\n\n\nYou are now among the elite data professionals who know how to build robust descriptions of your data and protections for pipelines and machine learning models. Join the Great Expectations Slack Channel to see how others are wielding these superpowers."
  },
  {
    "objectID": "gx_tutorials/getting_started_tutorial_final_v2_api/great_expectations/notebooks/sql/validation_playground.html",
    "href": "gx_tutorials/getting_started_tutorial_final_v2_api/great_expectations/notebooks/sql/validation_playground.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Watch a short tutorial video or read the written tutorial\n\n\n\n\n\nWe’d love it if you reach out for help on the Great Expectations Slack Channel\n\nimport json\nimport great_expectations as ge\nimport great_expectations.jupyter_ux\nfrom great_expectations.datasource.types import BatchKwargs\nimport datetime\n\n\n\n\nThis represents your project that you just created using great_expectations init.\n\ncontext = ge.data_context.DataContext()\n\n\n\n\nList expectation suites that you created in your project\n\ncontext.list_expectation_suite_names()\n\n\nexpectation_suite_name =  # TODO: set to a name from the list above\n\n\n\n\nTo learn more about get_batch, see this tutorial\n\n# list datasources of the type SqlAlchemyDatasource in your project\n[datasource['name'] for datasource in context.list_datasources() if datasource['class_name'] == 'SqlAlchemyDatasource']\n\n\ndatasource_name = # TODO: set to a datasource name from above\n\n\n# If you would like to validate an entire table or view in your database's default schema:\nbatch_kwargs = {'table': \"YOUR_TABLE\", 'datasource': datasource_name}\n\n# If you would like to validate an entire table or view from a non-default schema in your database:\nbatch_kwargs = {'table': \"YOUR_TABLE\", \"schema\": \"YOUR_SCHEMA\", 'datasource': datasource_name}\n\n# If you would like to validate the result set of a query:\n# batch_kwargs = {'query': 'SELECT YOUR_ROWS FROM YOUR_TABLE', 'datasource': datasource_name}\n\n\n\nbatch = context.get_batch(batch_kwargs, expectation_suite_name)\nbatch.head()\n\n\n\n\nValidation Operators provide a convenient way to bundle the validation of multiple expectation suites and the actions that should be taken after validation.\nWhen deploying Great Expectations in a real data pipeline, you will typically discover these needs:\n\nvalidating a group of batches that are logically related\nvalidating a batch against several expectation suites such as using a tiered pattern like warning and failure\ndoing something with the validation results (e.g., saving them for a later review, sending notifications in case of failures, etc.).\n\nRead more about Validation Operators in the tutorial\n\n# This is an example of invoking a validation operator that is configured by default in the great_expectations.yml file\n\n\"\"\"\nCreate a run_id. The run_id must be of type RunIdentifier, with optional run_name and run_time instantiation\narguments (or a dictionary with these keys). The run_name can be any string (this could come from your pipeline\nrunner, e.g. Airflow run id). The run_time can be either a dateutil parsable string or a datetime object.\nNote - any provided datetime will be assumed to be a UTC time. If no instantiation arguments are given, run_name will\nbe None and run_time will default to the current UTC datetime.\n\"\"\"\n\nrun_id = {\n  \"run_name\": \"some_string_that_uniquely_identifies_this_run\",  # insert your own run_name here\n  \"run_time\": datetime.datetime.now(datetime.timezone.utc)\n}\n\nresults = context.run_validation_operator(\n    \"action_list_operator\",\n    assets_to_validate=[batch],\n    run_id=run_id)\n\n\n\n\nLet’s now build and look at your Data Docs. These will now include an data quality report built from the ValidationResults you just created that helps you communicate about your data with both machines and humans.\nRead more about Data Docs in the tutorial\n\ncontext.open_data_docs()\n\n\n\n\n\n\n\n\n\ntypical workflow\n\n\n\nYou are now among the elite data professionals who know how to build robust descriptions of your data and protections for pipelines and machine learning models. Join the Great Expectations Slack Channel to see how others are wielding these superpowers."
  },
  {
    "objectID": "gx_tutorials/getting_started_tutorial_final_v2_api/great_expectations/notebooks/pandas/validation_playground.html",
    "href": "gx_tutorials/getting_started_tutorial_final_v2_api/great_expectations/notebooks/pandas/validation_playground.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Watch a short tutorial video or read the written tutorial\n\n\n\n\n\nWe’d love it if you reach out for help on the Great Expectations Slack Channel\n\nimport json\nimport great_expectations as ge\nimport great_expectations.jupyter_ux\nfrom great_expectations.datasource.types import BatchKwargs\nimport datetime\n\n\n\n\nThis represents your project that you just created using great_expectations init.\n\ncontext = ge.data_context.DataContext()\n\n\n\n\nList expectation suites that you created in your project\n\ncontext.list_expectation_suite_names()\n\n\nexpectation_suite_name =  # TODO: set to a name from the list above\n\n\n\n\nTo learn more about get_batch, see this tutorial\n\n# list datasources of the type PandasDatasource in your project\n[datasource['name'] for datasource in context.list_datasources() if datasource['class_name'] == 'PandasDatasource']\n\n\ndatasource_name = # TODO: set to a datasource name from above\n\n\n# If you would like to validate a file on a filesystem:\nbatch_kwargs = {'path': \"YOUR_FILE_PATH\", 'datasource': datasource_name}\n\n# If you already loaded the data into a Pandas Data Frame:\nbatch_kwargs = {'dataset': \"YOUR_DATAFRAME\", 'datasource': datasource_name}\n\n\nbatch = context.get_batch(batch_kwargs, expectation_suite_name)\nbatch.head()\n\n\n\n\nValidation Operators provide a convenient way to bundle the validation of multiple expectation suites and the actions that should be taken after validation.\nWhen deploying Great Expectations in a real data pipeline, you will typically discover these needs:\n\nvalidating a group of batches that are logically related\nvalidating a batch against several expectation suites such as using a tiered pattern like warning and failure\ndoing something with the validation results (e.g., saving them for a later review, sending notifications in case of failures, etc.).\n\nRead more about Validation Operators in the tutorial\n\n# This is an example of invoking a validation operator that is configured by default in the great_expectations.yml file\n\n\"\"\"\nCreate a run_id. The run_id must be of type RunIdentifier, with optional run_name and run_time instantiation\narguments (or a dictionary with these keys). The run_name can be any string (this could come from your pipeline\nrunner, e.g. Airflow run id). The run_time can be either a dateutil parsable string or a datetime object.\nNote - any provided datetime will be assumed to be a UTC time. If no instantiation arguments are given, run_name will\nbe None and run_time will default to the current UTC datetime.\n\"\"\"\n\nrun_id = {\n  \"run_name\": \"some_string_that_uniquely_identifies_this_run\",  # insert your own run_name here\n  \"run_time\": datetime.datetime.now(datetime.timezone.utc)\n}\n\nresults = context.run_validation_operator(\n    \"action_list_operator\",\n    assets_to_validate=[batch],\n    run_id=run_id)\n\n\n\n\nLet’s now build and look at your Data Docs. These will now include an data quality report built from the ValidationResults you just created that helps you communicate about your data with both machines and humans.\nRead more about Data Docs in the tutorial\n\ncontext.open_data_docs()\n\n\n\n\n\n\n\n\n\ntypical workflow\n\n\n\nYou are now among the elite data professionals who know how to build robust descriptions of your data and protections for pipelines and machine learning models. Join the Great Expectations Slack Channel to see how others are wielding these superpowers."
  },
  {
    "objectID": "gx_tutorials/gx_dbt_airflow_tutorial/great_expectations_projects/final/great_expectations/notebooks/spark/validation_playground.html",
    "href": "gx_tutorials/gx_dbt_airflow_tutorial/great_expectations_projects/final/great_expectations/notebooks/spark/validation_playground.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Watch a short tutorial video or read the written tutorial\n\n\n\n\n\nWe’d love it if you reach out for help on the Great Expectations Slack Channel\n\nimport json\nimport great_expectations as ge\nimport great_expectations.jupyter_ux\nfrom great_expectations.datasource.types import BatchKwargs\nfrom datetime import datetime\n\n\n\n\nThis represents your project that you just created using great_expectations init.\n\ncontext = ge.data_context.DataContext()\n\n\n\n\nList expectation suites that you created in your project\n\ncontext.list_expectation_suite_names()\n\n\nexpectation_suite_name =  # TODO: set to a name from the list above\n\n\n\n\nTo learn more about get_batch, see this tutorial\n\n# list datasources of the type SparkDFDatasource in your project\n[datasource['name'] for datasource in context.list_datasources() if datasource['class_name'] == 'SparkDFDatasource']\n\n\ndatasource_name = # TODO: set to a datasource name from above\n\n\n# If you would like to validate a file on a filesystem:\nbatch_kwargs = {'path': \"YOUR_FILE_PATH\", 'datasource': datasource_name}\n# To customize how Spark reads the file, you can add options under reader_options key in batch_kwargs (e.g., header='true') \n\n# If you already loaded the data into a PySpark Data Frame:\nbatch_kwargs = {'dataset': \"YOUR_DATAFRAME\", 'datasource': datasource_name}\n\n\nbatch = context.get_batch(batch_kwargs, expectation_suite_name)\nbatch.head()\n\n\n\n\nValidation Operators provide a convenient way to bundle the validation of multiple expectation suites and the actions that should be taken after validation.\nWhen deploying Great Expectations in a real data pipeline, you will typically discover these needs:\n\nvalidating a group of batches that are logically related\nvalidating a batch against several expectation suites such as using a tiered pattern like warning and failure\ndoing something with the validation results (e.g., saving them for a later review, sending notifications in case of failures, etc.).\n\nRead more about Validation Operators in the tutorial\n\n# This is an example of invoking a validation operator that is configured by default in the great_expectations.yml file\n\n#Generate a run id, a timestamp, or a meaningful string that will help you refer to validation results. We recommend they be chronologically sortable.\n# Let's make a simple sortable timestamp. Note this could come from your pipeline runner (e.g., Airflow run id).\nrun_id = datetime.utcnow().isoformat().replace(\":\", \"\") + \"Z\"\n\nresults = context.run_validation_operator(\n    \"action_list_operator\", \n    assets_to_validate=[batch], \n    run_id=run_id)\n\n\n\n\nLet’s now build and look at your Data Docs. These will now include an data quality report built from the ValidationResults you just created that helps you communicate about your data with both machines and humans.\nRead more about Data Docs in the tutorial\n\ncontext.open_data_docs()\n\n\n\n\n\n\n\n\n\ntypical workflow\n\n\n\nYou are now among the elite data professionals who know how to build robust descriptions of your data and protections for pipelines and machine learning models. Join the Great Expectations Slack Channel to see how others are wielding these superpowers."
  },
  {
    "objectID": "gx_tutorials/gx_dbt_airflow_tutorial/great_expectations_projects/final/great_expectations/notebooks/sql/validation_playground.html",
    "href": "gx_tutorials/gx_dbt_airflow_tutorial/great_expectations_projects/final/great_expectations/notebooks/sql/validation_playground.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Watch a short tutorial video or read the written tutorial\n\n\n\n\n\nWe’d love it if you reach out for help on the Great Expectations Slack Channel\n\nimport json\nimport great_expectations as ge\nimport great_expectations.jupyter_ux\nfrom great_expectations.datasource.types import BatchKwargs\nfrom datetime import datetime\n\n\n\n\nThis represents your project that you just created using great_expectations init.\n\ncontext = ge.data_context.DataContext()\n\n\n\n\nList expectation suites that you created in your project\n\ncontext.list_expectation_suite_names()\n\n\nexpectation_suite_name =  # TODO: set to a name from the list above\n\n\n\n\nTo learn more about get_batch, see this tutorial\n\n# list datasources of the type SqlAlchemyDatasource in your project\n[datasource['name'] for datasource in context.list_datasources() if datasource['class_name'] == 'SqlAlchemyDatasource']\n\n\ndatasource_name = # TODO: set to a datasource name from above\n\n\n# If you would like to validate an entire table or view in your database's default schema:\nbatch_kwargs = {'table': \"YOUR_TABLE\", 'datasource': datasource_name}\n\n# If you would like to validate an entire table or view from a non-default schema in your database:\nbatch_kwargs = {'table': \"YOUR_TABLE\", \"schema\": \"YOUR_SCHEMA\", 'datasource': datasource_name}\n\n# If you would like to validate the result set of a query:\n# batch_kwargs = {'query': 'SELECT YOUR_ROWS FROM YOUR_TABLE', 'datasource': datasource_name}\n\n\n\nbatch = context.get_batch(batch_kwargs, expectation_suite_name)\nbatch.head()\n\n\n\n\nValidation Operators provide a convenient way to bundle the validation of multiple expectation suites and the actions that should be taken after validation.\nWhen deploying Great Expectations in a real data pipeline, you will typically discover these needs:\n\nvalidating a group of batches that are logically related\nvalidating a batch against several expectation suites such as using a tiered pattern like warning and failure\ndoing something with the validation results (e.g., saving them for a later review, sending notifications in case of failures, etc.).\n\nRead more about Validation Operators in the tutorial\n\n# This is an example of invoking a validation operator that is configured by default in the great_expectations.yml file\n\n#Generate a run id, a timestamp, or a meaningful string that will help you refer to validation results. We recommend they be chronologically sortable.\n# Let's make a simple sortable timestamp. Note this could come from your pipeline runner (e.g., Airflow run id).\nrun_id = datetime.utcnow().isoformat().replace(\":\", \"\") + \"Z\"\n\nresults = context.run_validation_operator(\n    \"action_list_operator\", \n    assets_to_validate=[batch], \n    run_id=run_id)\n\n\n\n\nLet’s now build and look at your Data Docs. These will now include an data quality report built from the ValidationResults you just created that helps you communicate about your data with both machines and humans.\nRead more about Data Docs in the tutorial\n\ncontext.open_data_docs()\n\n\n\n\n\n\n\n\n\ntypical workflow\n\n\n\nYou are now among the elite data professionals who know how to build robust descriptions of your data and protections for pipelines and machine learning models. Join the Great Expectations Slack Channel to see how others are wielding these superpowers."
  },
  {
    "objectID": "gx_tutorials/gx_dbt_airflow_tutorial/great_expectations_projects/final/great_expectations/notebooks/pandas/validation_playground.html",
    "href": "gx_tutorials/gx_dbt_airflow_tutorial/great_expectations_projects/final/great_expectations/notebooks/pandas/validation_playground.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Watch a short tutorial video or read the written tutorial\n\n\n\n\n\nWe’d love it if you reach out for help on the Great Expectations Slack Channel\n\nimport json\nimport great_expectations as ge\nimport great_expectations.jupyter_ux\nfrom great_expectations.datasource.types import BatchKwargs\nfrom datetime import datetime\n\nOld pybigquery driver version detected. Consider upgrading to 0.4.14 or later.\n\n\n2020-03-27T12:13:30-0700 - INFO - Great Expectations logging enabled at INFO level by JupyterUX module.\n\n\n\n\n\nThis represents your project that you just created using great_expectations init.\n\ncontext = ge.data_context.DataContext()\n\n\n\n\nList expectation suites that you created in your project\n\ncontext.list_expectation_suite_names()\n\n['npi_small_file.critical']\n\n\n\nexpectation_suite_name =  'npi_small_file.critical'# TODO: set to a name from the list above\n\n\n\n\nTo learn more about get_batch, see this tutorial\n\n# list datasources of the type PandasDatasource in your project\n[datasource['name'] for datasource in context.list_datasources() if datasource['class_name'] == 'PandasDatasource']\n\n['files_datasource']\n\n\n\ndatasource_name = 'files_datasource'# TODO: set to a datasource name from above\n\n\n# If you would like to validate a file on a filesystem:\nbatch_kwargs = {'path': \"/Users/eugenemandel/projects/ge_tutorials/data/npi_small.csv\", 'datasource': datasource_name}\n\n# # If you already loaded the data into a Pandas Data Frame:\n# batch_kwargs = {'dataset': \"YOUR_DATAFRAME\", 'datasource': datasource_name}\n\n\nbatch = context.get_batch(batch_kwargs, expectation_suite_name)\nbatch.head()\n\n\n\n\n\n  \n    \n      \n      NPI\n      Entity_Type_Code\n      Organization_Name\n      Last_Name\n      First_Name\n      State\n      Taxonomy_Code\n    \n  \n  \n    \n      0\n      1457900839\n      2.0\n      TEXAS CLINIC OF CHIROPRACTIC\n      NaN\n      NaN\n      TX\n      111N00000X\n    \n    \n      1\n      1255519047\n      1.0\n      NaN\n      BRYANT-JONES\n      MARIA\n      FL\n      261QH0700X\n    \n    \n      2\n      1366091746\n      1.0\n      NaN\n      JONES\n      EBONY\n      DC\n      3747P1801X\n    \n    \n      3\n      1275182651\n      1.0\n      NaN\n      ORNELAS\n      LUPE\n      CA\n      101YA0400X\n    \n    \n      4\n      1194371344\n      1.0\n      NaN\n      WINTERS\n      STACY\n      MD\n      363L00000X\n    \n  \n\n\n\n\n\nbatch.validate()\n\n2020-03-27T11:18:30-0700 - INFO -   14 expectation(s) included in expectation_suite.\n\n\n{\n  \"evaluation_parameters\": {},\n  \"statistics\": {\n    \"evaluated_expectations\": 14,\n    \"successful_expectations\": 14,\n    \"unsuccessful_expectations\": 0,\n    \"success_percent\": 100.0\n  },\n  \"results\": [\n    {\n      \"result\": {\n        \"observed_value\": 18649\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"min_value\": 18639,\n          \"max_value\": 18659\n        },\n        \"expectation_type\": \"expect_table_row_count_to_be_between\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    },\n    {\n      \"result\": {\n        \"observed_value\": 7\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"value\": 7\n        },\n        \"expectation_type\": \"expect_table_column_count_to_equal\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    },\n    {\n      \"result\": {\n        \"observed_value\": [\n          \"NPI\",\n          \"Entity_Type_Code\",\n          \"Organization_Name\",\n          \"Last_Name\",\n          \"First_Name\",\n          \"State\",\n          \"Taxonomy_Code\"\n        ]\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"column_list\": [\n            \"NPI\",\n            \"Entity_Type_Code\",\n            \"Organization_Name\",\n            \"Last_Name\",\n            \"First_Name\",\n            \"State\",\n            \"Taxonomy_Code\"\n          ]\n        },\n        \"expectation_type\": \"expect_table_columns_to_match_ordered_list\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    },\n    {\n      \"result\": {\n        \"element_count\": 18649,\n        \"unexpected_count\": 491,\n        \"unexpected_percent\": 2.6328489463241995,\n        \"partial_unexpected_list\": []\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"column\": \"Entity_Type_Code\",\n          \"mostly\": 0.873671510536758\n        },\n        \"expectation_type\": \"expect_column_values_to_not_be_null\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    },\n    {\n      \"result\": {\n        \"observed_value\": [\n          1.0,\n          2.0\n        ],\n        \"element_count\": 18649,\n        \"missing_count\": 491,\n        \"missing_percent\": 2.6328489463241995\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"column\": \"Entity_Type_Code\",\n          \"value_set\": [\n            1.0,\n            2.0\n          ]\n        },\n        \"expectation_type\": \"expect_column_distinct_values_to_be_in_set\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    },\n    {\n      \"result\": {\n        \"observed_value\": 0.0,\n        \"element_count\": 18649,\n        \"missing_count\": 491,\n        \"missing_percent\": 2.6328489463241995\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"column\": \"Entity_Type_Code\",\n          \"partition_object\": {\n            \"values\": [\n              1.0,\n              2.0\n            ],\n            \"weights\": [\n              0.812314131512281,\n              0.1876858684877189\n            ]\n          },\n          \"threshold\": 0.6\n        },\n        \"expectation_type\": \"expect_column_kl_divergence_to_be_less_than\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    },\n    {\n      \"result\": {\n        \"element_count\": 18649,\n        \"unexpected_count\": 0,\n        \"unexpected_percent\": 0.0,\n        \"partial_unexpected_list\": []\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"column\": \"NPI\"\n        },\n        \"expectation_type\": \"expect_column_values_to_not_be_null\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    },\n    {\n      \"result\": {\n        \"observed_value\": 1003007766,\n        \"element_count\": 18649,\n        \"missing_count\": 0,\n        \"missing_percent\": 0.0\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"column\": \"NPI\",\n          \"min_value\": 1003007765,\n          \"max_value\": 1003007767\n        },\n        \"expectation_type\": \"expect_column_min_to_be_between\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    },\n    {\n      \"result\": {\n        \"observed_value\": 1992999676,\n        \"element_count\": 18649,\n        \"missing_count\": 0,\n        \"missing_percent\": 0.0\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"column\": \"NPI\",\n          \"min_value\": 1992999675,\n          \"max_value\": 1992999677\n        },\n        \"expectation_type\": \"expect_column_max_to_be_between\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    },\n    {\n      \"result\": {\n        \"observed_value\": 1500841664.0457933,\n        \"element_count\": 18649,\n        \"missing_count\": 0,\n        \"missing_percent\": 0.0\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"column\": \"NPI\",\n          \"min_value\": 1500841663.0457933,\n          \"max_value\": 1500841665.0457933\n        },\n        \"expectation_type\": \"expect_column_mean_to_be_between\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    },\n    {\n      \"result\": {\n        \"observed_value\": 1508307745.0,\n        \"element_count\": 18649,\n        \"missing_count\": 0,\n        \"missing_percent\": 0.0\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"column\": \"NPI\",\n          \"min_value\": 1508307744.0,\n          \"max_value\": 1508307746.0\n        },\n        \"expectation_type\": \"expect_column_median_to_be_between\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    },\n    {\n      \"result\": {\n        \"observed_value\": {\n          \"quantiles\": [\n            0.05,\n            0.25,\n            0.5,\n            0.75,\n            0.95\n          ],\n          \"values\": [\n            1053339952,\n            1245889518,\n            1508307745,\n            1750668489,\n            1952551186\n          ]\n        },\n        \"element_count\": 18649,\n        \"missing_count\": 0,\n        \"missing_percent\": 0.0\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"column\": \"NPI\",\n          \"quantile_ranges\": {\n            \"quantiles\": [\n              0.05,\n              0.25,\n              0.5,\n              0.75,\n              0.95\n            ],\n            \"value_ranges\": [\n              [\n                1053339951,\n                1053339953\n              ],\n              [\n                1245889517,\n                1245889519\n              ],\n              [\n                1508307744,\n                1508307746\n              ],\n              [\n                1750668488,\n                1750668490\n              ],\n              [\n                1952551185,\n                1952551187\n              ]\n            ]\n          }\n        },\n        \"expectation_type\": \"expect_column_quantile_values_to_be_between\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    },\n    {\n      \"result\": {\n        \"element_count\": 18649,\n        \"unexpected_count\": 15241,\n        \"unexpected_percent\": 81.72556169231594,\n        \"partial_unexpected_list\": []\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"column\": \"Organization_Name\",\n          \"mostly\": 0.08274438307684065\n        },\n        \"expectation_type\": \"expect_column_values_to_not_be_null\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    },\n    {\n      \"result\": {\n        \"element_count\": 18649,\n        \"missing_count\": 15241,\n        \"missing_percent\": 81.72556169231594,\n        \"unexpected_count\": 0,\n        \"unexpected_percent\": 0.0,\n        \"unexpected_percent_nonmissing\": 0.0,\n        \"partial_unexpected_list\": []\n      },\n      \"expectation_config\": {\n        \"kwargs\": {\n          \"column\": \"Organization_Name\",\n          \"min_value\": 1\n        },\n        \"expectation_type\": \"expect_column_value_lengths_to_be_between\",\n        \"meta\": {\n          \"SampleExpectationsDatasetProfiler\": {\n            \"confidence\": \"very low\"\n          }\n        }\n      },\n      \"success\": true,\n      \"exception_info\": {\n        \"raised_exception\": false,\n        \"exception_message\": null,\n        \"exception_traceback\": null\n      },\n      \"meta\": {}\n    }\n  ],\n  \"success\": true,\n  \"meta\": {\n    \"great_expectations.__version__\": \"0.9.7+228.g7b410a57\",\n    \"expectation_suite_name\": \"npi_small_file.critical\",\n    \"run_id\": \"20200327T181830.633221Z\",\n    \"batch_kwargs\": {\n      \"path\": \"/Users/eugenemandel/projects/ge_tutorials/data/npi_small.csv\",\n      \"datasource\": \"files_datasource\"\n    },\n    \"batch_markers\": {\n      \"ge_load_time\": \"20200327T181801.219912Z\",\n      \"pandas_data_fingerprint\": \"a5ebd04919bde23bcf25afadb9e661fb\"\n    },\n    \"batch_parameters\": null\n  }\n}\n\n\n\ntype(batch)\n\ngreat_expectations.dataset.pandas_dataset.PandasDataset\n\n\n\n\n\nValidation Operators provide a convenient way to bundle the validation of multiple expectation suites and the actions that should be taken after validation.\nWhen deploying Great Expectations in a real data pipeline, you will typically discover these needs:\n\nvalidating a group of batches that are logically related\nvalidating a batch against several expectation suites such as using a tiered pattern like warning and failure\ndoing something with the validation results (e.g., saving them for a later review, sending notifications in case of failures, etc.).\n\nRead more about Validation Operators in the tutorial\n\n# This is an example of invoking a validation operator that is configured by default in the great_expectations.yml file\n\n#Generate a run id, a timestamp, or a meaningful string that will help you refer to validation results. We recommend they be chronologically sortable.\n# Let's make a simple sortable timestamp. Note this could come from your pipeline runner (e.g., Airflow run id).\nrun_id = datetime.utcnow().isoformat().replace(\":\", \"\") + \"Z\"\n\nresults = context.run_validation_operator(\n    \"action_list_operator\", \n    assets_to_validate=[batch], \n    run_id=run_id)\n\n2020-03-27T12:13:41-0700 - INFO -   14 expectation(s) included in expectation_suite.\n\n\n\n\n\nLet’s now build and look at your Data Docs. These will now include an data quality report built from the ValidationResults you just created that helps you communicate about your data with both machines and humans.\nRead more about Data Docs in the tutorial\n\ncontext.open_data_docs()\n\n\n\n\n\n\n\n\n\ntypical workflow\n\n\n\nYou are now among the elite data professionals who know how to build robust descriptions of your data and protections for pipelines and machine learning models. Join the Great Expectations Slack Channel to see how others are wielding these superpowers."
  },
  {
    "objectID": "gx_tutorials/gx_glue_catalog_tutorial/notebooks/GE-Demo-GlueCatalog-QuickStart.html",
    "href": "gx_tutorials/gx_glue_catalog_tutorial/notebooks/GE-Demo-GlueCatalog-QuickStart.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "In this notebook, we will walk through the steps-by-steps to setup Great Expectations with AWS Glue Data Catalog. Before you can start running this notebook, you must start an interactive session.\n\n\nBefore starting coding, be aware that there are a few options to set up an AWS Glue interactive session. In the following code cell, we are using some of them. Refer to the AWS glue interactive session documentation for a complete view of the options available.\nRun the next code cell to setup the interactive session:\n\n%additional_python_modules great_expectations\n%glue_version 3.0\n%number_of_workers 2\n\nOnce you run the cell, the interactive session will be configured to use Glue 3.0, allocate 2 DPUs and install the Great Expectations package. Feel free to add or modify this setup as you like. The interactive session will not be created until we execute some code. Let’s start the session by running the following code cell to import some standard Glue libraries:\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\n\n\n\nFor the sake of simplicity, we will create the Great Expectations Data Context in-memory using an Amazon S3 bucket as the store backend. The following example shows a Data Context configuration with a Spark datasource using the new AWS Glue Catalog Data Connector. Refer to the following documentation for a better understanding.\nRun the following cell to create the Data Context:\n\nfrom great_expectations.data_context.types.base import DataContextConfig, DatasourceConfig, S3StoreBackendDefaults\nfrom great_expectations.data_context import BaseDataContext\n\ndata_connectors = {\n    \"Runtime\": {\n        \"class_name\": \"RuntimeDataConnector\",\n        \"batch_identifiers\": [\"batch_id\"]\n    },\n    \"InferredGlue\": {\n        \"class_name\": \"InferredAssetAWSGlueDataCatalogDataConnector\"\n    },\n    \"ConfiguredGlue\": {\n        \"class_name\": \"ConfiguredAssetAWSGlueDataCatalogDataConnector\",\n        \"assets\": {\n            \"nyc_trip_data_asset\": {\n                \"table_name\": \"tb_nyc_trip_data\",\n                \"database_name\": \"db_ge_with_glue_demo\"\n            }\n        }\n    }\n}\n\nglue_data_source = DatasourceConfig(\n    class_name=\"Datasource\",\n    execution_engine={\n        \"class_name\": \"SparkDFExecutionEngine\",\n        \"force_reuse_spark_context\": True,\n    },\n    data_connectors=data_connectors\n)\n\nmetastore_backed = S3StoreBackendDefaults(default_bucket_name=\"great-expectations-glue-demo-<AWS_ACCOUNT_ID>-us-east-1\")\n\ndata_context_config = DataContextConfig(\n    datasources={\"GlueDataSource\": glue_data_source},\n    store_backend_defaults=metastore_backed,\n)\n\ncontext = BaseDataContext(project_config=data_context_config)\n\nNow that we have a DataContext, we can check for the available data assets that the AWS Glue Data Connector was able to get from the Glue Catalog. Run the following code to get a list of available data assets:\n\nfrom pprint import pprint\n\npprint(context.get_available_data_asset_names())\n\nIf you have deployed the provided terraform code into you AWS account, you shall see that the InferredGlue connector returned the table db_ge_with_glue_demo.tb_nyc_trip_data. This table was created as part of the solution deployment. If you have more tables in the Glue Catalog, this connector shall output all of them.\nFor a fine-grained control of what tables shall be available, use the ConfiguredGlue as an example. The Configured Connector requires that you define each database table you would like to validate. Be aware that the connector will not validate if, for any of the tables you define, the table exists nor if you have permissions to access it. Note: If you’re using AWS Lake Formation, check if the Glue IAM role has permissions to access it.\n\n\n\nOnce you have the data context already setup, you can start to create the Expectations for our tables. Run the following code cell to create an Expectation Suite and a validator that we can use to create the expectations interactively.\n\nfrom great_expectations.core.batch import BatchRequest\n\nexpectation_suite_name = \"demo.taxi_trip.warning\"\n\n# Create Expectation Suite\nsuite = context.create_expectation_suite(\n    expectation_suite_name=expectation_suite_name,\n    overwrite_existing=True,\n)\n\n# Batch Request\nbatch_request = BatchRequest(\n    datasource_name=\"GlueDataSource\",\n    data_connector_name=\"InferredGlue\",\n    data_asset_name=\"db_ge_with_glue_demo.tb_nyc_trip_data\",\n    data_connector_query={\n        \"batch_filter_parameters\": {\n            \"year\": \"2022\", \n            \"month\": \"03\"\n        }\n    }\n)\n\n# Validator\nvalidator = context.get_validator(\n    batch_request=batch_request,\n    expectation_suite_name=expectation_suite_name,\n)\n\n# Print\ndf = validator.head(n_rows=5, fetch_all=False)\npprint(df.info())\n\nBe aware that, the table we are using in this demo, is partitioned by year and month. Check the Table in Glue Data Catalog and you shall see that there are three partitions, named: 2022-01, 2022-02, 2022-03. For each table partition, the connector will create a batch identifier that allows us to filter a batch of data based on the partition values. In the code above, we are loading the partition 2022-03. If you do not specify the partition, the connector will get the first partition available by default. Note: filtering a partition is only available if your table is partitioned, otherwise, the table data will be loaded in a single batch.\nAfter running the code cell above, you can start to define the Expectations you want. Use the following code as example on how to define the Expectations and save it:\n\n# Define the Expectations\nvalidator.expect_table_row_count_to_be_between(min_value=1, max_value=None)\nvalidator.expect_column_values_to_not_be_null(column=\"vendorid\")\nvalidator.expect_column_values_to_be_between(column=\"passenger_count\", min_value=0, max_value=9)\n\n# Save the Expectation Suite\nvalidator.save_expectation_suite(discard_failed_expectations=False)\n\nValidate that the Expectation Suite was saved into the S3 Bucket we have defined as our store backend.\n\n\n\nWith the Expectation Suite already created, we can create a Checkpoint to validate data. Run the following code cell to create, save, and run the Checkpoint to get the validation results:\n\nfrom ruamel import yaml\n\n# Create Checkpoint\nmy_checkpoint_name = \"demo.taxi_trip.checkpoint\" \n\nyaml_config = f\"\"\"\nname: {my_checkpoint_name}\nconfig_version: 1.0\nmodule_name: great_expectations.checkpoint\nclass_name: Checkpoint\nrun_name_template: \"%Y%m%d-TaxiTrip-GlueInferred\"\naction_list:\n  - name: store_validation_result\n    action:\n      class_name: StoreValidationResultAction\n  - name: update_data_docs\n    action:\n      class_name: UpdateDataDocsAction\n      site_names: []\nvalidations:\n  - batch_request:\n      datasource_name: GlueDataSource\n      data_connector_name: InferredGlue\n      data_asset_name: db_ge_with_glue_demo.tb_nyc_trip_data\n      data_connector_query:\n        batch_filter_parameters:\n          year: '2022'\n          month: '03'\n    expectation_suite_name: {expectation_suite_name}\n\"\"\"\n\n# Save Checkpoint\n_ = context.add_checkpoint(**yaml.load(yaml_config))\n\n# Run Checkpoint\nr = context.run_checkpoint(checkpoint_name=my_checkpoint_name)\npprint(r)\n\n🚀🚀 Congratulations! 🚀🚀 You’ve successfully connected Great Expectations with your data through AWS Glue Data Catalog."
  },
  {
    "objectID": "gx_tutorials/great_expectations/uncommitted/edit_checkpoint_data_quality_demo_checkpoint.html",
    "href": "gx_tutorials/great_expectations/uncommitted/edit_checkpoint_data_quality_demo_checkpoint.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Create Your Checkpoint\nUse this notebook to configure a new Checkpoint and add it to your project:\nCheckpoint Name: data_quality_demo_checkpoint\n\nfrom ruamel.yaml import YAML\nimport great_expectations as gx\nfrom pprint import pprint\n\nyaml = YAML()\ncontext = gx.get_context()\n\n\n\nCreate a Checkpoint Configuration\nIf you are new to Great Expectations or the Checkpoint feature, you should start with SimpleCheckpoint because it includes default configurations like a default list of post validation actions.\nIn the cell below we have created a sample Checkpoint configuration using your configuration and SimpleCheckpoint to run a single validation of a single Expectation Suite against a single Batch of data.\nTo keep it simple, we are just choosing the first available instance of each of the following items you have configured in your Data Context: * Datasource * DataConnector * DataAsset * Partition * Expectation Suite\nOf course this is purely an example, you may edit this to your heart’s content.\nMy configuration is not so simple - are there more advanced options?\nGlad you asked! Checkpoints are very versatile. For example, you can validate many Batches in a single Checkpoint, validate Batches against different Expectation Suites or against many Expectation Suites, control the specific post-validation actions based on Expectation Suite / Batch / results of validation among other features. Check out our documentation on Checkpoints for more details and for instructions on how to implement other more advanced features including using the Checkpoint class: - https://docs.greatexpectations.io/docs/reference/checkpoints_and_actions - https://docs.greatexpectations.io/docs/guides/validation/checkpoints/how_to_create_a_new_checkpoint - https://docs.greatexpectations.io/docs/guides/validation/checkpoints/how_to_configure_a_new_checkpoint_using_test_yaml_config\n\nmy_checkpoint_name = \"data_quality_demo_checkpoint\" # This was populated from your CLI command.\n\nyaml_config = f\"\"\"\nname: {my_checkpoint_name}\nconfig_version: 1.0\nclass_name: SimpleCheckpoint\nrun_name_template: \"%Y%m%d-%H%M%S-my-run-name-template\"\nvalidations:\n  - batch_request:\n      datasource_name: data_quality_demo\n      data_connector_name: default_inferred_data_connector_name\n      data_asset_name: yellow_tripdata_sample_2019-02.csv\n      data_connector_query:\n        index: -1\n    expectation_suite_name: data_quality_expectation_demo\n\"\"\"\nprint(yaml_config)\n\n\n\nCustomize Your Configuration\nThe following cells show examples for listing your current configuration. You can replace values in the sample configuration with these values to customize your Checkpoint.\n\n# Run this cell to print out the names of your Datasources, Data Connectors and Data Assets\npprint(context.get_available_data_asset_names())\n\n\ncontext.list_expectation_suite_names()\n\n\n\nTest Your Checkpoint Configuration\nHere we will test your Checkpoint configuration to make sure it is valid.\nThis test_yaml_config() function is meant to enable fast dev loops. If your configuration is correct, this cell will show a message that you successfully instantiated a Checkpoint. You can continually edit your Checkpoint config yaml and re-run the cell to check until the new config is valid.\nIf you instead wish to use python instead of yaml to configure your Checkpoint, you can use context.add_checkpoint() and specify all the required parameters.\n\nmy_checkpoint = context.test_yaml_config(yaml_config=yaml_config)\n\n\n\nReview Your Checkpoint\nYou can run the following cell to print out the full yaml configuration. For example, if you used SimpleCheckpoint this will show you the default action list.\n\nprint(my_checkpoint.get_config(mode=\"yaml\"))\n\n\n\nAdd Your Checkpoint\nRun the following cell to save this Checkpoint to your Checkpoint Store.\n\ncontext.add_checkpoint(**yaml.load(yaml_config))\n\n\n\nRun Your Checkpoint & Open Data Docs(Optional)\nYou may wish to run the Checkpoint now and review its output in Data Docs. If so uncomment and run the following cell.\n\n# context.run_checkpoint(checkpoint_name=my_checkpoint_name)\n# context.open_data_docs()"
  },
  {
    "objectID": "gx_tutorials/great_expectations/uncommitted/edit_data_quality_expectation_demo.html",
    "href": "gx_tutorials/great_expectations/uncommitted/edit_data_quality_expectation_demo.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "This process helps you avoid writing lots of boilerplate when authoring suites by allowing you to select columns and other factors that you care about and letting a profiler write some candidate expectations for you to adjust.\nExpectation Suite Name: data_quality_expectation_demo\n\nimport datetime\n\nimport pandas as pd\n\nimport great_expectations as gx\nimport great_expectations.jupyter_ux\nfrom great_expectations.core.batch import BatchRequest\nfrom great_expectations.checkpoint import SimpleCheckpoint\nfrom great_expectations.exceptions import DataContextError\n\ncontext = gx.data_context.DataContext()\n\nbatch_request = {'datasource_name': 'data_quality_demo', 'data_connector_name': 'default_inferred_data_connector_name', 'data_asset_name': 'yellow_tripdata_sample_2019-01.csv', 'limit': 1000}\n\nexpectation_suite_name = \"data_quality_expectation_demo\"\n\nvalidator = context.get_validator(\n    batch_request=BatchRequest(**batch_request),\n    expectation_suite_name=expectation_suite_name\n)\ncolumn_names = [f'\"{column_name}\"' for column_name in validator.columns()]\nprint(f\"Columns: {', '.join(column_names)}.\")\nvalidator.head(n_rows=5, fetch_all=False)"
  },
  {
    "objectID": "gx_tutorials/great_expectations/uncommitted/edit_data_quality_expectation_demo.html#next-steps",
    "href": "gx_tutorials/great_expectations/uncommitted/edit_data_quality_expectation_demo.html#next-steps",
    "title": "GiveDirectly",
    "section": "Next steps",
    "text": "Next steps\nAfter you review this initial Expectation Suite in Data Docs you should edit this suite to make finer grained adjustments to the expectations. This can be done by running great_expectations suite edit data_quality_expectation_demo."
  },
  {
    "objectID": "gx_tutorials/great_expectations/uncommitted/datasource_new.html",
    "href": "gx_tutorials/great_expectations/uncommitted/datasource_new.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Use this notebook to configure a new pandas Datasource and add it to your project.\n\nimport great_expectations as gx\nfrom great_expectations.cli.datasource import sanitize_yaml_and_save_datasource, check_if_datasource_name_exists\ncontext = gx.get_context()\n\n\n\nIf you are new to Great Expectations Datasources, you should check out our how-to documentation\nMy configuration is not so simple - are there more advanced options? Glad you asked! Datasources are versatile. Please see our How To Guides!\nGive your datasource a unique name:\n\ndatasource_name = \"my_datasource\"\n\n\n\nHere we are creating an example configuration. The configuration contains an InferredAssetFilesystemDataConnector which will add a Data Asset for each file in the base directory you provided. It also contains a RuntimeDataConnector which can accept filepaths. This is just an example, and you may customize this as you wish!\nAlso, if you would like to learn more about the DataConnectors used in this configuration, including other methods to organize assets, handle multi-file assets, name assets based on parts of a filename, please see our docs on InferredAssetDataConnectors and RuntimeDataConnectors.\n\nexample_yaml = f\"\"\"\nname: {datasource_name}\nclass_name: Datasource\nexecution_engine:\n  class_name: PandasExecutionEngine\ndata_connectors:\n  default_inferred_data_connector_name:\n    class_name: InferredAssetFilesystemDataConnector\n    base_directory: ../data\n    default_regex:\n      group_names:\n        - data_asset_name\n      pattern: (.*)\n  default_runtime_data_connector_name:\n    class_name: RuntimeDataConnector\n    assets:\n      my_runtime_asset_name:\n        batch_identifiers:\n          - runtime_batch_identifier_name\n\"\"\"\nprint(example_yaml)"
  },
  {
    "objectID": "gx_tutorials/great_expectations/uncommitted/datasource_new.html#save-your-datasource-configuration",
    "href": "gx_tutorials/great_expectations/uncommitted/datasource_new.html#save-your-datasource-configuration",
    "title": "GiveDirectly",
    "section": "Save Your Datasource Configuration",
    "text": "Save Your Datasource Configuration\nHere we will save your Datasource in your Data Context once you are satisfied with the configuration. Note that overwrite_existing defaults to False, but you may change it to True if you wish to overwrite. Please note that if you wish to include comments you must add them directly to your great_expectations.yml.\n\nsanitize_yaml_and_save_datasource(context, example_yaml, overwrite_existing=False)\ncontext.list_datasources()\n\nNow you can close this notebook and delete it!"
  },
  {
    "objectID": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#chapter-outline",
    "href": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#chapter-outline",
    "title": "GiveDirectly",
    "section": "Chapter Outline",
    "text": "Chapter Outline\n\n\n\n\nChapter Learning Objectives\n\n\nImports\n\n\n1. Introduction to spatial data\n\n\n2. Working with vector data\n\n\n3. Working with raster data\n\n\n4. Coordinate reference systems"
  },
  {
    "objectID": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#chapter-learning-objectives",
    "href": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#chapter-learning-objectives",
    "title": "GiveDirectly",
    "section": "Chapter Learning Objectives",
    "text": "Chapter Learning Objectives\n\n\nDescribe the difference between vector and raster data.\nLoad vector data into geopandas.\nPlot vector data using the geopandas method .plot().\nWrangle vector data using geopandas functions, methods, and attributes like gpd.sjoin(), gpd.clip(), .length(), .buffer(), etc.\nImport data from OpenStreetMap using osmnx.\nRead raster data with rasterio.\nDescribe at a high level why coordinate reference systems (CRS) are important and identify the CRS of a geopandas object using the .crs attribute and reproject it to another CRS using the .to_crs() method."
  },
  {
    "objectID": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#imports",
    "href": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#imports",
    "title": "GiveDirectly",
    "section": "Imports",
    "text": "Imports\n\n\nimport rasterio\nimport numpy as np\nimport osmnx as ox\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.rcParams.update({'font.size': 16, 'axes.labelweight': 'bold', 'figure.figsize': (6, 6), 'axes.edgecolor': '0.2'})"
  },
  {
    "objectID": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#introduction-to-spatial-data",
    "href": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#introduction-to-spatial-data",
    "title": "GiveDirectly",
    "section": "1. Introduction to spatial data",
    "text": "1. Introduction to spatial data\n\nSpatial data (or “geospatial” data) are data with location information. In this short course, we’ll learn to wrangle, plot, and model this kind of data to describe and understand its spatial information and dependence.\nTo get you excited about geospatial data, here’s a cool 3D map of UBC that we’ll build together next lecture:\n\n%%html\n<iframe src=\"../_images/ubc-3d.html\" width=\"80%\" height=\"500\"></iframe>\n\n\n\n\nThe first thing we need to know is that there are two main data formats used to represent spatial data: 1. Vector format 2. Raster format"
  },
  {
    "objectID": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#working-with-vector-data",
    "href": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#working-with-vector-data",
    "title": "GiveDirectly",
    "section": "2. Working with vector data",
    "text": "2. Working with vector data\n\n\n2.1. Vector data\nVector data is an intuitive and common spatial data format and the one we’ll focus on most in this chapter. Vector data is simply a collection of discrete locations ((x, y) values) called “vertices” that define one of three shapes: 1. Point: a single (x, y) point. Like the location of your house. 2. Line: two or more connected (x, y) points. Like a road. 3. Polygon: three or more (x, y) points connected and closed. Like a lake, or the border of a country.\n\n\nSource: National Ecological Observatory Network.\n\nVector data is most commonly stored in a “shapefile”. A shapefile is actually composed of 3 required files with the same prefix (here, spatial-data) but different extensions: 1. spatial-data.shp: main file that stores records of each shape geometries 2. spatial-data.shx: index of how the geometries in the main file relate to one-another 3. spatial-data.dbf: attributes of each record\nThere are other optional files that may also be part of a shapefile but we won’t worry about them for now. Each shapefile can only contain one type of shape. For example, the descriptions for a house (point), a road (line), and a postal code area (polygon) would be stored in three separate shapefiles.\n\nThere are other file-types for storing vector data too like geojson. These files can generally be imported into Python using the same methods and packages we use below.\n\n\n\n2.2. Geopandas\nThe Python geopandas library is the main library we’ll be using to work with vector data in Python. It’s built off shapely (which is the Python library for working with geometric objects in Python) and pandas. Similar to pandas, geopandas provides two key classes for spatial data manipulation: 1. GeoSeries: just like a pandas series but stores geometries like points, lines, polygons (we’ll see that shortly) 2. GeoDataFrame: just like a pandas dataframe with one or more columns of regular series and one or more columns of geoseries.\nAlong with those classes, we also have a variety of cool geopstial wrangling methods which we’ll explore in this chapter. For now, here’s a schematic of a GeoDataFrame:\n\n\nSource: GeoPandas Documentation.\n\nWe usually import geopandas using the alias gpd:\n\nimport geopandas as gpd\n\n\n\n2.3. Loading data\nLet’s take a look at loading in a shapefile to a GeoDataFrame now. I downloaded a shapefile of Candian provinces from statcan as a shapefile that looks like this:\nprovinces\n├── provinces.dbf\n├── provinces.shp\n├── provinces.shx\n└── provinces.prj  # this contains projection information which I'll dicuss later\nWe can read that shapefile using gpd.read_file():\n\nprovinces = gpd.read_file(\"data-spatial/provinces\")  # note that I point to the shapefile \"directory\" containg all the individual files\nprovinces = provinces.to_crs(\"EPSG:4326\")    # I'll explain this later, I'm converting to a different coordinate reference system\nprovinces\n\n\n\n\n\n  \n    \n      \n      PRUID\n      PRNAME\n      PRENAME\n      PRFNAME\n      PREABBR\n      PRFABBR\n      geometry\n    \n  \n  \n    \n      0\n      10\n      Newfoundland and Labrador / Terre-Neuve-et-Lab...\n      Newfoundland and Labrador\n      Terre-Neuve-et-Labrador\n      N.L.\n      T.-N.-L.\n      MULTIPOLYGON (((-57.40256 54.14965, -57.38429 ...\n    \n    \n      1\n      11\n      Prince Edward Island / Île-du-Prince-Édouard\n      Prince Edward Island\n      Île-du-Prince-Édouard\n      P.E.I.\n      Î.-P.-É.\n      MULTIPOLYGON (((-61.98300 46.45775, -61.98136 ...\n    \n    \n      2\n      12\n      Nova Scotia / Nouvelle-Écosse\n      Nova Scotia\n      Nouvelle-Écosse\n      N.S.\n      N.-É.\n      MULTIPOLYGON (((-61.90233 45.87878, -61.90057 ...\n    \n    \n      3\n      13\n      New Brunswick / Nouveau-Brunswick\n      New Brunswick\n      Nouveau-Brunswick\n      N.B.\n      N.-B.\n      MULTIPOLYGON (((-64.80155 47.80365, -64.80155 ...\n    \n    \n      4\n      24\n      Quebec / Québec\n      Quebec\n      Québec\n      Que.\n      Qc\n      MULTIPOLYGON (((-58.64703 51.20816, -58.63991 ...\n    \n    \n      5\n      35\n      Ontario\n      Ontario\n      Ontario\n      Ont.\n      Ont.\n      MULTIPOLYGON (((-88.86612 56.84777, -88.86838 ...\n    \n    \n      6\n      46\n      Manitoba\n      Manitoba\n      Manitoba\n      Man.\n      Man.\n      MULTIPOLYGON (((-94.82341 59.99352, -94.82398 ...\n    \n    \n      7\n      47\n      Saskatchewan\n      Saskatchewan\n      Saskatchewan\n      Sask.\n      Sask.\n      POLYGON ((-109.63748 60.00005, -109.62498 60.0...\n    \n    \n      8\n      48\n      Alberta\n      Alberta\n      Alberta\n      Alta.\n      Alb.\n      POLYGON ((-110.00001 59.95257, -110.00001 59.9...\n    \n    \n      9\n      59\n      British Columbia / Colombie-Britannique\n      British Columbia\n      Colombie-Britannique\n      B.C.\n      C.-B.\n      MULTIPOLYGON (((-135.40000 60.00006, -135.3875...\n    \n    \n      10\n      60\n      Yukon\n      Yukon\n      Yukon\n      Y.T.\n      Yn\n      MULTIPOLYGON (((-136.46988 68.86889, -136.4686...\n    \n    \n      11\n      61\n      Northwest Territories / Territoires du Nord-Ouest\n      Northwest Territories\n      Territoires du Nord-Ouest\n      N.W.T.\n      T.N.-O.\n      MULTIPOLYGON (((-134.49321 69.70799, -134.4947...\n    \n    \n      12\n      62\n      Nunavut\n      Nunavut\n      Nunavut\n      Nvt.\n      Nt\n      MULTIPOLYGON (((-94.65114 72.00040, -94.64164 ...\n    \n  \n\n\n\n\n\ntype(provinces)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\nBecause geopandas is built off pandas, our GeoDataFrame inherits most of the same functionality as a regular dataframe. For example, let’s try the .info() method:\n\nprovinces.info()\n\n<class 'geopandas.geodataframe.GeoDataFrame'>\nRangeIndex: 13 entries, 0 to 12\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   PRUID     13 non-null     object  \n 1   PRNAME    13 non-null     object  \n 2   PRENAME   13 non-null     object  \n 3   PRFNAME   13 non-null     object  \n 4   PREABBR   13 non-null     object  \n 5   PRFABBR   13 non-null     object  \n 6   geometry  13 non-null     geometry\ndtypes: geometry(1), object(6)\nmemory usage: 856.0+ bytes\n\n\nNote we have 5 columns of dtype “object” which typically means “strings” in pandas, and we have our one “geometry” column which contains vector data - polygons in this case. Well we have MULTIPOLYGONs which just means multiple polygons together, for example British Columbia has a lot of little islands, so to make a boundary of it, we need multiple polygons.\ngeopandas has built-in plotting functionality (just like pandas) which is useful for making quick plots to visualize your data:\n\nprovinces.plot(edgecolor=\"0.2\", figsize=(10, 8))\nplt.title(\"Canada Provinces and Territories\");\n\n\n\n\nThat looks like Canada to me! We can also index our GeoDataFrame just like a regular dataframe:\n\nprovinces.iloc[[0]]\n\n\n\n\n\n  \n    \n      \n      PRUID\n      PRNAME\n      PRENAME\n      PRFNAME\n      PREABBR\n      PRFABBR\n      geometry\n    \n  \n  \n    \n      0\n      10\n      Newfoundland and Labrador / Terre-Neuve-et-Lab...\n      Newfoundland and Labrador\n      Terre-Neuve-et-Labrador\n      N.L.\n      T.-N.-L.\n      MULTIPOLYGON (((-57.40256 54.14965, -57.38429 ...\n    \n  \n\n\n\n\n\nname = provinces.iloc[0][\"PRENAME\"]\nprovinces.iloc[[0]].plot(edgecolor=\"0.2\", figsize=(10, 8))\nplt.title(name);\n\n\n\n\nLet’s filter our dataframe for only British Columbia using the helpful pandas method .query():\n\nprovince = \"British Columbia\"\nbc = provinces.query(\"PRENAME == @province\").copy()\nbc\n\n\n\n\n\n  \n    \n      \n      PRUID\n      PRNAME\n      PRENAME\n      PRFNAME\n      PREABBR\n      PRFABBR\n      geometry\n    \n  \n  \n    \n      9\n      59\n      British Columbia / Colombie-Britannique\n      British Columbia\n      Colombie-Britannique\n      B.C.\n      C.-B.\n      MULTIPOLYGON (((-135.40000 60.00006, -135.3875...\n    \n  \n\n\n\n\nNow, let’s do some simple manipulation to clean up our dataframe:\n\nbc = (bc.loc[:, [\"PRENAME\", \"geometry\"]]\n        .rename(columns={\"PRNAME\": \"Province\"})\n        .reset_index(drop=True)\n      )\nbc\n\n\n\n\n\n  \n    \n      \n      PRENAME\n      geometry\n    \n  \n  \n    \n      0\n      British Columbia\n      MULTIPOLYGON (((-135.40000 60.00006, -135.3875...\n    \n  \n\n\n\n\n\nbc.plot(edgecolor=\"0.2\")\nplt.title(\"British Columbia\");\n\n\n\n\n\n\n2.3. Making data\nTypically, you’ll be loading data from a file like we did above (or using an API as we’ll do later in this chapter). But we can also create our own vector data. Let’s create some “points” for BC’s biggest cities in a regular dataframe:\n\ncities = pd.DataFrame(\n    {\"City\": [\"Vancouver\", \"Victoria\", \"Kelowna\"],\n     \"Population\": [2_264_823, 335_696, 151_957],\n     \"Latitude\": [49.260833, 48.428333, 49.888056],\n     \"Longitude\": [-123.113889, -123.364722, -119.495556],\n    }\n)\ncities\n\n\n\n\n\n  \n    \n      \n      City\n      Population\n      Latitude\n      Longitude\n    \n  \n  \n    \n      0\n      Vancouver\n      2264823\n      49.260833\n      -123.113889\n    \n    \n      1\n      Victoria\n      335696\n      48.428333\n      -123.364722\n    \n    \n      2\n      Kelowna\n      151957\n      49.888056\n      -119.495556\n    \n  \n\n\n\n\nWe can coerce that data into a GeoDataFrame using gpd.GeoDataFrame() and by using the function gpd.points_from_xy() to change our “Latitude” and “Longitude” columns to geometries:\n\ncities = gpd.GeoDataFrame(cities,\n                          crs=\"EPSG:4326\",  # I'll talk about this later\n                          geometry=gpd.points_from_xy(cities[\"Longitude\"], cities[\"Latitude\"]))\ncities\n\n\n\n\n\n  \n    \n      \n      City\n      Population\n      Latitude\n      Longitude\n      geometry\n    \n  \n  \n    \n      0\n      Vancouver\n      2264823\n      49.260833\n      -123.113889\n      POINT (-123.11389 49.26083)\n    \n    \n      1\n      Victoria\n      335696\n      48.428333\n      -123.364722\n      POINT (-123.36472 48.42833)\n    \n    \n      2\n      Kelowna\n      151957\n      49.888056\n      -119.495556\n      POINT (-119.49556 49.88806)\n    \n  \n\n\n\n\nLet’s plot those points on our map:\n\nax = bc.plot(edgecolor=\"0.2\")\ncities.plot(ax=ax, markersize=180, edgecolor=\"0.2\")\nplt.title(\"Big cities in B.C.\");\n\n\n\n\n\n\n2.4. Loading from Open Street Map\nSo we can read vector data from a file and we can create our own, but let’s see what real power feels like!\n\nOften it will be helpful to obtain data from an online source using an API. The most relevant “online source” here is OpenStreetMap (OSM), which is like the Wikipedia of geospatial data (think world map, road networks, bike networks, buidling heights, sandy coastlines, you name it). There are plenty of Python APIs for getting data from OSM but by far the best I’ve come across is osmnx:\nconda install -c conda-forge osmnx\nosmnx provides an easy-to-use API to query OSM data. I usually import it with the alias ox. Let’s get a polygon of Vancouver now using the function ox.geocode_to_gdf():\n\nBy default osmnx caches responses locally in a folder cache so that you can quickly access data again without needing to call the API. You can turn this behaviour off if you wish.\n\n\nimport osmnx as ox\n\nvancouver = ox.geocode_to_gdf(\"Vancouver, Canada\")\nvancouver.plot(edgecolor=\"0.2\")\nplt.title(\"Vancouver\");\n\n\n\n\nIt’s certainly Vancouver, but it looks a bit blocky. It might be a bit low resolution, or someone just decided this was the best way to encapsulate “Vancouver” on OSM. Either way, let’s use this polygon to “clip” a section of our higher-resolution provinces data which we downloaded earlier (and which is the official shapefile downloaded from statcan).\nThis is the first geometric wrangling operation we’ll see. I’ll show some more later, but think of “clipping” as passing a top layer of cookie dough (the map above), over a bottom layer cookiecutter (our high-resolution provinces data) to get a shape out:\n\nvan_bc = gpd.clip(bc, vancouver)\nvan_bc.plot(edgecolor=\"0.2\")\nplt.title(\"Vancouver\");\n\n\n\n\nThat looks better! Now we can clearly see Stanley Park at the top of the plot! Speaking of which, let’s get a polygon of Stanley Park:\n\nstanley_park = ox.geocode_to_gdf(\"Stanley Park, Vancouver\")\n\nAnd plot it on our map:\n\nax = van_bc.plot(edgecolor=\"0.2\")\nstanley_park.plot(ax=ax, edgecolor=\"0.2\", color=\"tomato\")\nplt.title(\"Stanley Park\");\n\n\n\n\nOkay let’s do one last cool thing! Let’s use osmnx to get the bicycle network within Stanley Park! We can get networks like road, rail, bike, etc., using the function ox.graph_from_place():\n\nbike_network = ox.graph_from_place(\"Stanley Park, Vancouver\",\n                                   network_type=\"bike\")\nbike_network\n\n<networkx.classes.multidigraph.MultiDiGraph at 0x13863b210>\n\n\nAs you can see, this returns a different object, a networkx MultiDiGraph which is a structure for holding network/graph-like objects such as road networks. We’re not interested in graph operations, we are just interested in geometries, so we’ll convert this to a GeoDataFrame using the function ox.graph_to_gdfs():\n\nbike_network = (ox.graph_to_gdfs(bike_network, nodes=False)\n                  .reset_index(drop=True)\n                  .loc[:, [\"name\", \"length\", \"bridge\", \"geometry\"]]\n               )\nbike_network\n\n\n\n\n\n  \n    \n      \n      name\n      length\n      bridge\n      geometry\n    \n  \n  \n    \n      0\n      Stanley Park Causeway\n      165.216\n      yes\n      LINESTRING (-123.13719 49.29767, -123.13718 49...\n    \n    \n      1\n      Stanley Park Causeway\n      62.901\n      NaN\n      LINESTRING (-123.13719 49.29767, -123.13741 49...\n    \n    \n      2\n      NaN\n      8.066\n      NaN\n      LINESTRING (-123.13211 49.29737, -123.13212 49...\n    \n    \n      3\n      Stanley Park Drive\n      80.558\n      NaN\n      LINESTRING (-123.13211 49.29737, -123.13204 49...\n    \n    \n      4\n      NaN\n      91.001\n      NaN\n      LINESTRING (-123.13211 49.29737, -123.13212 49...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      361\n      NaN\n      12.670\n      NaN\n      LINESTRING (-123.12854 49.29693, -123.12863 49...\n    \n    \n      362\n      NaN\n      44.325\n      NaN\n      LINESTRING (-123.12854 49.29693, -123.12869 49...\n    \n    \n      363\n      Stanley Park Drive\n      3.339\n      NaN\n      LINESTRING (-123.14577 49.29042, -123.14572 49...\n    \n    \n      364\n      Park Lane\n      29.437\n      NaN\n      LINESTRING (-123.14668 49.29078, -123.14658 49...\n    \n    \n      365\n      Stanley Park Drive\n      48.880\n      NaN\n      LINESTRING (-123.14668 49.29078, -123.14670 49...\n    \n  \n\n366 rows × 4 columns\n\n\n\nNow let’s plot this bike network on Stanley Park!\n\nax = stanley_park.plot(edgecolor=\"0.2\")\nbike_network.plot(ax=ax, edgecolor=\"0.2\", color=\"tomato\")\nplt.title(\"Stanley Park Cycling\");\n\n\n\n\nAmazing stuff!\n\n\n2.5. Basic wrangling\nSo we did some cool mapping and data manipulation above, but what’s it all for? Why would you want to do any of this? Well we can now start calculating and manipulating this spatial data to answer questions. Let’s answer some now.\n\nQuestion 1: What is the total length of bike lanes in Stanley Park?\nWell, our GeoDataFrame actually already has this information, it came for free from our original ox.graph_from_place() function and is in the “length” column:\n\ntotal_length = bike_network[\"length\"].sum()  # total length in m\nprint(f\"Total bike lane length: {total_length / 1000:.0f}km\")\n\nTotal bike lane length: 58km\n\n\nBut even if we didn’t have this column, we could still calculate lengths based on our line geometries and the .length attribute:\n\nbike_network[\"geometry\"]\n\n0      LINESTRING (-123.13719 49.29767, -123.13718 49...\n1      LINESTRING (-123.13719 49.29767, -123.13741 49...\n2      LINESTRING (-123.13211 49.29737, -123.13212 49...\n3      LINESTRING (-123.13211 49.29737, -123.13204 49...\n4      LINESTRING (-123.13211 49.29737, -123.13212 49...\n                             ...                        \n361    LINESTRING (-123.12854 49.29693, -123.12863 49...\n362    LINESTRING (-123.12854 49.29693, -123.12869 49...\n363    LINESTRING (-123.14577 49.29042, -123.14572 49...\n364    LINESTRING (-123.14668 49.29078, -123.14658 49...\n365    LINESTRING (-123.14668 49.29078, -123.14670 49...\nName: geometry, Length: 366, dtype: geometry\n\n\n\nbike_network[\"geometry\"].length\n\n/opt/miniconda3/envs/mds574/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n0      0.001653\n1      0.000751\n2      0.000073\n3      0.001029\n4      0.001151\n         ...   \n361    0.000133\n362    0.000477\n363    0.000044\n364    0.000393\n365    0.000628\nLength: 366, dtype: float64\n\n\nWhat’s that warning? More on that a bit later, but it’s telling us that our coordinate system has units of degrees - not linear units like meters which would be better for calculating distances. I’m going to convert my geometries to a coordinate systems based on linear units (meters). I’ll specify the projection “EPSG:3347” (Lambert projection) which is the one used by statcan:\n\nbike_network = bike_network.to_crs(\"EPSG:3347\")\nbike_network[\"geometry\"].length\n\n0      165.142206\n1       62.941716\n2        8.058879\n3       80.655282\n4       91.110800\n          ...    \n361     12.667118\n362     44.326065\n363      3.343832\n364     29.484229\n365     48.942512\nLength: 366, dtype: float64\n\n\n\ntotal_length = bike_network[\"geometry\"].length.sum()\nprint(f\"Total bike lane length: {total_length / 1000:.0f}km\")\n\nTotal bike lane length: 58km\n\n\nThe same as we got before! Nice!\n\n\nQuestion 2: What percentage of the area of Stanley Park is bike lanes?\nThis is a tougher one! First let’s calculate the “area” of our bike lanes. We need to make an assumption about the width of our lanes. This City of Vancouver planning document suggests the bike lanes around Stanley Park are about 3m so let’s go with that.\nI’m going to use the .buffer() method to turn the lines of my bike network into polygons with a specified width (3m in our cases). Because my bike_network data is in linear meters units now (remember, I changed the projection), I’m also going to convert our Stanley Park map to that projection (“EPSG:3347”):\n\nstanley_park = stanley_park.to_crs(\"EPSG:3347\")\n\nNow let’s “buffer” our bike lanes to be me 3m wide polygons. “Buffer” just mean to add some area around the object:\n\n\nwidth = 3  # desired with of bike lanes in meters\nbike_network[\"geometry\"] = bike_network.buffer(distance=width / 2)  # note that we provide distance as a radius (half the desired width)\nbike_network\n\n\n\n\n\n  \n    \n      \n      name\n      length\n      bridge\n      geometry\n    \n  \n  \n    \n      0\n      Stanley Park Causeway\n      165.216\n      yes\n      POLYGON ((4018562.106 2009363.283, 4018563.601...\n    \n    \n      1\n      Stanley Park Causeway\n      62.901\n      NaN\n      POLYGON ((4018554.879 2009389.952, 4018550.455...\n    \n    \n      2\n      NaN\n      8.066\n      NaN\n      POLYGON ((4018874.157 2009173.346, 4018874.220...\n    \n    \n      3\n      Stanley Park Drive\n      80.558\n      NaN\n      POLYGON ((4018877.663 2009164.604, 4018883.832...\n    \n    \n      4\n      NaN\n      91.001\n      NaN\n      POLYGON ((4018870.301 2009159.558, 4018867.565...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      361\n      NaN\n      12.670\n      NaN\n      POLYGON ((4019075.828 2009012.416, 4019075.828...\n    \n    \n      362\n      NaN\n      44.325\n      NaN\n      POLYGON ((4019065.791 2008999.105, 4019063.426...\n    \n    \n      363\n      Stanley Park Drive\n      3.339\n      NaN\n      POLYGON ((4017636.250 2008950.892, 4017636.334...\n    \n    \n      364\n      Park Lane\n      29.437\n      NaN\n      POLYGON ((4017601.662 2009019.800, 4017601.738...\n    \n    \n      365\n      Stanley Park Drive\n      48.880\n      NaN\n      POLYGON ((4017591.429 2009020.028, 4017591.421...\n    \n  \n\n366 rows × 4 columns\n\n\n\n\nax = stanley_park.plot(edgecolor=\"0.2\")\nbike_network.plot(ax=ax, edgecolor=\"tomato\")\nplt.title(\"Stanley Park Cycling\");\n\n\n\n\nNow we can calculate the area using the .area attribute. Note that geopandas is smart enough to know that you probably want to calculate these spatial attributes on the geometry column, so you don’t actually have to index that particular column (if you have multiple geometry columns, then you can choose which one is “active” and acted on by default using the .set_geometry() method):\n\nbike_network_area = bike_network.area.sum()\nprint(f\"Bike path area: {bike_network_area:.0f} m2\")\n\nBike path area: 177968 m2\n\n\nIf you think about it, that should be roughly similar to if we just multiplied our total_length from “Question 1” by 3 (the width of the bike paths):\n\ntotal_length * 3\n\n175429.60863417128\n\n\nWell that’s a nice sanity check! Now we just need the area of Stanley Park and we can calculate our ratio:\n\nstanley_park_area = stanley_park.area\nprint(f\"{bike_network_area / stanley_park_area[0] * 100:.2f}% of Stanley Park is bike paths.\")\n\n4.45% of Stanley Park is bike paths.\n\n\n\n\nQuestion 3: What FSA in Vancouver has the most bike lanes (by length)?\nAn FSA is a “forward sortation area”, basically a group of postcodes that all start with the same first 3 letters. So to answer this question, we need two things: 1. FSA polygons (available on statcan here) 2. The bike network for all of Vancouver\nI have already downloaded the above shapefile of FSAs for all of Canada. We’ll load it in and then clip it using our Vancouver polygon:\n\nfsa = gpd.read_file(\"data-spatial/fsa\")\nfsa = fsa.to_crs(\"EPSG:4326\")\nvan_fsa = gpd.clip(fsa, vancouver)\n\nNow let’s get the Vancouver bike network using osmnx:\n\nvan_bike_network = ox.graph_from_place(\"Vancouver\", network_type=\"bike\")\nvan_bike_network = (ox.graph_to_gdfs(van_bike_network, nodes=False)\n                      .reset_index(drop=True)\n                      .loc[:, [\"name\", \"length\", \"bridge\", \"geometry\"]]\n                   )\n\nLet’s take a look at our data so far:\n\nax = van_fsa.plot(edgecolor=\"0.2\")\nvan_bike_network.plot(ax=ax, edgecolor=\"tomato\", linewidth=0.5)\nplt.title(\"Vancouver Cycling\");\n\n\n\n\nOkay so how do we work out the total length of bike lanes in each FSA? We need to do a spatial join, which joins two geometries based on their locations. In the plot below, we’ll join the column of the dark dot to the column(s) of the grey polygon because the dark dot is contained within the spatial region of the polygon.\n\n\nSource: GISGeography.\n\nWe can do a spatial join with gpd.sjoin() (it’s just like joining in base pandas). There are different options for the argument op which allow you how to control the join. Below I’ll specify “contain”, meaning I only want to join when a bike lane is fully contained within an FSA (you can read more about op in the documentation);\n\njoined_data = gpd.sjoin(van_fsa, van_bike_network, how=\"inner\", op=\"contains\")\njoined_data\n\n\n\n\n\n  \n    \n      \n      CFSAUID\n      PRUID\n      PRNAME\n      geometry\n      index_right\n      name\n      length\n      bridge\n    \n  \n  \n    \n      1386\n      V5Z\n      59\n      British Columbia / Colombie-Britannique\n      POLYGON ((-123.11483 49.26939, -123.11482 49.2...\n      14683\n      West 49th Avenue\n      113.479\n      NaN\n    \n    \n      1386\n      V5Z\n      59\n      British Columbia / Colombie-Britannique\n      POLYGON ((-123.11483 49.26939, -123.11482 49.2...\n      11256\n      West 49th Avenue\n      113.479\n      NaN\n    \n    \n      1386\n      V5Z\n      59\n      British Columbia / Colombie-Britannique\n      POLYGON ((-123.11483 49.26939, -123.11482 49.2...\n      11211\n      West 49th Avenue\n      115.184\n      NaN\n    \n    \n      1386\n      V5Z\n      59\n      British Columbia / Colombie-Britannique\n      POLYGON ((-123.11483 49.26939, -123.11482 49.2...\n      11257\n      West 49th Avenue\n      115.184\n      NaN\n    \n    \n      1386\n      V5Z\n      59\n      British Columbia / Colombie-Britannique\n      POLYGON ((-123.11483 49.26939, -123.11482 49.2...\n      11214\n      Fremlin Street\n      51.729\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1615\n      V6R\n      59\n      British Columbia / Colombie-Britannique\n      POLYGON ((-123.21376 49.27669, -123.21346 49.2...\n      48960\n      NaN\n      221.171\n      NaN\n    \n    \n      1615\n      V6R\n      59\n      British Columbia / Colombie-Britannique\n      POLYGON ((-123.21376 49.27669, -123.21346 49.2...\n      48957\n      NaN\n      105.284\n      NaN\n    \n    \n      1615\n      V6R\n      59\n      British Columbia / Colombie-Britannique\n      POLYGON ((-123.21376 49.27669, -123.21346 49.2...\n      54169\n      NaN\n      105.284\n      NaN\n    \n    \n      1615\n      V6R\n      59\n      British Columbia / Colombie-Britannique\n      POLYGON ((-123.21376 49.27669, -123.21346 49.2...\n      40455\n      NaN\n      292.839\n      NaN\n    \n    \n      1615\n      V6R\n      59\n      British Columbia / Colombie-Britannique\n      POLYGON ((-123.21376 49.27669, -123.21346 49.2...\n      1231\n      NaN\n      292.839\n      NaN\n    \n  \n\n57907 rows × 8 columns\n\n\n\nNow we just need to .groupby():\n\n(joined_data[[\"CFSAUID\", \"length\"]].groupby(by=\"CFSAUID\")\n                                   .sum()\n                                   .sort_values(\"length\", ascending=False)\n                                   .head()\n)\n\n\n\n\n\n  \n    \n      \n      length\n    \n    \n      CFSAUID\n      \n    \n  \n  \n    \n      V6P\n      348095.253\n    \n    \n      V5R\n      281367.666\n    \n    \n      V5X\n      264815.800\n    \n    \n      V5S\n      259047.140\n    \n    \n      V5K\n      245773.116\n    \n  \n\n\n\n\nWe see that “V6P” has the largest length of bike lanes, what FSA is that?\n\nax = van_fsa.plot(edgecolor=\"0.2\")\nvan_fsa.query(\"CFSAUID == 'V6P'\").plot(ax=ax, edgecolor=\"0.2\", color=\"tomato\")\nplt.title(\"FSA with most bike lane length\");\n\n\n\n\nLooks good to me!"
  },
  {
    "objectID": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#working-with-raster-data",
    "href": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#working-with-raster-data",
    "title": "GiveDirectly",
    "section": "3. Working with raster data",
    "text": "3. Working with raster data\nUnlike vector data (geometric objects: like points, lines, polygons), raster data is a matrix of values of “pixels” (also called “cells”). Each cell represents a small area and contains a value representing some information:\n\n\nSource: National Ecological Observatory Network.\n\nRaster data is like digital image data you look at on your computer, except that now, each pixel represents a spatial region. The “resolution” of a raster is the area that each pixel represents. A 1 meter resolution raster means that each pixel represents a 1 m x 1 m area on the ground. However, when we say “high resolution” we often mean, a low value of resolution for each pixel, i.e., 1 meter resolution is higher than 8 meter resolution as exmplified by the image below:\n\n\nSource: National Ecological Observatory Network.\n\nLike vector data, there are different file formats for storing raster data. The most common is GeoTIFF (.tif), which is essentially an image file with georeferencing information embedded within it. Raster data is used for a variety of problems, common examples include satellite imagery and digital elevation models. Those things are gettings a bit outside the scope of this course but let’s briefly look at some raster data below. The core packge for working with raster data in Python is rasterio.\n\nimport rasterio\n\nI have a satellite image raster file of part of UBC in my data folder which I downloaded from the Abacus Data Network. Let’s load it in with rasterio:\n\ndataset = rasterio.open(\"tif/ubc-481E_5456N/481E_5456N.tif\")\n\nWe can start to investigate things like the width and height (in pixels/cells) of the raster:\n\nprint(f\"Width: {dataset.width} pixels\")\nprint(f\"Height: {dataset.height} pixels\")\n\nWidth: 10000 pixels\nHeight: 10000 pixels\n\n\nRaster data often have “bands” representing different information (for example, a colour image usually has red, green, and blue bands). This particular satellite image has 4 bands (in order: red, blue, green, infrared):\n\ndataset.count\n\n4\n\n\nWe could import the first band as a numpy array using:\n\nband1 = dataset.read(1)\nband1\n\narray([[ 70,  67,  52, ...,  88,  86,  85],\n       [ 64,  60,  53, ...,  89,  88,  92],\n       [ 67,  63,  62, ...,  93, 104, 107],\n       ...,\n       [ 58,  56,  54, ...,  52,  28,  20],\n       [ 55,  56,  55, ...,  55,  30,  25],\n       [ 54,  57,  59, ...,  30,  19,  29]], dtype=uint8)\n\n\nBut before importing anymore data, it’s helpful to just see the image. First I’m going to “downsample” my raster (reduce the resolution by increasing the cell size) to reduce the size of the data and speed things up a bit:\n\nrescale_factor = 0.5\nscaled_data = dataset.read(out_shape=(dataset.count, int(dataset.height * rescale_factor),\n                                      int(dataset.width * rescale_factor))\n                          )\nprint(f\"Scaled shape: {scaled_data.shape}\")\n\nScaled shape: (4, 5000, 5000)\n\n\nAs our data is just numpy array(s), we can plot it with the matplotlib function plt.imshow():\n\nfig, ax = plt.subplots(1, 4, figsize=(12, 4))\ncmaps = [\"Reds\", \"Blues\", \"Greens\", \"Purples\"]\nbands = [\"Red Band\", \"Blue Band\", \"Green Band\", \"Infrared Band\"]\nfor band in [0, 1, 2, 3]:\n    ax[band].imshow(scaled_data[band, :, :], cmap=cmaps[band])\n    ax[band].set_title(bands[band])\n    ax[band].axis(\"off\")\nplt.tight_layout();\n\n\n\n\nOf course, it looks more realistic using all channels:\n\nplt.figure(figsize=(5, 5))\nplt.imshow(np.moveaxis(scaled_data, 0, -1)[:, :, :3])\nplt.axis(\"off\")\nplt.title(\"All bands\");\n\n\n\n\nrasterio has lots of advanced fucntionality for manipulating and plotting raster data if you find the need to. Check out the documentation."
  },
  {
    "objectID": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#coordinate-reference-systems",
    "href": "python-for-geospatial-analysis/chapters/chapter1_intro-to-spatial.html#coordinate-reference-systems",
    "title": "GiveDirectly",
    "section": "4. Coordinate reference systems",
    "text": "4. Coordinate reference systems\n\nGenerally speaking a coordinate reference system is how we project the 3D surface of the Earth onto a 2D shape for easy viewing\n\nThere are many different projections and they are typically identified by an EPSG code. No projection is perfect (it’s impossible to perfectly flatten a 3d sphere) and each comprises on minimzing the distortion of shapes, distances, and areas of the Earth. At a basic level, all you need to know is that some projections are: - in angular units (degrees of latitude and longitude) and are good for locating places on Earth, for making global maps, and minimizing shape distortion. The most common is WGS 84 (“EPSG:4326”) - in linear units (e.g., meters) and are good for measuring distances. Most common is UTM which splits the Earth into different linear regions, the code for the region encompassing British Columbia is “EPSG:32610”\nBut many countries/regions use other specific projections which minimize distortion of that specific area. For example Statistics Canada uses the Lambert projection for Canada (“EPSG:3347”). Much of the time, you will know or be told which projection to use based on your specific data/project.\nLet’s take a quick look at some different projections for Canada now:\n\n# Load data from geopandas\ncanada = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')).query(\"name == 'Canada'\")\n\n# Plot\nfig, axs = plt.subplots(1, 3, figsize=(15, 12))\ncrs_list = [(\"WGS 84\", \"EPSG:4326\"), (\"Lambert\", \"EPSG:3347\"), (\"UTM 10N\", \"EPSG:32610\")]\nfor n, (name, epsg) in enumerate(crs_list):\n    canada.to_crs(epsg).plot(ax=axs[n])\n    axs[n].set_title(name)\nplt.tight_layout();"
  },
  {
    "objectID": "python-for-geospatial-analysis/chapters/chapter2_spatial-viz-and-modelling.html#chapter-outline",
    "href": "python-for-geospatial-analysis/chapters/chapter2_spatial-viz-and-modelling.html#chapter-outline",
    "title": "GiveDirectly",
    "section": "Chapter Outline",
    "text": "Chapter Outline\n\n\n\n\nChapter Learning Objectives\n\n\nImports\n\n\n1. Spatial visualization\n\n\n2. Spatial modelling"
  },
  {
    "objectID": "python-for-geospatial-analysis/chapters/chapter2_spatial-viz-and-modelling.html#chapter-learning-objectives",
    "href": "python-for-geospatial-analysis/chapters/chapter2_spatial-viz-and-modelling.html#chapter-learning-objectives",
    "title": "GiveDirectly",
    "section": "Chapter Learning Objectives",
    "text": "Chapter Learning Objectives\n\n\nMake informed choices about how to plot your spatial data, e.g., scattered, polygons, 3D, etc..\nPlot spatial data using libraries such as geopandas, plotly, and keplergl.\nInterpolate unobserved spatial data using deterministic methods such as nearest-neighbour interpolation.\nInterpolate data from one set of polygons to a different set of polygons using areal interpolation."
  },
  {
    "objectID": "python-for-geospatial-analysis/chapters/chapter2_spatial-viz-and-modelling.html#imports",
    "href": "python-for-geospatial-analysis/chapters/chapter2_spatial-viz-and-modelling.html#imports",
    "title": "GiveDirectly",
    "section": "Imports",
    "text": "Imports\n\n\nimport warnings\nimport keplergl\nimport numpy as np\nimport osmnx as ox\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nfrom skgstat import Variogram\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import Point\nfrom pykrige.ok import OrdinaryKriging\nfrom scipy.interpolate import NearestNDInterpolator\nfrom tobler.area_weighted import area_interpolate\n# Custom functions\nfrom scripts.utils import pixel2poly\n# Plotting defaults\nplt.style.use('ggplot')\npx.defaults.height = 400; px.defaults.width = 620\nplt.rcParams.update({'font.size': 16, 'axes.labelweight': 'bold', 'figure.figsize': (6, 6), 'axes.grid': False})"
  },
  {
    "objectID": "python-for-geospatial-analysis/chapters/chapter2_spatial-viz-and-modelling.html#spatial-visualization",
    "href": "python-for-geospatial-analysis/chapters/chapter2_spatial-viz-and-modelling.html#spatial-visualization",
    "title": "GiveDirectly",
    "section": "1. Spatial visualization",
    "text": "1. Spatial visualization\n\n\n1.1. Geopandas\nWe saw last chapter how to easily plot geospatial data using the geopandas method .plot(). This workflow is useful for making quick plots, exploring your data, and easily layering geometries. Let’s import some data of UBC buildings using osmnx (our Python API for accessing OpenStreetMap data) and make a quick plot:\n\nubc = (ox.geometries_from_place(\"University of British Columbia, Canada\", tags={'building':True})\n         .loc[:, [\"geometry\"]]                 # just keep the geometry column for now\n         .query(\"geometry.type == 'Polygon'\")  # only what polygons (buidling footprints)\n         .assign(Label=\"Building Footprints\")  # assign a label for later use\n         .reset_index(drop=True)               # reset to 0 integer indexing\n      )\nubc.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      Label\n    \n  \n  \n    \n      0\n      POLYGON ((-123.25526 49.26695, -123.25506 49.2...\n      Building Footprints\n    \n    \n      1\n      POLYGON ((-123.25328 49.26803, -123.25335 49.2...\n      Building Footprints\n    \n    \n      2\n      POLYGON ((-123.25531 49.26859, -123.25493 49.2...\n      Building Footprints\n    \n    \n      3\n      POLYGON ((-123.25403 49.26846, -123.25408 49.2...\n      Building Footprints\n    \n    \n      4\n      POLYGON ((-123.25455 49.26906, -123.25398 49.2...\n      Building Footprints\n    \n  \n\n\n\n\nRecall that we can make a plot using the .plot() method on a GeoDataFrame:\n\nax = ubc.plot(figsize=(8, 8), column=\"Label\", legend=True,\n              edgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\nplt.title(\"UBC\");\n\n\n\n\nSay I know the “point” location of my office but I want to locate the building footprint (a “polygon”). That’s easily done with geopandas!\nFirst, I’ll use shapely (the Python geometry library geopandas is built on) to make my office point, but you could also use the geopandas function gpd.points_from_xy() like we did last chapter:\n\npoint_office = Point(-123.2522145, 49.2629555)\npoint_office\n\n\n\n\nNow, I can use the .contains() method to find out which building footprint my office resides in:\n\nubc[ubc.contains(point_office)]\n\n\n\n\n\n  \n    \n      \n      geometry\n      Label\n    \n  \n  \n    \n      48\n      POLYGON ((-123.25217 49.26345, -123.25196 49.2...\n      Building Footprints\n    \n  \n\n\n\n\nLooks like it’s index 48! I’m going to change the label of that one to “Tom’s Office”:\n\nubc.loc[48, \"Label\"] = \"Tom's Office\"\n\nNow let’s make a plot!\n\nax = ubc.plot(figsize=(8, 8), column=\"Label\", legend=True,\n              edgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\nplt.title(\"UBC\");\n\n\n\n\nWe can add more detail to this map by including a background map. For this, we need to install the contextily package. Note that most web providers use the Web Mercator projection, “EPSG:3857” (interesting article on that here) so I’ll convert to that before plotting:\n\nimport contextily as ctx\n\nax = (ubc.to_crs(\"EPSG:3857\")\n         .plot(figsize=(10, 8), column=\"Label\", legend=True,\n               edgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\n     )\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)  # I'm using OSM as the source. See all provides with ctx.providers\nplt.axis(\"off\")\nplt.title(\"UBC\");\n\n\n\n\n\n\n1.2. Plotly\nThe above map is nice, but it sure would be helpful in some cases to have interactive functionality for our map don’t you think? Like the ability to zoom and pan? Well there are several packages out there that can help us with that, but plotly is one of the best for this kind of mapping. plotly supports plotting of maps backed by MapBox (a mapping and location data cloud platform).\nHere’s an example using the plotly.express function px.choropleth_mapbox():\n\nfig = px.choropleth_mapbox(ubc, geojson=ubc.geometry, locations=ubc.index, color=\"Label\",\n                           center={\"lat\": 49.261, \"lon\": -123.246}, zoom=12.5,\n                           mapbox_style=\"open-street-map\")\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=10))\n\n\n                                                \n\n\nYou can pan and zoom above as you desire! How about we add a terrain map instead:\n\nfig = px.choropleth_mapbox(ubc, geojson=ubc.geometry, locations=ubc.index, color=\"Label\",\n                           center={\"lat\": 49.261, \"lon\": -123.246}, zoom=12.5,\n                           mapbox_style=\"stamen-terrain\")\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=10))\n\n\n                                                \n\n\nWe are just colouring our geometries based on a label at the moment, but we could of course do some cooler things. Let’s colour by building area:\n\n# Calculate area\nubc[\"Area\"] = ubc.to_crs(epsg=3347).area  # note I'm projecting to EPSG:3347 (the projected system in meters that Statistics Canada useshttps://epsg.io/3347)\n\n# Make plot\nfig = px.choropleth_mapbox(ubc, geojson=ubc.geometry, locations=ubc.index, color=\"Area\",\n                           center={\"lat\": 49.261, \"lon\": -123.246}, zoom=12.5,\n                           mapbox_style=\"carto-positron\")\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=10))\n\n\n                                                \n\n\nCheck out the plotly documentation for more - there are many plotting options and examples to learn from! Other popular map plotting options include altair (doesn’t support interactivity yet), folium, and bokeh.\n\n\n1.3. Kepler.gl\nThe above mapping was pretty cool, but are you ready for more power?\n\nTime to introduce kepler.gl! keplergl is a web-based tool for visualing spatial data. Luckily, it has a nice Python API and Jupyter extension for us to use (see the install instructions). The basic way keplergl works is: 1. We create an instance of a map with keplergl.KeplerGl() 2. We add as much data to the map as we like with the .add_data() method 3. We customize and configure the map in any way we like using the GUI (graphical user interface)\n\nubc_map = keplergl.KeplerGl(height=500)\nubc_map.add_data(data=ubc.copy(), name=\"Building heights\")\nubc_map\n\nUser Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n\n\n\n\n\n\n%%html\n<iframe src=\"../_images/ubc-2d.html\" width=\"80%\" height=\"500\"></iframe>\n\n\n\n\nI’ll do you one better than that! Let’s add a 3D element to our plot! I’m going to load in some data of UBC building heights I downloaded from the City of Vancouver Open Data Portal:\n\nubc_bldg_heights = gpd.read_file(\"data-spatial/ubc-building-footprints-2009\")\nubc_bldg_heights.head()\n\n\n\n\n\n  \n    \n      \n      id\n      orient8\n      bldgid\n      topelev_m\n      med_slope\n      baseelev_m\n      hgt_agl\n      rooftype\n      area_m2\n      avght_m\n      minht_m\n      maxht_m\n      base_m\n      len\n      wid\n      geometry\n    \n  \n  \n    \n      0\n      158502.0\n      26.7542\n      112433.0\n      97.55\n      4.0\n      80.01\n      17.54\n      Flat\n      2085.07\n      13.70\n      0.00\n      21.76\n      81.40\n      110.61\n      41.11\n      POLYGON ((-123.25550 49.26127, -123.25544 49.2...\n    \n    \n      1\n      158503.0\n      26.7542\n      112433.0\n      100.58\n      13.0\n      81.50\n      19.08\n      Flat\n      2085.07\n      13.70\n      0.00\n      21.76\n      81.40\n      110.61\n      41.11\n      POLYGON ((-123.25561 49.26111, -123.25578 49.2...\n    \n    \n      2\n      158506.0\n      27.5790\n      112852.0\n      122.32\n      5.0\n      93.04\n      29.28\n      Flat\n      4116.37\n      18.02\n      0.03\n      30.55\n      93.73\n      67.22\n      66.37\n      POLYGON ((-123.24859 49.26136, -123.24850 49.2...\n    \n    \n      3\n      158508.0\n      28.2727\n      120767.0\n      109.85\n      1.0\n      90.51\n      19.33\n      Flat\n      5667.17\n      17.95\n      0.17\n      31.16\n      89.19\n      155.53\n      59.50\n      POLYGON ((-123.25340 49.26482, -123.25351 49.2...\n    \n    \n      4\n      158509.0\n      28.5496\n      122507.0\n      112.69\n      27.0\n      86.63\n      26.07\n      Pitched\n      1349.35\n      18.26\n      0.00\n      27.34\n      87.79\n      64.92\n      26.47\n      POLYGON ((-123.25520 49.26655, -123.25524 49.2...\n    \n  \n\n\n\n\nI’m going to combine this with our ubc data and do a bit of clean-up and wrangling. You’ll do this workflow in your lab too so I won’t spend too much time here:\n\nubc_bldg_heights = (gpd.sjoin(ubc, ubc_bldg_heights[[\"hgt_agl\", \"geometry\"]], how=\"inner\")\n                       .drop(columns=\"index_right\")\n                       .rename(columns={\"hgt_agl\": \"Height\"})\n                       .reset_index()\n                       .dissolve(by=\"index\", aggfunc=\"mean\")  # dissolve is like \"groupby\" in pandas. We use it because it retains geometry information\n                   )\n\nNow I’ll make a new map and configure it to be in 3D using the GUI!\n\nubc_height_map = keplergl.KeplerGl(height=500)\nubc_height_map.add_data(data=ubc_bldg_heights.copy(), name=\"Building heights\")\nubc_height_map.save_to_html(file_name='first_map.html')\n\nUser Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n\n\n\n\n\n\n%%html\n<iframe src=\"../_images/ubc-3d.html\" width=\"80%\" height=\"500\"></iframe>\n\n\n\n\nYou can save your configuration and customization for re-use later (see the docs here)"
  },
  {
    "objectID": "python-for-geospatial-analysis/chapters/chapter2_spatial-viz-and-modelling.html#spatial-modelling",
    "href": "python-for-geospatial-analysis/chapters/chapter2_spatial-viz-and-modelling.html#spatial-modelling",
    "title": "GiveDirectly",
    "section": "2. Spatial modelling",
    "text": "2. Spatial modelling\n\nThere’s typically two main ways we might want to “model” spatial data: 1. Spatial interpolation: use a set of observations in space to estimate the value of a spatial field 2. Areal interpolation: project data from one set of polygons to another set of polygons\nBoth are based on the fundamental premise: “everything is related to everything else, but near things are more related than distant things.” (Tobler’s first law of geography). To demonstrate some of these methods, we’ll look at the annual average air pollution (PM 2.5) recorded at stations across BC during 2020 (which I downloaded from DataBC):\n\npm25 = pd.read_csv(\"data/bc-pm25.csv\")\npm25.head()\n\n\n\n\n\n  \n    \n      \n      Station Name\n      Lat\n      Lon\n      EMS ID\n      PM_25\n    \n  \n  \n    \n      0\n      Abbotsford A Columbia Street\n      49.021500\n      -122.326600\n      E289309\n      4.7\n    \n    \n      1\n      Abbotsford Central\n      49.042800\n      -122.309700\n      E238212\n      4.6\n    \n    \n      2\n      Agassiz Municipal Hall\n      49.238032\n      -121.762334\n      E293810\n      6.7\n    \n    \n      3\n      Burnaby South\n      49.215300\n      -122.985600\n      E207418\n      3.8\n    \n    \n      4\n      Burns Lake Fire Centre\n      54.230700\n      -125.764300\n      E225267\n      5.4\n    \n  \n\n\n\n\n\nfig = px.scatter_mapbox(pm25, lat=\"Lat\", lon=\"Lon\", color=\"PM_25\", size=\"PM_25\",\n                        color_continuous_scale=\"RdYlGn_r\",\n                        center={\"lat\": 52.261, \"lon\": -123.246}, zoom=3.5,\n                        mapbox_style=\"carto-positron\", hover_name=\"Station Name\")\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=10))\nfig.show()\n\n\n                                                \n\n\nThe goal is to interpolate these point measurements to estimate the air pollution for all of BC.\n\n2.1. Deterministic spatial interpolation\nCreate surfaces directly from measured points using a mathematical function. Common techniques (all available in the scipy module interpolate) are: - Inverse distance weighted interpolation - Nearest neighbour interpolation - Polynomial interpolation - Radial basis function (RBF) interpolation\nLet’s try nearest neighbour interpolation now using scipy.interpolate.NearestNDInterpolator. As angular coordinates (lat/lon) are not good for measuring distances, I’m going to first convert my data to the linear, meter-based Lambert projection recommend by Statistics Canada and extract the x and y locations as columns in my GeoDataFrame (“Easting” and “Northing” respectively):\n\ngpm25 = (gpd.GeoDataFrame(pm25, crs=\"EPSG:4326\", geometry=gpd.points_from_xy(pm25[\"Lon\"], pm25[\"Lat\"]))\n            .to_crs(\"EPSG:3347\")\n        )\ngpm25[\"Easting\"], gpm25[\"Northing\"] = gpm25.geometry.x, gpm25.geometry.y\ngpm25.head()\n\n\n\n\n\n  \n    \n      \n      Station Name\n      Lat\n      Lon\n      EMS ID\n      PM_25\n      geometry\n      Easting\n      Northing\n    \n  \n  \n    \n      0\n      Abbotsford A Columbia Street\n      49.021500\n      -122.326600\n      E289309\n      4.7\n      POINT (4056511.036 1954658.126)\n      4.056511e+06\n      1.954658e+06\n    \n    \n      1\n      Abbotsford Central\n      49.042800\n      -122.309700\n      E238212\n      4.6\n      POINT (4058698.814 1956191.109)\n      4.058699e+06\n      1.956191e+06\n    \n    \n      2\n      Agassiz Municipal Hall\n      49.238032\n      -121.762334\n      E293810\n      6.7\n      POINT (4104120.445 1957264.772)\n      4.104120e+06\n      1.957265e+06\n    \n    \n      3\n      Burnaby South\n      49.215300\n      -122.985600\n      E207418\n      3.8\n      POINT (4023977.036 1996103.398)\n      4.023977e+06\n      1.996103e+06\n    \n    \n      4\n      Burns Lake Fire Centre\n      54.230700\n      -125.764300\n      E225267\n      5.4\n      POINT (4128403.079 2571148.393)\n      4.128403e+06\n      2.571148e+06\n    \n  \n\n\n\n\nNow let’s create a grid of values (a raster) to interpolate over. I’m just going to make a square grid of fixed resolution that spans the bounds of my observed data points (we’ll plot this shortly so you can see what it looks like):\n\nresolution = 25_000  # cell size in meters\ngridx = np.arange(gpm25.bounds.minx.min(), gpm25.bounds.maxx.max(), resolution)\ngridy = np.arange(gpm25.bounds.miny.min(), gpm25.bounds.maxy.max(), resolution)\n\nSo now let’s interpolate using a nearest neighbour method (the code below is straight from the scipy docs):\n\nmodel = NearestNDInterpolator(x = list(zip(gpm25[\"Easting\"], gpm25[\"Northing\"])),\n                              y = gpm25[\"PM_25\"])\nz = model(*np.meshgrid(gridx, gridy))\nplt.imshow(z);\n\n\n\n\nOkay so it looks like we successfully interpolated our made-up grid, but let’s re-project it back to our original map. To do this I need to convert each cell in my raster to a small polygon using a simple function I wrote called pixel2poly() which we imported at the beginning of the notebook:\n\npolygons, values = pixel2poly(gridx, gridy, z, resolution)\n\nNow we can convert that to a GeoDataFrame and plot using plotly:\n\npm25_model = (gpd.GeoDataFrame({\"PM_25_modelled\": values}, geometry=polygons, crs=\"EPSG:3347\")\n                 .to_crs(\"EPSG:4326\")\n             )\n\nfig = px.choropleth_mapbox(pm25_model, geojson=pm25_model.geometry, locations=pm25_model.index,\n                           color=\"PM_25_modelled\", color_continuous_scale=\"RdYlGn_r\", opacity=0.5,\n                           center={\"lat\": 52.261, \"lon\": -123.246}, zoom=3.5,\n                           mapbox_style=\"carto-positron\")\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=10))\nfig.update_traces(marker_line_width=0)\n\n\n                                                \n\n\nVery neat! Also, notice how the grid distorts a bit once we project it back onto an angular (degrees-based) projection from our linear (meter-based) projection.\n\n\n2.2. Probabilistic (geostatistical) spatial interpolation\nGeostatistical interpolation (called “Kriging”) differs to deterministic interpolation in that we interpolate using statistical models that include estimates of spatial autocorrelation. There’s a lot to read about kriging, I just want you to be aware of the concept in case you need to do something like this in the future. Usually I’d do this kind of interpolation in GIS software like ArcGIS or QGIS, but it’s possible to do in Python too.\nThe basic idea is that if we have a set of observations \\(Z(s)\\) at locations \\(s\\), we estimate the value of an unobserved location (\\(s_0\\)) as a weighted sum: \\[\\hat{Z}(s_0)=\\sum_{i=0}^{N}\\lambda_iZ(s_i)\\]\nWhere \\(N\\) is the size of \\(s\\) (number of observed samples) and \\(\\lambda\\) is an array of weights. The key is deciding which weights \\(\\lambda\\) to use. Kriging uses the spatial autocorrelation in the data to determine the weights. Spatial autocorrelation is calculated by looking at the squared difference (the variance) between points at similar distances apart, let’s see what that means:\n\nwarnings.filterwarnings(\"ignore\")  # Silence some warnings\nvario = Variogram(coordinates=gpm25[[\"Easting\", \"Northing\"]],\n                  values=gpm25[\"PM_25\"],\n                  n_lags=20)\nvario.distance_difference_plot();\n\n\n\n\nThe idea is to then fit a model to this data that describes how variance (spatial autocorrelation) changes with distance (“lag”) between locations. We look at the average variance in bins of the above distances/pairs and fit a line through them. This model is called a “variogram” and it’s analogous to the autocorrelation function for time series. It defines the variance (autocorrelation structure) as a function of distance:\n\nvario.plot(hist=False);\n\n\n\n\nThe above plot basically shows that: - at small distances (points are close together), variance is reduced because points are correlated - but at a distance around 400,000 m the variance flattens out indicating points are two far away to have any impactful spatial autocorrelation. This location is called the “range” while the variance at the “range” is called the “sill” (like a ceiling). We can extract the exact range:\n\nvario.describe()[\"effective_range\"]\n\n363018.90908542706\n\n\nBy the way, we call it the semi-variance because there is a factor of 0.5 in the equation to account for the fact that variance is calculated twice for each pair of points (read more here).\nRemember, our variogram defines the spatial autocorrelation of the data (i.e., how the locations in our region affect one another). Once we have a variogram model, we can use it to estimate the weights in our kriging model. I won’t go into detail on how this is done, but there is a neat walkthrough in the scikit-gstat docs here.\nAnyway, I’ll briefly use the pykrige library to do some kriging so you can get an idea of what it looks like:\n\nkrig = OrdinaryKriging(x=gpm25[\"Easting\"], y=gpm25[\"Northing\"], z=gpm25[\"PM_25\"], variogram_model=\"spherical\")\nz, ss = krig.execute(\"grid\", gridx, gridy)\nplt.imshow(z);\n\n\n\n\nNow let’s convert our raster back to polygons so we can map it. I’m also going to load in a polygon of BC using osmnx to clip my data so it fits nicely on my map this time:\n\npolygons, values = pixel2poly(gridx, gridy, z, resolution)\npm25_model = (gpd.GeoDataFrame({\"PM_25_modelled\": values}, geometry=polygons, crs=\"EPSG:3347\")\n                 .to_crs(\"EPSG:4326\")\n                 )\nbc = ox.geocode_to_gdf(\"British Columbia, Canada\")\npm25_model = gpd.clip(pm25_model, bc)\n\n\nfig = px.choropleth_mapbox(pm25_model, geojson=pm25_model.geometry, locations=pm25_model.index,\n                           color=\"PM_25_modelled\", color_continuous_scale=\"RdYlGn_r\",\n                           center={\"lat\": 52.261, \"lon\": -123.246}, zoom=3.5,\n                           mapbox_style=\"carto-positron\")\nfig.update_layout(margin=dict(l=0, r=0, t=30, b=10))\nfig.update_traces(marker_line_width=0)\n\n\n                                                \n\n\nI used an “ordinary kriging” interpolation above which is the simplest implementation of kriging. The are many other forms of kriging too that can account for underlying trends in the data (“universal kriging”), or even use a regression or classification model to make use of additional explanatory variables. pykrige supports most variations. In particular for the latter, pykrige can accept sklearn models which is useful!\n\n\n2.3. Areal interpolation\nAreal interpolation is concerned with mapping data from one polygonal representation to another. Imagine I want to map the air pollution polygons I just made to FSA polygons (recall FSA is “forward sortation area”, which are groups of postcodes). The most intuitive way to do this is to distribute values based on area proportions, hence “areal interpolation”.\nI’ll use the tobler library for this. First, load in the FSA polygons:\n\nvan_fsa = gpd.read_file(\"data-spatial/van-fsa\")\nax = van_fsa.plot(edgecolor=\"0.2\")\nplt.title(\"Vancouver FSA\");\n\n\n\n\nNow I’m just going to made a higher resolution interpolation using kriging so we can see some of the details on an FSA scale:\n\nresolution = 10_000  # cell size in meters\ngridx = np.arange(gpm25.bounds.minx.min(), gpm25.bounds.maxx.max(), resolution)\ngridy = np.arange(gpm25.bounds.miny.min(), gpm25.bounds.maxy.max(), resolution)\nkrig = OrdinaryKriging(x=gpm25[\"Easting\"], y=gpm25[\"Northing\"], z=gpm25[\"PM_25\"], variogram_model=\"spherical\")\nz, ss = krig.execute(\"grid\", gridx, gridy)\npolygons, values = pixel2poly(gridx, gridy, z, resolution)\npm25_model = (gpd.GeoDataFrame({\"PM_25_modelled\": values}, geometry=polygons, crs=\"EPSG:3347\")\n                 .to_crs(\"EPSG:4326\")\n                 )\n\nNow we can easily do the areal interpolation using the function area_interpolate():\n\nareal_interp = area_interpolate(pm25_model.to_crs(\"EPSG:3347\"),\n                                van_fsa.to_crs(\"EPSG:3347\"),\n                                intensive_variables=[\"PM_25_modelled\"]).to_crs(\"EPSG:4326\")\nareal_interp.plot(column=\"PM_25_modelled\", figsize=(8, 8),\n                  edgecolor=\"0.2\", cmap=\"RdBu\", legend=True)\nplt.title(\"FSA Air Pollution\");\n\n\n\n\nThere are other methods you can use for areal interpolation too, that include additional variables or use more advanced interpolation algorithms. The tobbler documentation describes some of these."
  },
  {
    "objectID": "data_quality/setup_datasource.html",
    "href": "data_quality/setup_datasource.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Use this notebook to configure a new pandas Datasource and add it to your project.\n\nimport os\nos.chdir('/home/thulasiram/personal/going_deep_and_wide/GiveDirectly/gx_tutorials/great_expectations')\n\n\nimport great_expectations as gx\nfrom great_expectations.cli.datasource import sanitize_yaml_and_save_datasource, check_if_datasource_name_exists\ncontext = gx.get_context()\n\n\n\nIf you are new to Great Expectations Datasources, you should check out our how-to documentation\nMy configuration is not so simple - are there more advanced options? Glad you asked! Datasources are versatile. Please see our How To Guides!\nGive your datasource a unique name:\n\ndatasource_name = \"data_quality_demo\"\n\n\n\nHere we are creating an example configuration. The configuration contains an InferredAssetFilesystemDataConnector which will add a Data Asset for each file in the base directory you provided. It also contains a RuntimeDataConnector which can accept filepaths. This is just an example, and you may customize this as you wish!\nAlso, if you would like to learn more about the DataConnectors used in this configuration, including other methods to organize assets, handle multi-file assets, name assets based on parts of a filename, please see our docs on InferredAssetDataConnectors and RuntimeDataConnectors.\n\nexample_yaml = f\"\"\"\nname: {datasource_name}\nclass_name: Datasource\nexecution_engine:\n  class_name: PandasExecutionEngine\ndata_connectors:\n  default_inferred_data_connector_name:\n    class_name: InferredAssetFilesystemDataConnector\n    base_directory: ../data\n    default_regex:\n      group_names:\n        - data_asset_name\n      pattern: (.*)\n  default_runtime_data_connector_name:\n    class_name: RuntimeDataConnector\n    assets:\n      my_runtime_asset_name:\n        batch_identifiers:\n          - runtime_batch_identifier_name\n\"\"\"\nprint(example_yaml)\n\n\nname: data_quality_demo\nclass_name: Datasource\nexecution_engine:\n  class_name: PandasExecutionEngine\ndata_connectors:\n  default_inferred_data_connector_name:\n    class_name: InferredAssetFilesystemDataConnector\n    base_directory: ../data\n    default_regex:\n      group_names:\n        - data_asset_name\n      pattern: (.*)\n  default_runtime_data_connector_name:\n    class_name: RuntimeDataConnector\n    assets:\n      my_runtime_asset_name:\n        batch_identifiers:\n          - runtime_batch_identifier_name"
  },
  {
    "objectID": "data_quality/setup_datasource.html#save-your-datasource-configuration",
    "href": "data_quality/setup_datasource.html#save-your-datasource-configuration",
    "title": "GiveDirectly",
    "section": "Save Your Datasource Configuration",
    "text": "Save Your Datasource Configuration\nHere we will save your Datasource in your Data Context once you are satisfied with the configuration. Note that overwrite_existing defaults to False, but you may change it to True if you wish to overwrite. Please note that if you wish to include comments you must add them directly to your great_expectations.yml.\n\nsanitize_yaml_and_save_datasource(context, example_yaml, overwrite_existing=False)\ncontext.list_datasources()\n\n[{'execution_engine': {'class_name': 'PandasExecutionEngine',\n   'module_name': 'great_expectations.execution_engine'},\n  'module_name': 'great_expectations.datasource',\n  'class_name': 'Datasource',\n  'name': 'data_quality_demo',\n  'data_connectors': {'default_inferred_data_connector_name': {'default_regex': {'group_names': ['data_asset_name'],\n     'pattern': '(.*)'},\n    'base_directory': '../data',\n    'module_name': 'great_expectations.datasource.data_connector',\n    'class_name': 'InferredAssetFilesystemDataConnector'},\n   'default_runtime_data_connector_name': {'module_name': 'great_expectations.datasource.data_connector',\n    'class_name': 'RuntimeDataConnector',\n    'assets': {'my_runtime_asset_name': {'module_name': 'great_expectations.datasource.data_connector.asset',\n      'class_name': 'Asset',\n      'batch_identifiers': ['runtime_batch_identifier_name']}}}}}]"
  },
  {
    "objectID": "data_quality/four_steps_to_data_quality.html",
    "href": "data_quality/four_steps_to_data_quality.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Great Expectations is a python package which helps ensuring data quality in four steps.\n\nSetup the Data context\nConnect to data\nCreate Expectations\nValidate Data\n\n\n\n\nFollow these four steps\n\n\n\n\nIn this Demo we will be using NYC taxi data to show how we can ensure the quality of data in production. This is an open data set which is updated every month. Each record in the data corresponds to one taxi ride and contains information such as the pick-up and drop-off location, the payment amount, and the number of passengers, among others.\nWe will be using two CSV files, each with 10,000 row sample of taxi trip records. A sample for January 2019 and a sample for February 2019.\nFor purposes of this tutorial, we are treating the January 2019 taxi data as our “current” data, and the February 2019 taxi data as “future” data that we have not yet looked at. We will use Great Expectations to build a profile of the January data and then use that profile to check for any unexpected data quality issues in the February data. In a real-life scenario, this would ensure that any problems with the February data would be caught (so it could be dealt with) before the February data is used in a production application!\n\n\n\n\n\n\n\n\n\n\n\nExplore Expecatations\n\n\n\n\nExplore validations"
  },
  {
    "objectID": "data_quality/run_checkpoint.html",
    "href": "data_quality/run_checkpoint.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Create Checkpoint\nUse this notebook to configure a new Checkpoint and add it to your project:\nCheckpoint Name: data_quality_demo_checkpoint\n\nimport os\nos.chdir('/home/thulasiram/personal/going_deep_and_wide/GiveDirectly/gx_tutorials/great_expectations')\n\n\nfrom ruamel.yaml import YAML\nimport great_expectations as gx\nfrom pprint import pprint\n\nyaml = YAML()\ncontext = gx.get_context()\n\n\n\nCreate a Checkpoint Configuration\nIf you are new to Great Expectations or the Checkpoint feature, you should start with SimpleCheckpoint because it includes default configurations like a default list of post validation actions.\nIn the cell below we have created a sample Checkpoint configuration using your configuration and SimpleCheckpoint to run a single validation of a single Expectation Suite against a single Batch of data.\nTo keep it simple, we are just choosing the first available instance of each of the following items you have configured in your Data Context: * Datasource * DataConnector * DataAsset * Partition * Expectation Suite\nOf course this is purely an example, you may edit this to your heart’s content.\nMy configuration is not so simple - are there more advanced options?\nGlad you asked! Checkpoints are very versatile. For example, you can validate many Batches in a single Checkpoint, validate Batches against different Expectation Suites or against many Expectation Suites, control the specific post-validation actions based on Expectation Suite / Batch / results of validation among other features. Check out our documentation on Checkpoints for more details and for instructions on how to implement other more advanced features including using the Checkpoint class: - https://docs.greatexpectations.io/docs/reference/checkpoints_and_actions - https://docs.greatexpectations.io/docs/guides/validation/checkpoints/how_to_create_a_new_checkpoint - https://docs.greatexpectations.io/docs/guides/validation/checkpoints/how_to_configure_a_new_checkpoint_using_test_yaml_config\n\nmy_checkpoint_name = \"data_quality_demo_checkpoint\" # This was populated from your CLI command.\n\nyaml_config = f\"\"\"\nname: {my_checkpoint_name}\nconfig_version: 1.0\nclass_name: SimpleCheckpoint\nrun_name_template: \"%Y%m%d-%H%M%S-my-run-name-template\"\nvalidations:\n  - batch_request:\n      datasource_name: data_quality_demo\n      data_connector_name: default_inferred_data_connector_name\n      data_asset_name: yellow_tripdata_sample_2019-02.csv\n      data_connector_query:\n        index: -1\n    expectation_suite_name: data_quality_expectation_demo\n\"\"\"\nprint(yaml_config)\n\n\nname: data_quality_demo_checkpoint\nconfig_version: 1.0\nclass_name: SimpleCheckpoint\nrun_name_template: \"%Y%m%d-%H%M%S-my-run-name-template\"\nvalidations:\n  - batch_request:\n      datasource_name: data_quality_demo\n      data_connector_name: default_inferred_data_connector_name\n      data_asset_name: yellow_tripdata_sample_2019-02.csv\n      data_connector_query:\n        index: -1\n    expectation_suite_name: data_quality_expectation_demo\n\n\n\n\n\nCustomize Your Configuration\nThe following cells show examples for listing your current configuration. You can replace values in the sample configuration with these values to customize your Checkpoint.\n\n# Run this cell to print out the names of your Datasources, Data Connectors and Data Assets\npprint(context.get_available_data_asset_names())\n\n{'data_quality_demo': {'default_inferred_data_connector_name': ['yellow_tripdata_sample_2019-01.csv',\n                                                                'yellow_tripdata_sample_2019-02.csv'],\n                       'default_runtime_data_connector_name': ['my_runtime_asset_name']}}\n\n\n\ncontext.list_expectation_suite_names()\n\n['data_quality_expectation_demo']\n\n\n\n\nTest Your Checkpoint Configuration\nHere we will test your Checkpoint configuration to make sure it is valid.\nThis test_yaml_config() function is meant to enable fast dev loops. If your configuration is correct, this cell will show a message that you successfully instantiated a Checkpoint. You can continually edit your Checkpoint config yaml and re-run the cell to check until the new config is valid.\nIf you instead wish to use python instead of yaml to configure your Checkpoint, you can use context.add_checkpoint() and specify all the required parameters.\n\nmy_checkpoint = context.test_yaml_config(yaml_config=yaml_config)\n\nAttempting to instantiate class from config...\n    Instantiating as a SimpleCheckpoint, since class_name is SimpleCheckpoint\n    Successfully instantiated SimpleCheckpoint\n\n\nCheckpoint class name: SimpleCheckpoint\n\n\n\n\nReview Your Checkpoint\nYou can run the following cell to print out the full yaml configuration. For example, if you used SimpleCheckpoint this will show you the default action list.\n\nprint(my_checkpoint.get_config(mode=\"yaml\"))\n\nname: data_quality_demo_checkpoint\nconfig_version: 1.0\ntemplate_name:\nmodule_name: great_expectations.checkpoint\nclass_name: Checkpoint\nrun_name_template: '%Y%m%d-%H%M%S-my-run-name-template'\nexpectation_suite_name:\nbatch_request: {}\naction_list:\n  - name: store_validation_result\n    action:\n      class_name: StoreValidationResultAction\n  - name: store_evaluation_params\n    action:\n      class_name: StoreEvaluationParametersAction\n  - name: update_data_docs\n    action:\n      class_name: UpdateDataDocsAction\n      site_names: []\nevaluation_parameters: {}\nruntime_configuration: {}\nvalidations:\n  - batch_request:\n      datasource_name: data_quality_demo\n      data_connector_name: default_inferred_data_connector_name\n      data_asset_name: yellow_tripdata_sample_2019-02.csv\n      data_connector_query:\n        index: -1\n    expectation_suite_name: data_quality_expectation_demo\nprofilers: []\nge_cloud_id:\nexpectation_suite_ge_cloud_id:\n\n\n\n\n\nAdd Your Checkpoint\nRun the following cell to save this Checkpoint to your Checkpoint Store.\n\ncontext.add_checkpoint(**yaml.load(yaml_config))\n\n{\n  \"action_list\": [\n    {\n      \"name\": \"store_validation_result\",\n      \"action\": {\n        \"class_name\": \"StoreValidationResultAction\"\n      }\n    },\n    {\n      \"name\": \"store_evaluation_params\",\n      \"action\": {\n        \"class_name\": \"StoreEvaluationParametersAction\"\n      }\n    },\n    {\n      \"name\": \"update_data_docs\",\n      \"action\": {\n        \"class_name\": \"UpdateDataDocsAction\",\n        \"site_names\": []\n      }\n    }\n  ],\n  \"batch_request\": {},\n  \"class_name\": \"Checkpoint\",\n  \"config_version\": 1.0,\n  \"evaluation_parameters\": {},\n  \"module_name\": \"great_expectations.checkpoint\",\n  \"name\": \"data_quality_demo_checkpoint\",\n  \"profilers\": [],\n  \"run_name_template\": \"%Y%m%d-%H%M%S-my-run-name-template\",\n  \"runtime_configuration\": {},\n  \"validations\": [\n    {\n      \"batch_request\": {\n        \"datasource_name\": \"data_quality_demo\",\n        \"data_connector_name\": \"default_inferred_data_connector_name\",\n        \"data_asset_name\": \"yellow_tripdata_sample_2019-02.csv\",\n        \"data_connector_query\": {\n          \"index\": -1\n        }\n      },\n      \"expectation_suite_name\": \"data_quality_expectation_demo\"\n    }\n  ]\n}\n\n\n\n\nRun Your Checkpoint & Open Data Docs(Optional)\nYou may wish to run the Checkpoint now and review its output in Data Docs. If so uncomment and run the following cell.\n\ncontext.run_checkpoint(checkpoint_name=my_checkpoint_name)\ncontext.open_data_docs()"
  },
  {
    "objectID": "data_quality/generate_expectation.html",
    "href": "data_quality/generate_expectation.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "This process helps you avoid writing lots of boilerplate when authoring suites by allowing you to select columns and other factors that you care about and letting a profiler write some candidate expectations for you to adjust.\nExpectation Suite Name: data_quality_expectation_demo\n\nimport os\nos.chdir('/home/thulasiram/personal/going_deep_and_wide/GiveDirectly/gx_tutorials/great_expectations')\n\n\nimport datetime\n\nimport pandas as pd\n\nimport great_expectations as gx\nimport great_expectations.jupyter_ux\nfrom great_expectations.core.batch import BatchRequest\nfrom great_expectations.checkpoint import SimpleCheckpoint\nfrom great_expectations.exceptions import DataContextError\n\ncontext = gx.data_context.DataContext()\n\nbatch_request = {'datasource_name': 'data_quality_demo', 'data_connector_name': 'default_inferred_data_connector_name', 'data_asset_name': 'yellow_tripdata_sample_2019-01.csv', 'limit': 1000}\n\nexpectation_suite_name = \"data_quality_expectation_demo\"\n\nvalidator = context.get_validator(\n    batch_request=BatchRequest(**batch_request),\n    expectation_suite_name=expectation_suite_name\n)\ncolumn_names = [f'\"{column_name}\"' for column_name in validator.columns()]\nprint(f\"Columns: {', '.join(column_names)}.\")\nvalidator.head(n_rows=5, fetch_all=False)\n\n2022-12-23T15:35:54+0530 - INFO - Great Expectations logging enabled at 20 level by JupyterUX module.\n\n\n\n\n\nColumns: \"vendor_id\", \"pickup_datetime\", \"dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"rate_code_id\", \"store_and_fwd_flag\", \"pickup_location_id\", \"dropoff_location_id\", \"payment_type\", \"fare_amount\", \"extra\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\", \"total_amount\", \"congestion_surcharge\".\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      vendor_id\n      pickup_datetime\n      dropoff_datetime\n      passenger_count\n      trip_distance\n      rate_code_id\n      store_and_fwd_flag\n      pickup_location_id\n      dropoff_location_id\n      payment_type\n      fare_amount\n      extra\n      mta_tax\n      tip_amount\n      tolls_amount\n      improvement_surcharge\n      total_amount\n      congestion_surcharge\n    \n  \n  \n    \n      0\n      1\n      2019-01-15 03:36:12\n      2019-01-15 03:42:19\n      1\n      1.0\n      1\n      N\n      230\n      48\n      1\n      6.5\n      0.5\n      0.5\n      1.95\n      0.0\n      0.3\n      9.75\n      NaN\n    \n    \n      1\n      1\n      2019-01-25 18:20:32\n      2019-01-25 18:26:55\n      1\n      0.8\n      1\n      N\n      112\n      112\n      1\n      6.0\n      1.0\n      0.5\n      1.55\n      0.0\n      0.3\n      9.35\n      0.0\n    \n    \n      2\n      1\n      2019-01-05 06:47:31\n      2019-01-05 06:52:19\n      1\n      1.1\n      1\n      N\n      107\n      4\n      2\n      6.0\n      0.0\n      0.5\n      0.00\n      0.0\n      0.3\n      6.80\n      NaN\n    \n    \n      3\n      1\n      2019-01-09 15:08:02\n      2019-01-09 15:20:17\n      1\n      2.5\n      1\n      N\n      143\n      158\n      1\n      11.0\n      0.0\n      0.5\n      3.00\n      0.0\n      0.3\n      14.80\n      NaN\n    \n    \n      4\n      1\n      2019-01-25 18:49:51\n      2019-01-25 18:56:44\n      1\n      0.8\n      1\n      N\n      246\n      90\n      1\n      6.5\n      1.0\n      0.5\n      1.65\n      0.0\n      0.3\n      9.95\n      0.0"
  },
  {
    "objectID": "data_quality/generate_expectation.html#next-steps",
    "href": "data_quality/generate_expectation.html#next-steps",
    "title": "GiveDirectly",
    "section": "Next steps",
    "text": "Next steps\nAfter you review this initial Expectation Suite in Data Docs you should edit this suite to make finer grained adjustments to the expectations. This can be done by running great_expectations suite edit data_quality_expectation_demo."
  },
  {
    "objectID": "cdr_methods/cdr_features.html",
    "href": "cdr_methods/cdr_features.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "active_days\nnumber_of_contacts\nnumber_of_interactions\ncall_duration\npercent_nocturnal\npercent_initiated_conversations\nresponse_delay_text\nresponse_rate_text\nentropy_of_contacts\nInteractions_per_contact\npercent_pareto_interactions (percentage of user’s contact that account for 80% of its interactions)\npercent_pareto_durations\n\n\n\n\n\nNumber of unique places (antennas) visited\nEntropy of antennas\npercent at home\nradius of gyration (the equivalent distance of the mass from the center of gravity, for all visited places)\nfrequent_antennas - location that accounts for 80% of the locations the user was\nchurn_rate - Computes the frequency spent at every towers each week, and returns the distribution of the cosine similarity between two consecutives week\n\n\n\n\n\nDirected, weighted matrix for call, text etc\nDirected, Unweighted matrix\nUndirected, weighted matrix\nUndirected, Unweighted matrix\nClustering coefficient - Measure of the degree to which nodes in a graph tend to cluster together\nclustering coefficient unweighted of users weighted undirected network\nclustering coefficient weighted (undirected)\nassortativity of indicators(The extent to which nodes of a graphlink to others of the same degree)\nassortativity of attributes\n\n\n\n\n\nRecharge amounts\nTime between recharges\npercent pareto recharges\nNumber of recharges\nAverage daily balance estimated from all recharges\n\n\n\n\n\n\nMobile data will not be uniform across different networks. A different model may be required for different network."
  },
  {
    "objectID": "cdr_methods/resources.html",
    "href": "cdr_methods/resources.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Resources for analyzing CDR\n\nPython packages\nBandicoot open source library by MIT\nCellyzer\n\n\nBlogpost\nCDR data analysis using Neo4j"
  },
  {
    "objectID": "cdr_methods/phone_metadata.html",
    "href": "cdr_methods/phone_metadata.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Predicting poverty and wealth from Mobile phone metadata\n\nResearch paper by prof. Joshua Blumenstock\n\nMobile phone use reflects the structure of individual’s social network, patterns of travel and location choice and histories of consumption and expenditure\nSurvey on asset ownership, housing characteristics and other welfare indicators.\nConstructed a composite wealth index\nMobile phone data is used to predict the wealth index calculated from survey data\nFeatures constructed are:-\n\nTotal volume\nIntensity\nTiming\nDirection of communication etc\nStructure of the individual’s contact network\nPatterns of mobility based on geospatial markers\n\nElastic Net regularization was used in modelling\nGeospatial markers in the phone data enabled to study the geographic distribution of subscriber of wealth at an extremely fine degree of spatial granularity\nThere was a strong correlation between the mobile metadata predictions and the DHS survey data at district and village levels. Correlations persisted even for comparing clusters within urban and rural areas\nThis approach can be used to predict other metrics as well. Rates of district electrification estimated from phone records are comparable to those reported in the DHS survey\n\n\n\n\nPredicted Vs Actual wealth of Mobile users\n\n\n\n\n\nWealth Prediction for Rawanda"
  },
  {
    "objectID": "data_manipulation/series_manipulation.html",
    "href": "data_manipulation/series_manipulation.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Pandas Series Manipulation\n\nimport pandas as pd\nimport numpy as np\n\n\nConvert non-categorical series to an ordered category\n\ns2 = pd.Series(['m' , 'l' , 'xs' , 's' , 'xl'],dtype='category')\n\n\ns2\n\n0     m\n1     l\n2    xs\n3     s\n4    xl\ndtype: category\nCategories (5, object): ['l', 'm', 's', 'xl', 'xs']\n\n\n\n# Ordering by creating a new series\nsize_type = pd.api.types.CategoricalDtype(\n    categories=['s','m','l'],ordered=True)\ns3 = s2.astype(size_type)\n\n\ns3\n\n0      m\n1      l\n2    NaN\n3      s\n4    NaN\ndtype: category\nCategories (3, object): ['s' < 'm' < 'l']\n\n\n\n# Now we can do comparisons on them\ns3 > 's'\n\n0     True\n1     True\n2    False\n3    False\n4    False\ndtype: bool\n\n\n\n# Ordering the original series itself\ns2.cat.reorder_categories(['xs','s','m','l','xl'],ordered=True)\n\n0     m\n1     l\n2    xs\n3     s\n4    xl\ndtype: category\nCategories (5, object): ['xs' < 's' < 'm' < 'l' < 'xl']\n\n\n\n\n\nMethods on Series\n\n\nSeries column dtype will be coerced to float if Null values are there. The column type will be int in case null values are not there.\n\n\nCheck memory usage of a Series\n\n# shows memory the pandas object is taking\ns3.nbytes\n\n29\n\n\n\n# shows memory that includes strings\ns3.memory_usage()\n\n265\n\n\n\n# Include memory used by python objects in the series\ns3.memory_usage(deep=True)\n\n415\n\n\n\n\nTreating outliers with clip method\n\nurl = 'https://github.com/mattharrison/datasets/raw/master/data/vehicles.csv.zip'\n\n\ndf = pd.read_csv(url)\n\n/tmp/ipykernel_202760/4176769558.py:1: DtypeWarning: Columns (68,70,71,72,73,74,76,79) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(url)\n\n\n\ncity_mpg = df.city08\n\n\nhighway_mpg = df.highway08\n\n\n# chaining on series\n(city_mpg\n    .loc[:446]\n    .clip(lower=city_mpg.quantile(.05),\n          upper=city_mpg.quantile(.95))\n)\n\n0      19\n1      11\n2      23\n3      11\n4      17\n       ..\n442    15\n443    15\n444    15\n445    15\n446    27\nName: city08, Length: 447, dtype: int64"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Study undertook to predict poverty using Data science and Machine learning"
  },
  {
    "objectID": "misc/datasets.html",
    "href": "misc/datasets.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Satellites\n\nLandsat\nDigitalGlobe\nSentinel\nMODIS\n\n\n\nWeb services\n\nplanet (website)\nGoogle Earth static map API"
  },
  {
    "objectID": "misc/challenges.html",
    "href": "misc/challenges.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "* Model Interpretability * Privacy of the mobile users * Fairness of the algorithms *"
  },
  {
    "objectID": "distributed_processing/fugue_quickstart.html",
    "href": "distributed_processing/fugue_quickstart.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Fugue Quickstart\n\nImport the required libraries\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom typing import List, Dict, Iterable, Any\n\n\n\nCreate a model in Sklearn and do predictions using Spark\n\n\nX = pd.DataFrame({\"x_1\": [1, 1, 2, 2], \"x_2\":[1, 2, 2, 3]})\ny = np.dot(X, np.array([1, 2])) + 3\nreg = LinearRegression().fit(X, y)\n\n\n# define our predict function\ndef predict(df: pd.DataFrame, model: LinearRegression) -> pd.DataFrame:\n    \"\"\"\n    Function to predict results using a pre-built model\n    \"\"\"\n    return df.assign(predicted=model.predict(df))\n\n# create test data\ninput_df = pd.DataFrame({\"x_1\": [3, 4, 6, 6], \"x_2\":[3, 3, 6, 6]})\n\n# test the predict function\npredict(input_df, reg)\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      predicted\n    \n  \n  \n    \n      0\n      3\n      3\n      12.0\n    \n    \n      1\n      4\n      3\n      13.0\n    \n    \n      2\n      6\n      6\n      21.0\n    \n    \n      3\n      6\n      6\n      21.0\n    \n  \n\n\n\n\n\n# import Fugue\nfrom fugue import transform\n\n# create a spark dataframe\nsdf = spark.createDataFrame(input_df)\n\n# use Fugue transform to switch exection to spark\nresult = transform(\n    df=sdf,\n    using=predict,\n    schema=\"*,predicted:double\",\n    params=dict(model=reg),\n    engine=spark\n)\n\n# display results\nprint(type(result))\nresult.show()\n\n<class 'pyspark.sql.dataframe.DataFrame'>\n\n\n[Stage 2:==========================================>               (8 + 3) / 11]\n\n\n+---+---+------------------+\n|x_1|x_2|         predicted|\n+---+---+------------------+\n|  3|  3|              12.0|\n|  4|  3|              13.0|\n|  6|  6|20.999999999999996|\n|  6|  6|20.999999999999996|\n+---+---+------------------+\n\n\n\n                                                                                \n\n\n\n\nDo the predictions in Dask\n\n# using transform to bring predict to dask execution\nresult = transform(\n    df=input_df.copy(),\n    using=predict,\n    schema=\"*,predicted:double\",\n    params=dict(model=reg),\n    engine=\"dask\"\n)\n\n# display results\nprint(type(result))\nresult.compute().head()\n\n<class 'dask.dataframe.core.DataFrame'>\n\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      predicted\n    \n  \n  \n    \n      0\n      3\n      3\n      12.0\n    \n    \n      0\n      4\n      3\n      13.0\n    \n    \n      0\n      6\n      6\n      21.0\n    \n    \n      0\n      6\n      6\n      21.0\n    \n  \n\n\n\n\n\n\nReturn the output as a Pandas Dataframe\n\n# use as_local=True to return a Pandas DataFrame\nlocal_result = transform(\n    df=input_df,\n    using=predict,\n    schema=\"*,predicted:double\",\n    params=dict(model=reg),\n    engine=\"dask\",\n    as_local=True\n)\n\nprint(type(local_result))\nlocal_result.head()\n\n<class 'pandas.core.frame.DataFrame'>\n\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      predicted\n    \n  \n  \n    \n      0\n      3\n      3\n      12.0\n    \n    \n      1\n      4\n      3\n      13.0\n    \n    \n      2\n      6\n      6\n      21.0\n    \n    \n      3\n      6\n      6\n      21.0\n    \n  \n\n\n\n\n\n\nType Hints\nThe input type annotation tells Fugue what to convert the input data to before the function is applied whereas the output type annotation informs Fugue how to convert it back to a Pandas, Spark, Dask, or Ray DataFrame.\n\n\nSchema\nWhen using transform() function, the best practice is to provide schema definition.When using the transform(), the * in a schema expression means all existing columns. From there we can add new columns by adding “,column_name:type”\n\ndf = pd.DataFrame({\"a\": [1,2,3], \"b\": [1,2,3], \"c\": [1,2,3]})\n\n\ndef add_col(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Function that creates a column with a value of column a + 1.\n    \"\"\"\n    return df.assign(new_col=df[\"a\"] + 1)\n\ntransform(\n    df=df, \n    using=add_col, \n    schema=\"*,new_col:int\"\n    )\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n      new_col\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      2\n    \n    \n      1\n      2\n      2\n      2\n      3\n    \n    \n      2\n      3\n      3\n      3\n      4\n    \n  \n\n\n\n\n\n\nPartitioning\nThe type hint conversion is applied on the partition level.\n\ndf = pd.DataFrame({\"a\": [1,2,3,4], \"b\": [1,2,3,4], \"c\": [1,2,3,4]})\n\ndef size(df: pd.DataFrame) -> Iterable[Dict[str,Any]]:\n    \"\"\"\n    Function that calculates the size of a DataFrame.\n    \"\"\"\n    yield {\"size\":df.shape[0]}\n\n\ntransform(\n    df=df, \n    using=size, \n    schema=\"size:int\", \n    engine=\"dask\",\n    as_local=True\n    )\n\n\n\n\n\n  \n    \n      \n      size\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      1\n    \n    \n      2\n      1\n    \n    \n      3\n      1\n    \n  \n\n\n\n\nThe type hint conversion happens on each partition. We can control the partition by specifying the column.\n\ndf = pd.DataFrame({\"col1\": [\"a\",\"a\",\"a\",\"b\",\"b\",\"b\"], \n                   \"col2\": [1,2,3,4,5,6]})\ndf\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n    \n  \n  \n    \n      0\n      a\n      1\n    \n    \n      1\n      a\n      2\n    \n    \n      2\n      a\n      3\n    \n    \n      3\n      b\n      4\n    \n    \n      4\n      b\n      5\n    \n    \n      5\n      b\n      6\n    \n  \n\n\n\n\n\ndef min_max(df:pd.DataFrame) -> List[Dict[str,Any]]:\n    \"\"\"\n    Calculates the min and max of a given column based\n    on the grouping of a separate column.\n    \"\"\"\n    return [{\"group\": df.iloc[0][\"col1\"], \n             \"max\": df['col2'].max(), \n             \"min\": df['col2'].min()}]\n\n\ntransform(\n    df=df, \n    using=min_max, \n    schema=\"group:str, max:int, min:int\",\n    partition={\"by\": \"col1\"}\n    )\n\n\n\n\n\n  \n    \n      \n      group\n      max\n      min\n    \n  \n  \n    \n      0\n      a\n      3\n      1\n    \n    \n      1\n      b\n      6\n      4\n    \n  \n\n\n\n\nWe can use transform() operation to save the output as a parquet file\n\ndf = pd.DataFrame({\"a\": [1,2,3], \"b\": [1,2,3], \"c\": [1,2,3]})\ndf.to_parquet(\"../data/df.parquet\")\n\n\ndef drop_col(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    A function that drops a column labelled 'b'.\n    \"\"\"\n    return df.drop(\"b\", axis=1)\n\ntransform(\n    df=\"../data/df.parquet\",\n    using=drop_col,\n    schema=\"*-b\",\n    engine=spark,\n    save_path=\"../data/processed.parquet\"\n    )\n\npd.read_parquet(\"../data/processed.parquet/\").head()\n\n                                                                                \n\n\n\n\n\n\n  \n    \n      \n      a\n      c\n    \n  \n  \n    \n      0\n      1\n      1\n    \n    \n      1\n      2\n      2\n    \n    \n      2\n      3\n      3\n    \n  \n\n\n\n\nThis expression makes it easy for users to toggle between running Pandas with sampled data and using Spark, Dask or Ray on the full dataset.We can use transform() to distribute the processing of a single step in our process."
  },
  {
    "objectID": "distributed_processing/fugue_intro.html",
    "href": "distributed_processing/fugue_intro.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Fugue\nThis module is included to show how we can easily port pandas code to a compute engine of our choice (with minimal code changes).\nFugue provides an easier interface to using distributed compute effectively and accelerates big data projects.Fugue ports Python, Pandas and SQL code to Spark, Dask and Ray\n\n\n\nBenefits of Fugue"
  },
  {
    "objectID": "distributed_processing/fugue_sql.html",
    "href": "distributed_processing/fugue_sql.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "FugueSQL\nFugueSQL can be used on top of Pandas, Spark and Dask. FugueSQL is parsed and then executed on top of the underlying engine.\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom fugue_notebook import setup\nsetup(is_lab=False)\n\n\n\n\n\n\nimport pandas as pd\n\ndf = pd.DataFrame({\"col1\": [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"], \"col2\": [1,2,3,4,5,6]})\ndf2 = pd.DataFrame({\"col1\": [\"A\", \"B\"], \"col3\": [1, 2]})\n\n\nRun FugueSQL\n\n%%fsql\n   SELECT df.col1, df.col2, df2.col3\n     FROM df\nLEFT JOIN df2\n       ON df.col1 = df2.col1\n    WHERE df.col1 = \"A\"\n    PRINT\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n      col3\n    \n  \n  \n    \n      0\n      A\n      1\n      1\n    \n    \n      1\n      A\n      2\n      1\n    \n    \n      2\n      A\n      3\n      1\n    \n  \n\n\n\n\nschema: col1:str,col2:long,col3:long\n\n\n\n\nUsing FugueSQL dataframe in Python\n\n%%fsql\nSELECT *\n  FROM df\n YIELD DATAFRAME AS result\n\n\nprint(type(result))\nprint(result.native.head())\n\n<class 'fugue.dataframe.pandas_dataframe.PandasDataFrame'>\n  col1  col2\n0    A     1\n1    A     2\n2    A     3\n3    B     4\n4    B     5\n\n\n\n\nLoading files\n\n%%fsql\ndf = LOAD \"../data/processed.parquet\"\n\nnew = SELECT *\n        FROM df\n       YIELD DATAFRAME AS result\n\n\nprint(result.native)\n\n   a  c\n0  1  1\n1  2  2\n2  3  3\n\n\nCommon table expressions (CTEs) are also supported by FugueSQL\n\n\nUsing python code on SQL\n\nf = pd.DataFrame({\"col1\": [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"], \"col2\": [1,2,3,4,5,6]})\n\n\n# schema: *+col2:float\ndef std_dev(df: pd.DataFrame) -> pd.DataFrame:\n    return df.assign(col2=df['col2']/df['col2'].max())\n\nThe function above is defined to handle one group of data at a time. In order to apply it per group, we partition the DataFrame first by group using the PREPARTITION and TRANSFORM keywords of FugueSQL.\n\n%%fsql\nTRANSFORM df PREPARTITION BY col1 USING std_dev\nPRINT\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n    \n  \n  \n    \n      0\n      A\n      0.333333\n    \n    \n      1\n      A\n      0.666667\n    \n    \n      2\n      A\n      1.000000\n    \n    \n      3\n      B\n      0.666667\n    \n    \n      4\n      B\n      0.833333\n    \n    \n      5\n      B\n      1.000000\n    \n  \n\n\n\n\nschema: col1:str,col2:float\n\n\n\n\nRun SQL code using either Duckdb, Spark or Dask engine\nFugue supports Pandas, Spark, Dask, and DuckDB. For operations on a laptop or single machine, DuckDB may give significant improvements over Pandas because it has a query optimizer.\nFor data that is too large to process on a single machine, Spark or Dask can be used. All we need to do is specify the engine in the cell. For example, to run on DuckDB we can do:\n\n%%fsql duckdb\nTRANSFORM df PREPARTITION BY col1 USING std_dev\nPRINT\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n    \n  \n  \n    \n      0\n      A\n      0.333333\n    \n    \n      1\n      A\n      0.666667\n    \n    \n      2\n      A\n      1.000000\n    \n    \n      3\n      B\n      0.666667\n    \n    \n      4\n      B\n      0.833333\n    \n    \n      5\n      B\n      1.000000\n    \n  \n\n\n\n\nschema: col1:str,col2:float\n\n\n\n%%fsql spark\nTRANSFORM df PREPARTITION BY col1 USING std_dev\nPRINT\n\n\n\n\n\n  \n    \n      \n      col1\n      col2\n    \n  \n  \n    \n      0\n      A\n      0.333333\n    \n    \n      1\n      A\n      0.666667\n    \n    \n      2\n      A\n      1.000000\n    \n    \n      3\n      B\n      0.666667\n    \n    \n      4\n      B\n      0.833333\n    \n    \n      5\n      B\n      1.000000\n    \n  \n\n\n\n\nschema: col1:str,col2:float"
  },
  {
    "objectID": "satellite_image_classification/train.html",
    "href": "satellite_image_classification/train.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Satellite Images Classification\n\nImport the required libraries\n\nimport torch\nimport argparse\nimport torch.nn as nn\nimport torch.optim as optim\nimport argparse\nimport cv2\nfrom matplotlib import pyplot as plt\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom model import build_model\nfrom utils import save_model, save_plots\nfrom datasets import train_loader, valid_loader, dataset\nfrom tqdm.notebook import tqdm\n\nClasses: ['cloudy', 'desert', 'green_area', 'water']\nTotal number of images: 5631\nTotal training images: 4505\nTotal valid_images: 1126\n\n\n\n\nLoad the weights for Reset Model\n\nlr = 0.001\nepochs = 20\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"computation device: {device}\\n\")\n\ncomputation device: cuda\n\n\n\n\nmodel = build_model(\n    pretrained=True, fine_tune=False, num_classes=len(dataset.classes)).to(device)\n   \n# total parameters and trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\n\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\\n\")\n\n[INFO]: Loading pre-trained weights\n[INFO]: Freezing hidden layers...\n21,286,724 total parameters.\n2,052 training parameters.\n\n\n\n\n# optimizer\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# loss function\ncriterion = nn.CrossEntropyLoss()\n\n\n\nTraining and Validation Functions\n\ndef train(model, trainloader, optimizer, criterion):\n    model.train()\n    print('Training')\n    train_running_loss = 0.0\n    train_running_correct = 0\n    counter = 0\n    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n        counter += 1\n        image, labels = data\n        image = image.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        # forward pass\n        outputs = model(image)\n        # calculate the loss\n        loss = criterion(outputs, labels)\n        train_running_loss += loss.item()\n        # calculate the accuracy\n        _, preds = torch.max(outputs.data, 1)\n        train_running_correct += (preds == labels).sum().item()\n        # backpropagation\n        loss.backward()\n        # update the optimizer parameters\n        optimizer.step()\n    \n    # loss and accuracy for the complete epoch\n    epoch_loss = train_running_loss / counter\n    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n    return epoch_loss, epoch_acc\n\n\ndef validate(model, testloader, criterion, class_names):\n    model.eval()\n    print('Validation')\n    valid_running_loss = 0.0\n    valid_running_correct = 0\n    counter = 0\n    \n    # we need two lists to keep track of class-wise accuracy\n    class_correct = list(0. for i in range(len(class_names)))\n    class_total = list(0. for i in range(len(class_names)))\n    \n    with torch.no_grad():\n        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n            counter += 1\n            \n            image, labels = data\n            image = image.to(device)\n            labels = labels.to(device)\n            # forward pass\n            outputs = model(image)\n            # calculate the loss\n            loss = criterion(outputs, labels)\n            valid_running_loss += loss.item()\n            # calculate the accuracy\n            _, preds = torch.max(outputs.data, 1)\n            valid_running_correct += (preds == labels).sum().item()\n            \n            # calculate the accuracy for each class\n            correct  = (preds == labels).squeeze()\n            for i in range(len(preds)):\n                label = labels[i]\n                class_correct[label] += correct[i].item()\n                class_total[label] += 1\n        \n    # loss and accuracy for the complete epoch\n    epoch_loss = valid_running_loss / counter\n    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n    \n    # print the accuracy for each class after every epoch\n    print('\\n')\n    for i in range(len(class_names)):\n        print(f\"Accuracy of class {class_names[i]}: {100*class_correct[i]/class_total[i]}\")\n    print('\\n')\n        \n    return epoch_loss, epoch_acc\n\n\n\nTrain for 20 Epochs\n\n# lists to keep track of losses and accuracies\ntrain_loss, valid_loss = [], []\ntrain_acc, valid_acc = [], []\n# start the training\nfor epoch in range(epochs):\n    #print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n    train_epoch_loss, train_epoch_acc = train(model, train_loader, \n                                              optimizer, criterion)\n    valid_epoch_loss, valid_epoch_acc = validate(model, valid_loader,  \n                                                 criterion, dataset.classes)\n    train_loss.append(train_epoch_loss)\n    valid_loss.append(valid_epoch_loss)\n    train_acc.append(train_epoch_acc)\n    valid_acc.append(valid_epoch_acc)\n    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n    print('-'*50)\n# save the trained model weights\nsave_model(epochs, model, optimizer, criterion)\n# save the loss and accuracy plots\nsave_plots(train_acc, valid_acc, train_loss, valid_loss)\nprint('TRAINING COMPLETE')\n\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 95.57522123893806\nAccuracy of class green_area: 96.53979238754326\nAccuracy of class water: 93.8566552901024\n\n\nTraining loss: 0.057, training acc: 98.180\nValidation loss: 0.142, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.37106918238993\nAccuracy of class desert: 92.92035398230088\nAccuracy of class green_area: 97.57785467128028\nAccuracy of class water: 83.2764505119454\n\n\nTraining loss: 0.049, training acc: 98.557\nValidation loss: 0.208, validation acc: 93.428\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 96.88581314878893\nAccuracy of class water: 84.98293515358361\n\n\nTraining loss: 0.048, training acc: 98.424\nValidation loss: 0.189, validation acc: 94.405\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.23008849557522\nAccuracy of class green_area: 95.50173010380622\nAccuracy of class water: 91.46757679180887\n\n\nTraining loss: 0.047, training acc: 98.690\nValidation loss: 0.162, validation acc: 96.004\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.74213836477988\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 89.96539792387543\nAccuracy of class water: 96.24573378839591\n\n\nTraining loss: 0.060, training acc: 98.091\nValidation loss: 0.161, validation acc: 95.648\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.11320754716981\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 96.88581314878893\nAccuracy of class water: 88.39590443686006\n\n\nTraining loss: 0.049, training acc: 98.313\nValidation loss: 0.159, validation acc: 95.382\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.34513274336283\nAccuracy of class green_area: 97.92387543252595\nAccuracy of class water: 91.12627986348123\n\n\nTraining loss: 0.049, training acc: 98.402\nValidation loss: 0.143, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 93.77162629757785\nAccuracy of class water: 93.8566552901024\n\n\nTraining loss: 0.040, training acc: 98.912\nValidation loss: 0.161, validation acc: 96.092\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 92.47787610619469\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 91.80887372013652\n\n\nTraining loss: 0.039, training acc: 98.690\nValidation loss: 0.163, validation acc: 95.115\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 89.419795221843\n\n\nTraining loss: 0.044, training acc: 98.468\nValidation loss: 0.162, validation acc: 95.382\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 97.48427672955975\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 87.5432525951557\nAccuracy of class water: 93.51535836177474\n\n\nTraining loss: 0.043, training acc: 98.513\nValidation loss: 0.178, validation acc: 94.139\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.42767295597484\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 88.23529411764706\nAccuracy of class water: 96.24573378839591\n\n\nTraining loss: 0.041, training acc: 98.602\nValidation loss: 0.162, validation acc: 95.293\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.23008849557522\nAccuracy of class green_area: 86.85121107266436\nAccuracy of class water: 92.83276450511946\n\n\nTraining loss: 0.044, training acc: 98.513\nValidation loss: 0.183, validation acc: 94.139\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.34513274336283\nAccuracy of class green_area: 94.80968858131487\nAccuracy of class water: 92.15017064846417\n\n\nTraining loss: 0.042, training acc: 98.668\nValidation loss: 0.157, validation acc: 95.826\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 92.38754325259515\nAccuracy of class water: 95.56313993174061\n\n\nTraining loss: 0.044, training acc: 98.446\nValidation loss: 0.143, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 96.53979238754326\nAccuracy of class water: 93.51535836177474\n\n\nTraining loss: 0.039, training acc: 98.468\nValidation loss: 0.125, validation acc: 96.714\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 95.15570934256056\nAccuracy of class water: 92.83276450511946\n\n\nTraining loss: 0.038, training acc: 98.713\nValidation loss: 0.149, validation acc: 96.004\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.74213836477988\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 89.27335640138408\nAccuracy of class water: 94.53924914675768\n\n\nTraining loss: 0.041, training acc: 98.713\nValidation loss: 0.154, validation acc: 95.204\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.37106918238993\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 88.73720136518772\n\n\nTraining loss: 0.040, training acc: 98.602\nValidation loss: 0.163, validation acc: 95.471\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 97.2318339100346\nAccuracy of class water: 91.46757679180887\n\n\nTraining loss: 0.042, training acc: 98.557\nValidation loss: 0.133, validation acc: 96.359\n--------------------------------------------------\nTRAINING COMPLETE\n\n\n\n\n\n\n\n\n\n\nInference\n\nimport torch\nimport cv2\nimport torchvision.transforms as transforms\nfrom model import build_model\n\n\ndevice = 'cpu'\n\n\n# list containing all the labels\nlabels = ['cloudy', 'desert', 'green_area', 'water']\n# initialize the model and load the trained weights\nmodel = build_model(\n    pretrained=False, fine_tune=False, num_classes=4\n).to(device)\n\nprint('[INFO]: Loading custom-trained weights...')\ncheckpoint = torch.load('outputs/model.pth', map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# define preprocess transforms\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n]) \n\n[INFO]: Not loading pre-trained weights\n[INFO]: Freezing hidden layers...\n[INFO]: Loading custom-trained weights...\n\n\n\ndef inference(input):\n# read and preprocess the image\n    image = cv2.imread(input)\n    # get the ground truth class\n    gt_class = input.split('/')[-1].split('.')[0]\n    orig_image = image.copy()\n    # convert to RGB format\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = transform(image)\n    # add batch dimension\n    image = torch.unsqueeze(image, 0)\n    with torch.no_grad():\n        outputs = model(image.to(device))\n    output_label = torch.topk(outputs, 1)\n    pred_class = labels[int(output_label.indices)]\n    cv2.putText(orig_image, \n        f\"GT: {gt_class}\",\n        (10, 25),\n        cv2.FONT_HERSHEY_SIMPLEX, \n        1, (0, 255, 0), 2, cv2.LINE_AA\n    )\n    cv2.putText(orig_image, \n        f\"Pred: {pred_class}\",\n        (10, 55),\n        cv2.FONT_HERSHEY_SIMPLEX, \n        1, (0, 0, 255), 2, cv2.LINE_AA\n    )\n    print(f\"GT: {gt_class}, pred: {pred_class}\")\n    #image = cv2.imshow('Result', orig_image)\n    rgb_image = cv2.cvtColor(orig_image,cv2.COLOR_BGR2RGB)\n    fig = plt.figure()\n    plt.axis('off')\n    plt.grid(b=None)\n    plt.imshow(rgb_image)\n    cv2.imwrite(f\"outputs/{gt_class}.png\",\n        orig_image)\n\n\ninference(input='input/test_data/cloudy.jpeg')\n\nGT: cloudy, pred: cloudy\n\n\n\n\n\n\ninference(input='input/test_data/desert.jpeg')\n\nGT: desert, pred: desert"
  },
  {
    "objectID": "deeplearning_implementations/satellite_images_classification.html",
    "href": "deeplearning_implementations/satellite_images_classification.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Satellite Images Classification\n\nImport the required libraries\n\nimport torch\nimport argparse\nimport torch.nn as nn\nimport torch.optim as optim\nimport argparse\nimport cv2\nfrom matplotlib import pyplot as plt\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom model import build_model\nfrom utils import save_model, save_plots\nfrom datasets import train_loader, valid_loader, dataset\nfrom tqdm.notebook import tqdm\n\nClasses: ['cloudy', 'desert', 'green_area', 'water']\nTotal number of images: 5631\nTotal training images: 4505\nTotal valid_images: 1126\n\n\n\n\nLoad the weights for Reset Model\n\nlr = 0.001\nepochs = 20\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"computation device: {device}\\n\")\n\ncomputation device: cuda\n\n\n\n\nmodel = build_model(\n    pretrained=True, fine_tune=False, num_classes=len(dataset.classes)).to(device)\n   \n# total parameters and trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\n\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\\n\")\n\n[INFO]: Loading pre-trained weights\n[INFO]: Freezing hidden layers...\n21,286,724 total parameters.\n2,052 training parameters.\n\n\n\n\n# optimizer\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# loss function\ncriterion = nn.CrossEntropyLoss()\n\n\n\nTraining and Validation Functions\n\ndef train(model, trainloader, optimizer, criterion):\n    model.train()\n    print('Training')\n    train_running_loss = 0.0\n    train_running_correct = 0\n    counter = 0\n    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n        counter += 1\n        image, labels = data\n        image = image.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        # forward pass\n        outputs = model(image)\n        # calculate the loss\n        loss = criterion(outputs, labels)\n        train_running_loss += loss.item()\n        # calculate the accuracy\n        _, preds = torch.max(outputs.data, 1)\n        train_running_correct += (preds == labels).sum().item()\n        # backpropagation\n        loss.backward()\n        # update the optimizer parameters\n        optimizer.step()\n    \n    # loss and accuracy for the complete epoch\n    epoch_loss = train_running_loss / counter\n    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n    return epoch_loss, epoch_acc\n\n\ndef validate(model, testloader, criterion, class_names):\n    model.eval()\n    print('Validation')\n    valid_running_loss = 0.0\n    valid_running_correct = 0\n    counter = 0\n    \n    # we need two lists to keep track of class-wise accuracy\n    class_correct = list(0. for i in range(len(class_names)))\n    class_total = list(0. for i in range(len(class_names)))\n    \n    with torch.no_grad():\n        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n            counter += 1\n            \n            image, labels = data\n            image = image.to(device)\n            labels = labels.to(device)\n            # forward pass\n            outputs = model(image)\n            # calculate the loss\n            loss = criterion(outputs, labels)\n            valid_running_loss += loss.item()\n            # calculate the accuracy\n            _, preds = torch.max(outputs.data, 1)\n            valid_running_correct += (preds == labels).sum().item()\n            \n            # calculate the accuracy for each class\n            correct  = (preds == labels).squeeze()\n            for i in range(len(preds)):\n                label = labels[i]\n                class_correct[label] += correct[i].item()\n                class_total[label] += 1\n        \n    # loss and accuracy for the complete epoch\n    epoch_loss = valid_running_loss / counter\n    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n    \n    # print the accuracy for each class after every epoch\n    print('\\n')\n    for i in range(len(class_names)):\n        print(f\"Accuracy of class {class_names[i]}: {100*class_correct[i]/class_total[i]}\")\n    print('\\n')\n        \n    return epoch_loss, epoch_acc\n\n\n\nTrain for 20 Epochs\n\n# lists to keep track of losses and accuracies\ntrain_loss, valid_loss = [], []\ntrain_acc, valid_acc = [], []\n# start the training\nfor epoch in range(epochs):\n    #print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n    train_epoch_loss, train_epoch_acc = train(model, train_loader, \n                                              optimizer, criterion)\n    valid_epoch_loss, valid_epoch_acc = validate(model, valid_loader,  \n                                                 criterion, dataset.classes)\n    train_loss.append(train_epoch_loss)\n    valid_loss.append(valid_epoch_loss)\n    train_acc.append(train_epoch_acc)\n    valid_acc.append(valid_epoch_acc)\n    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n    print('-'*50)\n# save the trained model weights\nsave_model(epochs, model, optimizer, criterion)\n# save the loss and accuracy plots\nsave_plots(train_acc, valid_acc, train_loss, valid_loss)\nprint('TRAINING COMPLETE')\n\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 95.57522123893806\nAccuracy of class green_area: 96.53979238754326\nAccuracy of class water: 93.8566552901024\n\n\nTraining loss: 0.057, training acc: 98.180\nValidation loss: 0.142, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.37106918238993\nAccuracy of class desert: 92.92035398230088\nAccuracy of class green_area: 97.57785467128028\nAccuracy of class water: 83.2764505119454\n\n\nTraining loss: 0.049, training acc: 98.557\nValidation loss: 0.208, validation acc: 93.428\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 96.88581314878893\nAccuracy of class water: 84.98293515358361\n\n\nTraining loss: 0.048, training acc: 98.424\nValidation loss: 0.189, validation acc: 94.405\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.23008849557522\nAccuracy of class green_area: 95.50173010380622\nAccuracy of class water: 91.46757679180887\n\n\nTraining loss: 0.047, training acc: 98.690\nValidation loss: 0.162, validation acc: 96.004\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.74213836477988\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 89.96539792387543\nAccuracy of class water: 96.24573378839591\n\n\nTraining loss: 0.060, training acc: 98.091\nValidation loss: 0.161, validation acc: 95.648\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.11320754716981\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 96.88581314878893\nAccuracy of class water: 88.39590443686006\n\n\nTraining loss: 0.049, training acc: 98.313\nValidation loss: 0.159, validation acc: 95.382\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.34513274336283\nAccuracy of class green_area: 97.92387543252595\nAccuracy of class water: 91.12627986348123\n\n\nTraining loss: 0.049, training acc: 98.402\nValidation loss: 0.143, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 93.77162629757785\nAccuracy of class water: 93.8566552901024\n\n\nTraining loss: 0.040, training acc: 98.912\nValidation loss: 0.161, validation acc: 96.092\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 92.47787610619469\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 91.80887372013652\n\n\nTraining loss: 0.039, training acc: 98.690\nValidation loss: 0.163, validation acc: 95.115\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 89.419795221843\n\n\nTraining loss: 0.044, training acc: 98.468\nValidation loss: 0.162, validation acc: 95.382\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 97.48427672955975\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 87.5432525951557\nAccuracy of class water: 93.51535836177474\n\n\nTraining loss: 0.043, training acc: 98.513\nValidation loss: 0.178, validation acc: 94.139\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.42767295597484\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 88.23529411764706\nAccuracy of class water: 96.24573378839591\n\n\nTraining loss: 0.041, training acc: 98.602\nValidation loss: 0.162, validation acc: 95.293\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.23008849557522\nAccuracy of class green_area: 86.85121107266436\nAccuracy of class water: 92.83276450511946\n\n\nTraining loss: 0.044, training acc: 98.513\nValidation loss: 0.183, validation acc: 94.139\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.34513274336283\nAccuracy of class green_area: 94.80968858131487\nAccuracy of class water: 92.15017064846417\n\n\nTraining loss: 0.042, training acc: 98.668\nValidation loss: 0.157, validation acc: 95.826\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 92.38754325259515\nAccuracy of class water: 95.56313993174061\n\n\nTraining loss: 0.044, training acc: 98.446\nValidation loss: 0.143, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 96.53979238754326\nAccuracy of class water: 93.51535836177474\n\n\nTraining loss: 0.039, training acc: 98.468\nValidation loss: 0.125, validation acc: 96.714\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 95.15570934256056\nAccuracy of class water: 92.83276450511946\n\n\nTraining loss: 0.038, training acc: 98.713\nValidation loss: 0.149, validation acc: 96.004\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.74213836477988\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 89.27335640138408\nAccuracy of class water: 94.53924914675768\n\n\nTraining loss: 0.041, training acc: 98.713\nValidation loss: 0.154, validation acc: 95.204\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.37106918238993\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 88.73720136518772\n\n\nTraining loss: 0.040, training acc: 98.602\nValidation loss: 0.163, validation acc: 95.471\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 97.2318339100346\nAccuracy of class water: 91.46757679180887\n\n\nTraining loss: 0.042, training acc: 98.557\nValidation loss: 0.133, validation acc: 96.359\n--------------------------------------------------\nTRAINING COMPLETE\n\n\n\n\n\n\n\n\n\n\nInference\n\nimport torch\nimport cv2\nimport torchvision.transforms as transforms\nfrom model import build_model\n\n\ndevice = 'cpu'\n\n\n# list containing all the labels\nlabels = ['cloudy', 'desert', 'green_area', 'water']\n# initialize the model and load the trained weights\nmodel = build_model(\n    pretrained=False, fine_tune=False, num_classes=4\n).to(device)\n\nprint('[INFO]: Loading custom-trained weights...')\ncheckpoint = torch.load('outputs/model.pth', map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# define preprocess transforms\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n]) \n\n[INFO]: Not loading pre-trained weights\n[INFO]: Freezing hidden layers...\n[INFO]: Loading custom-trained weights...\n\n\n\ndef inference(input):\n# read and preprocess the image\n    image = cv2.imread(input)\n    # get the ground truth class\n    gt_class = input.split('/')[-1].split('.')[0]\n    orig_image = image.copy()\n    # convert to RGB format\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = transform(image)\n    # add batch dimension\n    image = torch.unsqueeze(image, 0)\n    with torch.no_grad():\n        outputs = model(image.to(device))\n    output_label = torch.topk(outputs, 1)\n    pred_class = labels[int(output_label.indices)]\n    cv2.putText(orig_image, \n        f\"GT: {gt_class}\",\n        (10, 25),\n        cv2.FONT_HERSHEY_SIMPLEX, \n        1, (0, 255, 0), 2, cv2.LINE_AA\n    )\n    cv2.putText(orig_image, \n        f\"Pred: {pred_class}\",\n        (10, 55),\n        cv2.FONT_HERSHEY_SIMPLEX, \n        1, (0, 0, 255), 2, cv2.LINE_AA\n    )\n    print(f\"GT: {gt_class}, pred: {pred_class}\")\n    #image = cv2.imshow('Result', orig_image)\n    rgb_image = cv2.cvtColor(orig_image,cv2.COLOR_BGR2RGB)\n    fig = plt.figure()\n    plt.axis('off')\n    plt.grid(b=None)\n    plt.imshow(rgb_image)\n    cv2.imwrite(f\"outputs/{gt_class}.png\",\n        orig_image)\n\n\ninference(input='input/test_data/cloudy.jpeg')\n\nGT: cloudy, pred: cloudy\n\n\n\n\n\n\ninference(input='input/test_data/desert.jpeg')\n\nGT: desert, pred: desert"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A Study on predicting poverty using Data Science and Machine learning"
  },
  {
    "objectID": "how_to_measure_poverty/approches.html",
    "href": "how_to_measure_poverty/approches.html",
    "title": "GiveDirectly",
    "section": "",
    "text": "Approaches to measuring poverty - Notes from the book\n\n\nAccording to the Book Measuring Poverty Around the World by Prof. Anthony B Atkinson, the following are different ways to measure poverty are - Basic Needs based approach, Capabilities based approach, Rights based approach and Subjective and perception based approach\n\n\n\nThis approach is based on either consumption or Income.\nIn this approach there are three steps involved. They are\n\nEstimating the nutritional requirements\nConverting the nutritional requirements into food budget\nMaking an allowance for nonfood items\n\nAbove three steps will decide the threshold for the poverty line\nThe drawback of this approach is that a lot of judegment is required at every step of the process making comparisions difficult across time and regions\n\n\n\n\n\nIndividual well-being should be judged in terms of the functionings achieved by the individual and of the capabilities open to them\nIn terms of measuring poverty this should be seen as a deprivation of capabilities, where the deprivation limits the capabilties of the individual to purse their goals in life.\nPoverty is measured by asking the individual about his/her current income and his unmet aspirations like taking a vacation, purchasing a bike etc. There is no clarity on what threshold to select for the poverty in this case.\nTheir is judgement involved in this approach as well\n\n\n\n\nCapability Approach to Absolute and Relative poverty\n\n\n\n\n\n\nEqual rights are provided to all individuals in this approach. In Basic needs approach the poverty line for men and women was different due to differences in estimation for nutritional needs. In Rights approach all humans are considered equal. Along with the food needs Rights approach also considers health, education, and other dimensions as well as a basic human right\n\n\n\n\n\nIn subjective approach each individual classifies himself if he is poor or not. There was lot of criticism that institutions measuring poverty is not considering the views of the people who are suffering from poverty. This method adds considerable value by recording the view of the individual. The drawback of this method is that there are multiple poverty values existing in parallel and this method is subjective and requires a objective measure to make it more actionable.\n\n\n\n\n\nIncome and expenditure are commonly used as metrics to be compared with a determined poverty threshold\nSurveys on living standards and household income and expenditure are generally the sources of data used for deriving expenditure and income\nIn most developed countries, the national poverty line is based on relative standards. This relative poverty line considers the median income of an individual or family to maintain an average living standard as a point of comparison to those who might be considered poor.\nThe World Bank has international poverty lines including $3.20 per day, which is based on 2011 purchasing power parity. However, many countries adopt the cost of basic needs approach in measuring absolute poverty. This estimates severe deprivation of basic human needs such as food, safe drinking water, sanitation facilities, health, shelter, education, and information (UN 1995). The approach determines a food basket that meets the minimum nutritional requirements set by the World Health Organization and Food and Agricultural Organization of the United Nations.\nIncorporating “equivalence scale adjustments” is another common practice in poverty estimation. An equivalence scale indicates that households with the same income or expenditure do not necessarily have the same economic capacity, since this capacity will depend on the number of dependent members in the household. Economic status is therefore usually determined by dividing the household income or expenditure by the family or household size,then determining whether the resulting value is above or below the poverty line. Some NSOs also assign index weights based on the age of the family members to estimate poverty"
  }
]