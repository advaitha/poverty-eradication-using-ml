[
  {
    "objectID": "spatial_data_processing/visualization.html",
    "href": "spatial_data_processing/visualization.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Data Visualization using Folium\n\nimport folium\n\n\nfrom pyproj import crs\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n\nCreate a simple Interactive Map\n\nm = folium.Map(location=[20.59,78.96],zoom_start=5,control_scale=True)\n\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nChange the Style of the Map\n\nm = folium.Map(\n    location = [20.59,78.96],\n    tiles = \"Stamen Toner\",\n    zoom_start = 5,\n    control_scale = True,\n    prefer_canvas = True\n)\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nAdding layers to the Map - Pin location\n\n#18.5786832,73.7666697\nm = folium.Map(location=[20.59,78.96],\n                zoom_start=5,control_scale=True)\nfolium.Marker(\n    location = [18.5786832,73.7666697],\n    popup='Sai Eshnaya Apartments',\n    icon = folium.Icon(color='green',icon='ok-sign')\n).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nMark good resedential areas in Pune\n\npoints_fp = '../data/addresses.shp'\npoints = gpd.read_file(points_fp)\n\n\npoints.head()\n\n\n\n\n\n  \n    \n      \n      id\n      addr\n      geometry\n    \n  \n  \n    \n      0\n      1000\n      Boat Club Road, 411001, Pune, Maharastra\n      POINT (73.87826 18.53937)\n    \n    \n      1\n      1001\n      Koregaon, 415501, Pune, Maharastra\n      POINT (73.89299 18.53772)\n    \n    \n      2\n      1002\n      Kothrud, 411038, Pune, Maharastra\n      POINT (73.80767 18.50389)\n    \n    \n      3\n      1003\n      Balewadi, 411045, Pune, Maharastra\n      POINT (73.76912 18.57767)\n    \n    \n      4\n      1004\n      Baner, 411047, Pune, Maharastra\n      POINT (73.77686 18.56424)\n    \n  \n\n\n\n\n\npoints_gjson = folium.features.GeoJson(points, name='Good Residential Areas')\n\n\nm = folium.Map(location=[18.5786832,73.7666697], tiles=\"cartodbpositron\",\n                zoom_start=8,\n                control_scale=True)\npoints_gjson.add_to(m)\nfolium.LayerControl().add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate a Heatmap of the locations\n\npoints[\"x\"] = points[\"geometry\"].apply(lambda geom: geom.x)\npoints[\"y\"] = points[\"geometry\"].apply(lambda geom: geom.y)\n\n# Create a list of coordinate pairs\nlocations = list(zip(points[\"y\"], points[\"x\"]))\n\n\nfrom folium.plugins import HeatMap\n\n# Create a Map instance\nm = folium.Map(\n    location=[18.5786832,73.7666697], tiles=\"stamentoner\", zoom_start=10, control_scale=True\n)\n\n# Add heatmap to map instance\n# Available parameters: HeatMap(data, name=None, min_opacity=0.5, max_zoom=18, max_val=1.0, radius=25, blur=15, gradient=None, overlay=True, control=True, show=True)\nHeatMap(locations).add_to(m)\n\n# Alternative syntax:\n# m.add_child(HeatMap(points_array, radius=15))\n\n# Show map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate a Clustered point Map\n\nfrom folium.plugins import MarkerCluster\n\n\n# Create a Map instance\nm = folium.Map(\n    location=[18.5786832,73.7666697], tiles=\"cartodbpositron\", zoom_start=12, control_scale=True\n)\n\n\n# Get x and y coordinates for each point\npoints[\"x\"] = points[\"geometry\"].apply(lambda geom: geom.x)\npoints[\"y\"] = points[\"geometry\"].apply(lambda geom: geom.y)\n\n# Create a list of coordinate pairs\nlocations = list(zip(points[\"y\"], points[\"x\"]))\n\n\n# Create a folium marker cluster\nmarker_cluster = MarkerCluster(locations)\n\n# Add marker cluster to map\nmarker_cluster.add_to(m)\n\n# Show map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate a Choropleth Map\n\nimport geopandas as gpd\nfrom pyproj import CRS\nimport requests\nimport geojson\n\n# Specify the url for web feature service\nurl = \"https://kartta.hsy.fi/geoserver/wfs\"\n\n# Specify parameters (read data in json format).\n# Available feature types in this particular data source: http://geo.stat.fi/geoserver/vaestoruutu/wfs?service=wfs&version=2.0.0&request=describeFeatureType\nparams = dict(\n    service=\"WFS\",\n    version=\"2.0.0\",\n    request=\"GetFeature\",\n    typeName=\"asuminen_ja_maankaytto:Vaestotietoruudukko_2018\",\n    outputFormat=\"json\",\n)\n\n# Fetch data from WFS using requests\nr = requests.get(url, params=params)\n\n# Create GeoDataFrame from geojson\ndata = gpd.GeoDataFrame.from_features(geojson.loads(r.content))\n\n# Check the data\ndata.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      index\n      asukkaita\n      asvaljyys\n      ika0_9\n      ika10_19\n      ika20_29\n      ika30_39\n      ika40_49\n      ika50_59\n      ika60_69\n      ika70_79\n      ika_yli80\n    \n  \n  \n    \n      0\n      POLYGON ((25472499.995 6689749.005, 25472499.9...\n      688\n      9\n      28.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      1\n      POLYGON ((25472499.995 6685998.998, 25472499.9...\n      703\n      5\n      51.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      2\n      POLYGON ((25472499.995 6684249.004, 25472499.9...\n      710\n      8\n      44.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      3\n      POLYGON ((25472499.995 6683999.005, 25472499.9...\n      711\n      5\n      90.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n    \n      4\n      POLYGON ((25472499.995 6682998.998, 25472499.9...\n      715\n      11\n      41.0\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n      99\n    \n  \n\n\n\n\n\nfrom pyproj import CRS\n\n# Define crs\ndata.crs = CRS.from_epsg(3879)\n\n\n# Re-project to WGS84\ndata = data.to_crs(epsg=4326)\n\n# Check layer crs definition\nprint(data.crs)\n\nEPSG:4326\n\n\n\n# Change the name of a column\ndata = data.rename(columns={\"asukkaita\": \"pop18\"})\n\n\ndata[\"geoid\"] = data.index.astype(str)\n\n\n# Select only needed columns\ndata = data[[\"geoid\", \"pop18\", \"geometry\"]]\n\n# Convert to geojson (not needed for the simple coropleth map!)\n# pop_json = data.to_json()\n\n# check data\ndata.head()\n\n\n\n\n\n  \n    \n      \n      geoid\n      pop18\n      geometry\n    \n  \n  \n    \n      0\n      0\n      9\n      POLYGON ((24.50236 60.31928, 24.50233 60.32152...\n    \n    \n      1\n      1\n      5\n      POLYGON ((24.50287 60.28562, 24.50284 60.28787...\n    \n    \n      2\n      2\n      8\n      POLYGON ((24.50311 60.26992, 24.50308 60.27216...\n    \n    \n      3\n      3\n      5\n      POLYGON ((24.50315 60.26767, 24.50311 60.26992...\n    \n    \n      4\n      4\n      11\n      POLYGON ((24.50328 60.25870, 24.50325 60.26094...\n    \n  \n\n\n\n\n\nm = folium.Map(\n    location=[60.25, 24.8], tiles=\"cartodbpositron\", zoom_start=10, control_scale=True\n)\n\n# Plot a choropleth map\n# Notice: 'geoid' column that we created earlier needs to be assigned always as the first column\nfolium.Choropleth(\n    geo_data=data,\n    name=\"Population in 2018\",\n    data=data,\n    columns=[\"geoid\", \"pop18\"],\n    key_on=\"feature.id\",\n    fill_color=\"YlOrRd\",\n    fill_opacity=0.7,\n    line_opacity=0.2,\n    line_color=\"white\",\n    line_weight=0,\n    highlight=False,\n    smooth_factor=1.0,\n    # threshold_scale=[100, 250, 500, 1000, 2000],\n    legend_name=\"Population in Helsinki\",\n).add_to(m)\n\n# Show map\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nCreate Choropleth Map with Interaction\n\n# Convert points to GeoJson\nfolium.features.GeoJson(\n    data,\n    name=\"Labels\",\n    style_function=lambda x: {\n        \"color\": \"transparent\",\n        \"fillColor\": \"transparent\",\n        \"weight\": 0,\n    },\n    tooltip=folium.features.GeoJsonTooltip(\n        fields=[\"pop18\"], aliases=[\"Population\"], labels=True, sticky=False\n    ),\n).add_to(m)\n\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "spatial_data_processing/plot_buildings_with_area.html",
    "href": "spatial_data_processing/plot_buildings_with_area.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Visualizing Buildings in a location along with its Area\n\nImport the required libraries\n\nimport osmnx as ox\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nimport keplergl\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\n\nPlot the map for Pune\n\npune = ox.geocode_to_gdf(\"Pune, India\")\npune.plot(edgecolor=\"0.2\")\nplt.title(\"Pune\")\n\nText(0.5, 1.0, 'Pune')\n\n\n\n\n\n\n\nPlot your location on the Map\n\nmy_location = pd.DataFrame(\n    {\"location\":[\"Baner\"],\n    \"Longitude\":[73.7747862],\n    \"Latitude\":[18.578686]}\n)\n\n\nmy_location\n\n\n\n\n\n  \n    \n      \n      location\n      Longitude\n      Latitude\n    \n  \n  \n    \n      0\n      Baner\n      73.774786\n      18.578686\n    \n  \n\n\n\n\n\nmy_location = gpd.GeoDataFrame(my_location,\n                    crs = \"EPSG:4326\",\n                    geometry=gpd.points_from_xy(my_location[\"Longitude\"],my_location[\"Latitude\"]))\n\n\nmy_location\n\n\n\n\n\n  \n    \n      \n      location\n      Longitude\n      Latitude\n      geometry\n    \n  \n  \n    \n      0\n      Baner\n      73.774786\n      18.578686\n      POINT (73.77479 18.57869)\n    \n  \n\n\n\n\n\nax = pune.plot(edgecolor=\"0.2\")\nmy_location.plot(ax=ax,markersize=60,edgecolor=\"0.2\",color='red')\nplt.title(\"My Location in Pune\")\n\nText(0.5, 1.0, 'My Location in Pune')\n\n\n\n\n\n\n\nGet the Bike Routes for your location\n\nbike_network = ox.graph_from_point(center_point=(18.5584546,73.7852182),dist=400,network_type='bike')\nbike_network\n\n<networkx.classes.multidigraph.MultiDiGraph at 0x7f3d9ae77f40>\n\n\n\nbike_network = (ox.graph_to_gdfs(bike_network, nodes=False)\n                  .reset_index(drop=True)\n                  .loc[:, [\"name\", \"length\", \"geometry\"]]\n               )\nbike_network\n\n\n\n\n\n  \n    \n      \n      name\n      length\n      geometry\n    \n  \n  \n    \n      0\n      Gopal Hari Deshmukh Marg\n      52.331\n      LINESTRING (73.78688 18.56143, 73.78638 18.56145)\n    \n    \n      1\n      NaN\n      127.050\n      LINESTRING (73.78688 18.56143, 73.78683 18.560...\n    \n    \n      2\n      Pancard Clubs Road\n      8.998\n      LINESTRING (73.78638 18.56153, 73.78638 18.56145)\n    \n    \n      3\n      Gopal Hari Deshmukh Marg\n      58.852\n      LINESTRING (73.78638 18.56153, 73.78689 18.561...\n    \n    \n      4\n      Pancard Clubs Road\n      8.998\n      LINESTRING (73.78638 18.56145, 73.78638 18.56153)\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      201\n      NaN\n      32.554\n      LINESTRING (73.78206 18.55943, 73.78175 18.55946)\n    \n    \n      202\n      NaN\n      47.738\n      LINESTRING (73.78206 18.55943, 73.78204 18.55986)\n    \n    \n      203\n      NaN\n      19.034\n      LINESTRING (73.78206 18.55943, 73.78224 18.55944)\n    \n    \n      204\n      NaN\n      32.554\n      LINESTRING (73.78175 18.55946, 73.78206 18.55943)\n    \n    \n      205\n      NaN\n      19.034\n      LINESTRING (73.78224 18.55944, 73.78206 18.55943)\n    \n  \n\n206 rows × 3 columns\n\n\n\n\nbike_network.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\ntotal_length = bike_network[\"length\"].sum()\nprint(f\"Total bike lane length: {total_length / 1000:.0f}km\")\n\nTotal bike lane length: 16km\n\n\n\n\nPlot Bike routes on the Map\n\nimport contextily as ctx\n\nax = (bike_network.to_crs(\"EPSG:3857\")\n         .plot(figsize=(10, 8), legend=True,\n               edgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\n     )\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)  # I'm using OSM as the source. See all provides with ctx.providers\nplt.axis(\"off\")\nplt.title(\"Baner\")\n\nText(0.5, 1.0, 'Baner')\n\n\n\n\n\n\n\nGet the building details in your area\n\ntags = {'building':True}\n\n\nbaner_buildings = ox.geometries_from_point(center_point=(18.5584546,73.7852182),dist=400,tags=tags)\n\n\nbaner_buildings = baner_buildings.assign(label='Building Footprints').reset_index()\n\n\n(baner_buildings.head(10).T)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      element_type\n      node\n      way\n      way\n      way\n      way\n      way\n      way\n      way\n      way\n      way\n    \n    \n      osmid\n      1432130601\n      264286363\n      359568513\n      359568520\n      359568545\n      359568551\n      359568562\n      359684077\n      359684097\n      359684099\n    \n    \n      building\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n      yes\n    \n    \n      name\n      UBICS\n      Baneshwar Temple\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      geometry\n      POINT (73.7860015 18.5612441)\n      POLYGON ((73.7869388 18.5589314, 73.7869469 18...\n      POLYGON ((73.7889321 18.5605767, 73.7890453 18...\n      POLYGON ((73.7876847 18.56149, 73.7877554 18.5...\n      POLYGON ((73.78786 18.5613415, 73.7880387 18.5...\n      POLYGON ((73.7881551 18.559907, 73.7882733 18....\n      POLYGON ((73.7828324 18.5618882, 73.782974 18....\n      POLYGON ((73.7813494 18.5612504, 73.7814597 18...\n      POLYGON ((73.781994 18.5613356, 73.7821481 18....\n      POLYGON ((73.7814951 18.5606079, 73.7817434 18...\n    \n    \n      nodes\n      NaN\n      [2699768401, 2699768402, 2699768403, 269976840...\n      [3642483644, 3642483643, 3642483641, 364248364...\n      [3642483654, 3642483653, 3642483648, 364248365...\n      [3642483649, 3642483647, 3642483645, 364248364...\n      [3642483640, 3642483639, 3642483637, 364248363...\n      [3642483660, 3642483659, 3642483657, 364248365...\n      [3643589874, 3643589877, 3643589867, 364358986...\n      [3643589882, 3643589883, 3643589873, 364358987...\n      [3643589826, 3643589830, 3643589824, 364358981...\n    \n    \n      amenity\n      NaN\n      place_of_worship\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      religion\n      NaN\n      hindu\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      addr:city\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      addr:postcode\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      addr:street\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ways\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      type\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      label\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n      Building Footprints\n    \n  \n\n\n\n\n\nbaner_buildings.name.fillna(value='not_known',inplace=True)\n\n\nbaner_buildings.shape\n\n(255, 14)\n\n\n\n\nVisualize the buildings on the map\n\nax = (baner_buildings.to_crs(\"EPSG:3857\")\n         .plot(figsize=(10, 12),column=\"name\",legend=True,\n               edgecolor=\"0.2\", markersize=200, cmap=\"rainbow\")\n     )\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)  # I'm using OSM as the source. See all provides with ctx.providers\nplt.axis(\"off\")\nplt.title(\"Baner\")\n\nText(0.5, 1.0, 'Baner')\n\n\n\n\n\n\nbaner_buildings = baner_buildings.to_crs(epsg=3347)\nbaner_buildings = baner_buildings.assign(area=baner_buildings.area)\n\n\nbaner_buildings= baner_buildings[['geometry','area']]\n\n\n\nPlot the buildings on the Map along with Area\n\nbaner_map = keplergl.KeplerGl(height=500)\nbaner_map.add_data(data=baner_buildings.copy(), name=\"Building area\")\n#baner_map.add_data(data=baner_buildings.copy(), name=\"height\")\n#baner_map\n\nUser Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n\n\n\nbaner_map.save_to_html(file_name='first_map.html')\n\nMap saved to first_map.html!\n\n\n\n%%html\n<iframe src=\"first_map.html\" width=\"80%\" height=\"500\"></iframe>\n\n\n\n\n\n#from IPython.display import IFrame\n#IFrame(src='first_map.html', width=700, height=600)"
  },
  {
    "objectID": "spatial_data_processing/osm_processing copy.html",
    "href": "spatial_data_processing/osm_processing copy.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "OpenStreetMap\n\nIt is an crowd-sourced dataset\nIt contains data about streets, buildings, services, landuse etc.\nOSMnx is a package used to retrieve, construct, analyze and visualize street networks from OpenStreetMap and also retrieve data about points of interest such as restaurants, schools and lots of different kind of services.\nIt is also easy to conduct network routing based on walking, cycling or driving by combining OSMnx functionalities with a package called NetworkX\n\n\nimport osmnx as ox\nimport matplotlib.pyplot as plt\n\n\n#place_name = \"Togo, Africa\"\nplace_name = {18.5786832,73.7666697}\n\n\ngraph = ox.graph_from_point(place_name,dist=750,dist_type='bbox',network_type=\"drive\")\n\nEmptyOverpassResponse: There are no data elements in the response JSON\n\n\n\ntype(graph)\n\nnetworkx.classes.multidigraph.MultiDiGraph\n\n\n\nfig, ax = ox.plot_graph(graph)\n\n\n\n\n\nnodes, edges = ox.graph_to_gdfs(graph)\n\n\nnodes.head()\n\n\n\n\n\n  \n    \n      \n      y\n      x\n      street_count\n      geometry\n    \n    \n      osmid\n      \n      \n      \n      \n    \n  \n  \n    \n      652724178\n      18.574935\n      73.763832\n      4\n      POINT (73.76383 18.57493)\n    \n    \n      652724182\n      18.574981\n      73.764610\n      3\n      POINT (73.76461 18.57498)\n    \n    \n      763423062\n      18.571967\n      73.764768\n      3\n      POINT (73.76477 18.57197)\n    \n    \n      871491336\n      18.574828\n      73.770821\n      4\n      POINT (73.77082 18.57483)\n    \n    \n      1377773005\n      18.574932\n      73.763664\n      3\n      POINT (73.76366 18.57493)\n    \n  \n\n\n\n\n\nedges.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      osmid\n      oneway\n      highway\n      reversed\n      length\n      name\n      geometry\n      access\n      lanes\n      ref\n      maxspeed\n    \n    \n      u\n      v\n      key\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      652724178\n      7984103956\n      0\n      669050753\n      False\n      primary\n      False\n      13.448\n      NaN\n      LINESTRING (73.76383 18.57493, 73.76387 18.57482)\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6262990166\n      0\n      669050753\n      False\n      primary\n      True\n      60.618\n      NaN\n      LINESTRING (73.76383 18.57493, 73.76364 18.57545)\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      6305085563\n      0\n      73533877\n      True\n      secondary\n      False\n      24.596\n      Moze College Road\n      LINESTRING (73.76383 18.57493, 73.76402 18.574...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      652724182\n      4676484316\n      0\n      73533877\n      True\n      secondary\n      False\n      69.380\n      Moze College Road\n      LINESTRING (73.76461 18.57498, 73.76493 18.575...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7983557257\n      0\n      [250171874, 223437750]\n      False\n      residential\n      [False, True]\n      306.812\n      Echinus Court Road\n      LINESTRING (73.76461 18.57498, 73.76460 18.575...\n      yes\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\narea = ox.geocode_to_gdf(place_name)\n\nValueError: each query must be a dict or a string\n\n\n\ntype(area)\n\nNameError: name 'area' is not defined\n\n\n\narea\n\n\n\n\n\n  \n    \n      \n      geometry\n      bbox_north\n      bbox_south\n      bbox_east\n      bbox_west\n      place_id\n      osm_type\n      osm_id\n      lat\n      lon\n      display_name\n      class\n      type\n      importance\n    \n  \n  \n    \n      0\n      POLYGON ((74.91434 26.83563, 74.91534 26.83465...\n      27.860562\n      26.440461\n      76.285428\n      74.914344\n      298175590\n      relation\n      1950062\n      27.150677\n      75.747016\n      Jaipur, Rajasthan, India\n      boundary\n      administrative\n      0.671968\n    \n  \n\n\n\n\n\narea.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\ntags = {\"building\":True}\n\n\nbuildings = ox.geometries_from_place(place_name,tags)\n\nTypeError: query must be dict, string, or list of strings\n\n\n\nlen(buildings)\n\n32708\n\n\n\nbuildings.head()\n\n\n\n\n\n  \n    \n      \n      \n      nodes\n      building\n      geometry\n      area\n      barrier\n      currency:INR\n      layer\n      name\n      payment:cash\n      payment:fasttag\n      ...\n      name:tg\n      name:fr\n      motor_vehicle\n      architect\n      historic:civilization\n      outdoor_seating\n      location\n      parking\n      changing_table\n      toilets:disposal\n    \n    \n      element_type\n      osmid\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      way\n      383032803\n      [3862350688, 3862350689, 3862350690, 386235069...\n      residential\n      POLYGON ((75.82021 26.78322, 75.82020 26.78289...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032804\n      [3862350692, 3862350693, 3862350694, 386235069...\n      residential\n      POLYGON ((75.82045 26.78350, 75.82042 26.78332...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032805\n      [3862350696, 3862350697, 3862350698, 386235069...\n      residential\n      POLYGON ((75.82068 26.78322, 75.82084 26.78320...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032806\n      [3862350700, 3862350701, 3862350702, 386235070...\n      residential\n      POLYGON ((75.82069 26.78321, 75.82085 26.78319...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      383032807\n      [3862350704, 3862350705, 3862350706, 386235070...\n      residential\n      POLYGON ((75.82068 26.78297, 75.82079 26.78295...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 132 columns\n\n\n\n\nbuildings.shape\n\n(32708, 132)\n\n\n\n# List key-value pairs for tags\ntags = {\"amenity\":\"restaurant\"}\n\n\n# Retrieve restaurants\nrestaurants = ox.geometries_from_place(place_name, tags)\n\n# How many restaurants do we have?\nlen(restaurants)\n\n127\n\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the footprint\narea.plot(ax=ax, facecolor=\"black\")\n\n# Plot street edges\nedges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\",alpha=0.9)\n\n# Plot buildings\nbuildings.plot(ax=ax, facecolor=\"yellow\", markersize=20)\n\n# Plot restaurants\nrestaurants.plot(ax=ax, color=\"red\", markersize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "spatial_data_processing/spatial_modelling.html",
    "href": "spatial_data_processing/spatial_modelling.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "import warnings\nimport keplergl\nimport numpy as np\nimport osmnx as ox\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nfrom skgstat import Variogram\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import Point\nfrom pykrige.ok import OrdinaryKriging\nfrom scipy.interpolate import NearestNDInterpolator\nfrom tobler.area_weighted import area_interpolate\n# Custom functions\nfrom scripts.utils import pixel2poly\n# Plotting defaults\nplt.style.use('ggplot')\npx.defaults.height = 400; px.defaults.width = 620\nplt.rcParams.update({'font.size': 16, 'axes.labelweight': 'bold', 'figure.figsize': (6, 6), 'axes.grid': False})"
  },
  {
    "objectID": "spatial_data_processing/geographic_data_formats.html",
    "href": "spatial_data_processing/geographic_data_formats.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Geographic information can be represented in two forms vector or raster\nVector representations are constructed from points in geographical space which are connected to each other forming lines and polygons\nRasters are constructed from rectangular cells that form a uniform grid. Each cell of the grid contains a value representing some information such as elevation, temperature or presence / absence\nSpatio-temporal data incorporates time as additional dimension to the geographic dimension\n\n\n\n\nGeometric objects like points, lines and polygons are used\n\n\n\n\nVector data representation\n\n\n\n\n\nAttribute data is typically attached to the geometries that describe the given entity with various possible characteristics. Attributes are always linked to the geometries in one way or another.\n\n\n\n\n\nGDAL (Geospatial Data Abstraction Library) is a library for reading and writing raster and vector data formats which is used by most of the software libraries\n\n\n\n\nIntroduced by ESRI\nFilename extension is .shp\nIt is made of multiple separate files\nA valid shapefile dataset consist of:\n\n.shp - Feature geometries\n.shx - Positional index for the feature geometries\n.dbf - Attribute information\n.prj - Information about CRS of the dataset\n\n\n\n\n\n\nOpen format for encoding variety of geographic data structures along with their attribute information\nFilename extension is .geojson\nFile is not compressed\nAn example of GeoJSON data\n\n{\"type\": \"FeatureCollection\", \n    \"features\": [\n        {\"type\": \"Feature\", \"properties\": {\"id\": 75553155, \"timestamp\": 1494181812},\n        \"geometry\": {\"type\": \"MultiLineString\", \"coordinates\": [[[26.938, 60.520], [26.938, 60.520]], [[26.937, 60.521], [26.937, 60.521]], [[26.937, 60.521], [26.936, 60.522]]]}\n        }, \n        {\"type\": \"Feature\", \"properties\": {\"id\": 424099695, \"timestamp\": 1465572910}, \n        \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[26.935, 60.521], [26.935, 60.521], [26.935, 60.521], [26.935, 60.521], [26.935, 60.521]]]}\n        }\n    ]\n}\n\n\n\n\nIt uses SQLite database container to store the data\nFilename extension is .gpkg\n\n\n\n\n\nGeography Markup Language (GML) is an XML based format\nIt serves as a modeling language for geographic systems as well as an open interchange format for geographic transactions on the Internet\nFile extension is .gml\n\n\n\n\n\n\n\nData is represented as arrays of cells (called pixels) to represent real-world objects or continuous phenomena Ex- Digital photos with RGB channels\nWe can store other information to pixels, such as elevation or temperature data or more detailed spectral information that capture how the light reflects from objects on earth at different wave-lengths\n\n\n\n\nRaster data representation\n\n\n\n\n\nRaster Bit Depth\n\n\n\n\n\nxarray Data Format\n\n\n\n\n\n\n\nBased on TIFF format developed by NASA\nFile extension is .tif\n\n\n\n\n\nCloud Optimized GeoTIFF (COG)\nFile extension is .tif\n\n\n\n\n\nNetwork Common Data Form\nVariables stored in NetCDF are often measured multiple times per day over large (e.g. continental) areas\nThe file extension of NetCDF is .nc4\n\n\n\n\n\nUsed to transfer Raster files between applications\nThe file extension of ASCII Raster File is .asc\n\n\n\n\n\nThe ERDAS Imagine file format (IMG) is proprietary file format that was originally created by an image processing software company called ERDAS. The file can be accompanied with an .xml file which stores metadata information about the raster layer\nThe file extension of Imagine file format is .img\n\n\n\n\n\n\n\nNetworkx is used to store graph objects\npysal rely on sparse adjacency matrix"
  },
  {
    "objectID": "spatial_data_processing/osm_processing.html",
    "href": "spatial_data_processing/osm_processing.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "OpenStreetMap\n\nIt is an crowd-sourced dataset\nIt contains data about streets, buildings, services, landuse etc.\nOSMnx is a package used to retrieve, construct, analyze and visualize street networks from OpenStreetMap and also retrieve data about points of interest such as restaurants, schools and lots of different kind of services.\nIt is also easy to conduct network routing based on walking, cycling or driving by combining OSMnx functionalities with a package called NetworkX\n\n\nGet Street Network Graph for Tirupathi\n\nimport osmnx as ox\nimport matplotlib.pyplot as plt\n\n\nplace_name = \"Tirupathi, Andhra Pradesh, India\"\n\n\ngraph = ox.graph_from_place(place_name)\n\n\ntype(graph)\n\nnetworkx.classes.multidigraph.MultiDiGraph\n\n\n\nfig, ax = ox.plot_graph(graph)\n\n\n\n\n\nnodes, edges = ox.graph_to_gdfs(graph)\n\n\nnodes.head()\n\n\n\n\n\n  \n    \n      \n      y\n      x\n      street_count\n      geometry\n    \n    \n      osmid\n      \n      \n      \n      \n    \n  \n  \n    \n      3726004217\n      13.626082\n      79.391887\n      3\n      POINT (79.39189 13.62608)\n    \n    \n      3726082024\n      13.624080\n      79.381771\n      3\n      POINT (79.38177 13.62408)\n    \n    \n      3726082625\n      13.624315\n      79.383015\n      3\n      POINT (79.38302 13.62431)\n    \n    \n      3726082626\n      13.624330\n      79.383098\n      3\n      POINT (79.38310 13.62433)\n    \n    \n      3726082627\n      13.624499\n      79.393360\n      3\n      POINT (79.39336 13.62450)\n    \n  \n\n\n\n\n\nedges.head()\n\n\n\n\n\n  \n    \n      \n      \n      \n      osmid\n      highway\n      oneway\n      reversed\n      length\n      geometry\n      tunnel\n      bridge\n    \n    \n      u\n      v\n      key\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      3726004217\n      3727759169\n      0\n      368755785\n      service\n      False\n      True\n      102.448\n      LINESTRING (79.39189 13.62608, 79.39096 13.62589)\n      NaN\n      NaN\n    \n    \n      3726082692\n      0\n      368765668\n      service\n      True\n      False\n      11.398\n      LINESTRING (79.39189 13.62608, 79.39189 13.626...\n      NaN\n      NaN\n    \n    \n      3726082024\n      3726082625\n      0\n      368755785\n      service\n      False\n      False\n      136.948\n      LINESTRING (79.38177 13.62408, 79.38302 13.62431)\n      NaN\n      NaN\n    \n    \n      3726082653\n      0\n      368765683\n      service\n      False\n      False\n      247.024\n      LINESTRING (79.38177 13.62408, 79.38146 13.625...\n      NaN\n      NaN\n    \n    \n      3726082625\n      3726082626\n      0\n      368755785\n      service\n      False\n      False\n      9.070\n      LINESTRING (79.38302 13.62431, 79.38310 13.62433)\n      NaN\n      NaN\n    \n  \n\n\n\n\n\narea = ox.geocode_to_gdf(place_name)\n\n\ntype(area)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\narea\n\n\n\n\n\n  \n    \n      \n      geometry\n      bbox_north\n      bbox_south\n      bbox_east\n      bbox_west\n      place_id\n      osm_type\n      osm_id\n      lat\n      lon\n      display_name\n      class\n      type\n      importance\n    \n  \n  \n    \n      0\n      POLYGON ((79.37901 13.62928, 79.38167 13.62409...\n      13.634066\n      13.623569\n      79.39373\n      79.379014\n      191306545\n      way\n      369041142\n      13.626914\n      79.386643\n      Sri Venkateshwara Veterinary University, Tirup...\n      amenity\n      university\n      0.718072\n    \n  \n\n\n\n\n\narea.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\n\nGet Building information\n\ntags = {\"building\":True}\n\n\nbuildings = ox.geometries_from_place(place_name,tags)\n\n\nlen(buildings)\n\n103\n\n\n\nbuildings.head()\n\n\n\n\n\n  \n    \n      \n      \n      nodes\n      building\n      geometry\n      layer\n      name\n      ways\n      type\n    \n    \n      element_type\n      osmid\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      way\n      368754138\n      [3725992120, 3725992118, 3725992325, 372599232...\n      yes\n      POLYGON ((79.38556 13.62655, 79.38560 13.62650...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      368765639\n      [3727711369, 3727711372, 3727711373, 372771137...\n      yes\n      POLYGON ((79.38762 13.62865, 79.38763 13.62865...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      368765640\n      [3726082631, 3726082641, 3726082638, 372608263...\n      yes\n      POLYGON ((79.38655 13.62537, 79.38662 13.62548...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      368765641\n      [3726082662, 3726082681, 3726082697, 372608269...\n      yes\n      POLYGON ((79.38255 13.62598, 79.38244 13.62610...\n      1\n      Admin Office (Dr. Y.S.R. Bhavan)\n      NaN\n      NaN\n    \n    \n      368765644\n      [3726083018, 3726083020, 3726082991, 372608298...\n      yes\n      POLYGON ((79.38190 13.62782, 79.38196 13.62783...\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nbuildings.shape\n\n(103, 7)\n\n\n\n# List key-value pairs for tags\ntags = {\"railway\":True}\n\n\n# Retrieve restaurants\nrailway = ox.geometries_from_place(place_name, tags)\n\n# How many restaurants do we have?\nlen(railway)\n\n1\n\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the footprint\narea.plot(ax=ax, facecolor=\"black\")\n\n# Plot street edges\nedges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\")\n\n# Plot buildings\nbuildings.plot(ax=ax, facecolor=\"silver\", alpha=0.7)\n\n# Plot restaurants\nrailway.plot(ax=ax, color=\"red\", alpha=0.7, markersize=20)\nplt.tight_layout()\n\n\n\n\n\n\nGet Park Information\n\ntags = {\"leisure\": \"park\", \"landuse\": \"grass\"}\n\n\nparks = ox.geometries_from_place(place_name, tags)\nprint(\"Retrieved\", len(parks), \"objects\")\n\nRetrieved 5 objects\n\n\n\nparks.head(3)\n\n\n\n\n\n  \n    \n      \n      \n      nodes\n      landuse\n      geometry\n    \n    \n      element_type\n      osmid\n      \n      \n      \n    \n  \n  \n    \n      way\n      368765686\n      [3726082943, 3726082942, 3726082954, 372608295...\n      grass\n      POLYGON ((79.38144 13.62756, 79.38141 13.62755...\n    \n    \n      368765687\n      [3726082995, 3726082981, 3726082971, 372608299...\n      grass\n      POLYGON ((79.38149 13.62773, 79.38150 13.62770...\n    \n    \n      368765688\n      [3726083023, 3726083010, 3726082983, 372608300...\n      grass\n      POLYGON ((79.38150 13.62785, 79.38152 13.62778...\n    \n  \n\n\n\n\n\nparks.plot(color='green')\n\n<AxesSubplot: >\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the footprint\narea.plot(ax=ax, facecolor=\"black\")\n\n# Plot the parks\nparks.plot(ax=ax, facecolor=\"green\")\n\n# Plot street edges\nedges.plot(ax=ax, linewidth=1, edgecolor=\"dimgray\")\n\n# Plot buildings\nbuildings.plot(ax=ax, facecolor=\"silver\", alpha=0.7)\n\n# Plot restaurants\nrailway.plot(ax=ax, color=\"red\", alpha=0.7, markersize=20)\nplt.tight_layout()"
  },
  {
    "objectID": "spatial_data_processing/spatial_analysis.html",
    "href": "spatial_data_processing/spatial_analysis.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "import geopandas as gpd\nfrom pathlib import Path\n\n\ninput_path = '/home/thulasiram/personal/going_deep_and_wide/togo/togo-targeting-replication/data/shapefiles/cantons.geojson'\ndata = gpd.read_file(input_path)\n\n\ntype(data)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      canton\n      poverty\n      geometry\n    \n  \n  \n    \n      0\n      1\n      3.738084\n      MULTIPOLYGON (((0.75228 6.83786, 0.75137 6.840...\n    \n    \n      1\n      2\n      7.096286\n      MULTIPOLYGON (((0.69026 6.80602, 0.69627 6.806...\n    \n    \n      2\n      3\n      0.824586\n      MULTIPOLYGON (((0.63102 6.74430, 0.63295 6.747...\n    \n    \n      3\n      4\n      3.983729\n      MULTIPOLYGON (((0.67259 6.85123, 0.67714 6.849...\n    \n    \n      4\n      5\n      7.708810\n      MULTIPOLYGON (((0.75269 6.84116, 0.75137 6.840...\n    \n  \n\n\n\n\n\nprint(\"Number of rows\",len(data[\"canton\"]))\nprint(\"Number of classes\",data[\"canton\"].nunique())\n\nNumber of rows 387\nNumber of classes 387\n\n\n\n\n\n\ndata.plot()\n\n<AxesSubplot: >\n\n\n\n\n\nChecking the shape and area of the first Multipolygon in the data\n\ndata.at[0,\"geometry\"]\n\n\n\n\n\n# Calculating area with lat and long is wrong, to calculate \n# area correctly we need to change the co-ordinate reference system\nround(data.at[0,\"geometry\"].area,3)\n\n0.014\n\n\n\n\n\n\ndata.plot(\"poverty\",legend=True)\n\n<AxesSubplot: >\n\n\n\n\n\n\ndata.explore(\"poverty\",legend=True)\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n\nGeocoding is the process of transforming place names or addresses into coordinates\n\n# Import necessary modules\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n\n# Filepath\nfp = '../data/addresses.txt'\n\n# Read the data\ndata = pd.read_csv(fp,sep=\";\")\n\n\nlen(data)\n\n5\n\n\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      id\n      addr\n    \n  \n  \n    \n      0\n      1000\n      Boat Club Road, 411001, Pune, Maharastra\n    \n    \n      1\n      1001\n      Koregaon, 415501, Pune, Maharastra\n    \n    \n      2\n      1002\n      Kothrud, 411038, Pune, Maharastra\n    \n    \n      3\n      1003\n      Balewadi, 411045, Pune, Maharastra\n    \n    \n      4\n      1004\n      Baner, 411047, Pune, Maharastra\n    \n  \n\n\n\n\n\n# Import the geocoding tool\nfrom geopandas.tools import geocode\n\n# Geocode addresses using Nominatim. Remember to provide a custom \"application name\" in the user_agent parameter!\ngeo = geocode(data[\"addr\"], provider=\"nominatim\", user_agent=\"autogis_xx\", timeout=4)\n\n\ngeo.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      address\n    \n  \n  \n    \n      0\n      POINT (73.87826 18.53937)\n      Boat Club Road, Pune City, Pune, Maharashtra, ...\n    \n    \n      1\n      POINT (73.89299 18.53772)\n      Koregaon Park, Suyojan Society, Ghorpuri, Pune...\n    \n    \n      2\n      POINT (73.80767 18.50389)\n      Kothrud, Pune City, Maharashtra, 411038, India\n    \n    \n      3\n      POINT (73.76912 18.57767)\n      Prakashgad Society, Balewadi, Perfect 10 Inter...\n    \n    \n      4\n      POINT (73.77686 18.56424)\n      Baner, Pune City, Maharashtra, 511045, India\n    \n  \n\n\n\n\n\n# Joining the original dataframe with geoencoded dataframe\njoin = geo.join(data)\njoin = join.drop(columns=['address'])\n\n\njoin.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      id\n      addr\n    \n  \n  \n    \n      0\n      POINT (73.87826 18.53937)\n      1000\n      Boat Club Road, 411001, Pune, Maharastra\n    \n    \n      1\n      POINT (73.89299 18.53772)\n      1001\n      Koregaon, 415501, Pune, Maharastra\n    \n    \n      2\n      POINT (73.80767 18.50389)\n      1002\n      Kothrud, 411038, Pune, Maharastra\n    \n    \n      3\n      POINT (73.76912 18.57767)\n      1003\n      Balewadi, 411045, Pune, Maharastra\n    \n    \n      4\n      POINT (73.77686 18.56424)\n      1004\n      Baner, 411047, Pune, Maharastra\n    \n  \n\n\n\n\n\n\n\n\nfrom shapely.geometry import box\n\nminx = 73.76\nminy = 18.537\nmaxx = 73.89\nmaxy = 18.56\ngeom = box(minx, miny, maxx, maxy)\nclipping_gdf = gpd.GeoDataFrame({\"geometry\": [geom]}, index=[0], crs=\"epsg:4326\")\n\n# Explore the extent on a map\nclipping_gdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Write the data to shape file\noutfp = r\"../data/addresses.shp\"\njoin.to_file(outfp)"
  },
  {
    "objectID": "spatial_data_processing/crs.html",
    "href": "spatial_data_processing/crs.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Coordinate Reference Systems (CRS)\nA CRS tells python how coordinates are related to places on the Earth. A map projection (or a projected coordinate system) is a systematic transformation of the latitudes and longitudes into a plain surface where units are quite commonly represented as meters (instead of decimal degrees). This transformation is used to represent the three dimensional earth on a flat, two dimensional map.\nThere is no perfect projection and we should know the strength and weaknesses of projection systems and choose a projection system that best fits our purpose.\nWe can reproject the geometries from crs to another using to_crs() function from GeoPandas.\nWe can define the coordinate system in different formats using pyproj CRS\n\nImport and view the data\n\nimport geopandas as gpd\n\n\n# Read the data\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n\nworld.head(4)\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      0\n      889953.0\n      Oceania\n      Fiji\n      FJI\n      5496\n      MULTIPOLYGON (((180.00000 -16.06713, 180.00000...\n    \n    \n      1\n      58005463.0\n      Africa\n      Tanzania\n      TZA\n      63177\n      POLYGON ((33.90371 -0.95000, 34.07262 -1.05982...\n    \n    \n      2\n      603253.0\n      Africa\n      W. Sahara\n      ESH\n      907\n      POLYGON ((-8.66559 27.65643, -8.66512 27.58948...\n    \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n  \n\n\n\n\n\n\nView the CRS of the data\n\n# Check the CRS of the data.\n# Lat Long data should have EPSG 4326 and WGS 84\nworld.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n\nChange the CRS and visualize the data\n\nax = world.plot()\nax.set_title(\"WGS84 (lat/lon)\")\nworld = world[(world.name != \"Antarctica\") & (world.name != \"Fr. S. Antarctic Lands\")]\n# Data in Mercator Projection\nworld = world.to_crs(\"EPSG:3395\")\nax = world.plot()\nax.set_title(\"Mercator\")\n\nText(0.5, 1.0, 'Mercator')\n\n\n\n\n\n\n\n\n\n\nOrthographic Projection\n\n# Orthographic projection\nfrom pyproj import CRS\n\n\n# Define an orthographic projection, from: http://www.statsmapsnpix.com/2019/09/globe-projections-and-insets-in-qgis.html\northo = CRS.from_proj4(\n    \"+proj=ortho +lat_0=60.00 +lon_0=23.0000 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\"\n)\n\n# Re-project and plot\nax = world.to_crs(ortho).plot()\n\n# Remove x and y axis\nax.axis(\"off\")\nax.set_title(\"Orthographic\")\n\nText(0.5, 1.0, 'Orthographic')"
  },
  {
    "objectID": "spatial_data_processing/gee_timelapse.html",
    "href": "spatial_data_processing/gee_timelapse.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Steps to create a Landsat timelapse:\n\nPan and zoom to your area of interest, or click the globe icon at the upper left corner to search for a location.\nUse the drawing tool to draw a rectangle anywhere on the map.\nAdjust the parameters (e.g., start year, end year, title) if needed.\nClick the Create timelapse button to create a timelapse.\nOnce the timelapse has been added to the map, click the hyperlink at the end if you want to download the GIF.\n\n\nimport os\nimport ee\nimport geemap\nimport ipywidgets as widgets\n\n\nMap = geemap.Map()\nMap.add_basemap('HYBRID')\nMap\n\n\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 10.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 1984.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 2020.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 5.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 10.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 30.\n\n\nTraitError: The 'value' trait of a Label instance expected a unicode string, not the int 0.\n\n\n\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-0e20ead6bdef0b1d973c2259323cb508:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map.\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-0ed6f7095071ae8dea998b8fcb895851:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map.\n\n\nGenerating URL...\nDownloading GIF image from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/videoThumbnails/8557d6ed3c0cdb0ffa673666569b4260-be14a4d9b69711ca1550db81d8fda1a8:getPixels\nPlease wait ...\nAn error occurred while downloading.\nUser memory limit exceeded.\nThe input gif file does not exist.\nThe input gif file does not exist.\nAdding GIF to the map ...\nThe provided file does not exist.\nThe timelapse has been added to the map."
  },
  {
    "objectID": "spatial_data_processing/raster_data_processing.html",
    "href": "spatial_data_processing/raster_data_processing.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Converting Data from Raster to Tabular (Geometry) format\n\nImport the libraries\n\nimport pandas\nimport osmnx\nimport geopandas \nimport rioxarray\nimport xarray\nimport datashader as ds\nimport contextily as cx\nfrom shapely import geometry\nimport matplotlib.pyplot as plt\nimport folium\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\n\nDownload Geopackage\n\n# URL for the geopackage\nurl = (\"https://jeodpp.jrc.ec.europa.eu/ftp/\"\\\n       \"jrc-opendata/GHSL/\"\\\n       \"GHS_FUA_UCDB2015_GLOBE_R2019A/V1-0/\"\\\n       \"GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.zip\"\n      )\nurl\n\n'https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL/GHS_FUA_UCDB2015_GLOBE_R2019A/V1-0/GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.zip'\n\n\n\n\nVisualize the map\n\n# Visualize the Map for Sao Paulo\np = f\"zip+{url}!GHS_FUA_UCDB2015_GLOBE_R2019A_54009_1K_V1_0.gpkg\"\nfuas = geopandas.read_file(p)\nsao_paulo = fuas.query(\"eFUA_name == 'São Paulo'\").to_crs(\"EPSG:4326\")\n\n\nax = sao_paulo.plot(alpha=0.5, figsize=(9, 9))\ncx.add_basemap(ax, crs=sao_paulo.crs);\n\n\n\n\n\n\nDownload the population data\n\nurl = (\"https://cidportal.jrc.ec.europa.eu/ftp/\"\\\n       \"jrc-opendata/GHSL/GHS_POP_MT_GLOBE_R2019A/\"\\\n       \"GHS_POP_E2015_GLOBE_R2019A_54009_250/V1-0/\"\\\n       \"tiles/\"\\\n       \"GHS_POP_E2015_GLOBE_R2019A_54009_250_V1_0_13_11.zip\"\n      )\nurl\n\n'https://cidportal.jrc.ec.europa.eu/ftp/jrc-opendata/GHSL/GHS_POP_MT_GLOBE_R2019A/GHS_POP_E2015_GLOBE_R2019A_54009_250/V1-0/tiles/GHS_POP_E2015_GLOBE_R2019A_54009_250_V1_0_13_11.zip'\n\n\n\n# Population data in raster format\n%%time\np = f\"zip+{url}!GHS_POP_E2015_GLOBE_R2019A_54009_250_V1_0_13_11.tif\"\nghsl = rioxarray.open_rasterio(p)\nghsl\n\nCPU times: user 35.6 ms, sys: 4.12 ms, total: 39.8 ms\nWall time: 7.12 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 4000, x: 4000)>\n[16000000 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -5.041e+06 -5.041e+06 ... -4.041e+06 -4.041e+06\n  * y            (y) float64 -2e+06 -2e+06 -2.001e+06 ... -3e+06 -3e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    _FillValue:     -200.0\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 4000x: 4000...[16000000 values with dtype=float32]Coordinates: (4)band(band)int641array([1])x(x)float64-5.041e+06 ... -4.041e+06array([-5040875., -5040625., -5040375., ..., -4041625., -4041375., -4041125.])y(y)float64-2e+06 -2e+06 ... -3e+06 -3e+06array([-2000125., -2000375., -2000625., ..., -2999375., -2999625., -2999875.])spatial_ref()int640crs_wkt :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]spatial_ref :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :-5041000.0 250.0 0.0 -2000000.0 0.0 -250.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([-5040875.0, -5040625.0, -5040375.0, -5040125.0, -5039875.0,\n              -5039625.0, -5039375.0, -5039125.0, -5038875.0, -5038625.0,\n              ...\n              -4043375.0, -4043125.0, -4042875.0, -4042625.0, -4042375.0,\n              -4042125.0, -4041875.0, -4041625.0, -4041375.0, -4041125.0],\n             dtype='float64', name='x', length=4000))yPandasIndexPandasIndex(Float64Index([-2000125.0, -2000375.0, -2000625.0, -2000875.0, -2001125.0,\n              -2001375.0, -2001625.0, -2001875.0, -2002125.0, -2002375.0,\n              ...\n              -2997625.0, -2997875.0, -2998125.0, -2998375.0, -2998625.0,\n              -2998875.0, -2999125.0, -2999375.0, -2999625.0, -2999875.0],\n             dtype='float64', name='y', length=4000))Attributes: (4)AREA_OR_POINT :Area_FillValue :-200.0scale_factor :1.0add_offset :0.0\n\n\n\n\nVisualize the population on raster data\n\ncvs = ds.Canvas(plot_width=600, plot_height=600)\nagg = cvs.raster(ghsl.where(ghsl>0).sel(band=1))\n\n\nf, ax = plt.subplots(1, figsize=(9, 7))\nagg.plot.imshow(ax=ax, alpha=0.5, cmap=\"cividis_r\")\ncx.add_basemap(\n    ax, \n    crs=ghsl.rio.crs, \n    zorder=-1, \n    source=cx.providers.CartoDB.Voyager\n)\n\n\n\n\n\n# Clip the data for Sao Paulo\nghsl_sp = ghsl.rio.clip(sao_paulo.to_crs(ghsl.rio.crs).geometry.iloc[0])\nghsl_sp\n\n/home/thulasiram/miniconda3/envs/geopy/lib/python3.9/site-packages/rasterio/features.py:290: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n  for index, item in enumerate(shapes):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 416, x: 468)>\narray([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -4.482e+06 -4.482e+06 ... -4.365e+06 -4.365e+06\n  * y            (y) float64 -2.822e+06 -2.822e+06 ... -2.926e+06 -2.926e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0\n    _FillValue:     -200.0xarray.DataArrayband: 1y: 416x: 468-200.0 -200.0 -200.0 -200.0 -200.0 ... -200.0 -200.0 -200.0 -200.0array([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)Coordinates: (4)band(band)int641array([1])x(x)float64-4.482e+06 ... -4.365e+06axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([-4481875., -4481625., -4481375., ..., -4365625., -4365375., -4365125.])y(y)float64-2.822e+06 ... -2.926e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([-2822125., -2822375., -2822625., ..., -2925375., -2925625., -2925875.])spatial_ref()int640crs_wkt :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]spatial_ref :PROJCS[\"World_Mollweide\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Mollweide\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :-4482000.0 250.0 0.0 -2822000.0 0.0 -250.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([-4481875.0, -4481625.0, -4481375.0, -4481125.0, -4480875.0,\n              -4480625.0, -4480375.0, -4480125.0, -4479875.0, -4479625.0,\n              ...\n              -4367375.0, -4367125.0, -4366875.0, -4366625.0, -4366375.0,\n              -4366125.0, -4365875.0, -4365625.0, -4365375.0, -4365125.0],\n             dtype='float64', name='x', length=468))yPandasIndexPandasIndex(Float64Index([-2822125.0, -2822375.0, -2822625.0, -2822875.0, -2823125.0,\n              -2823375.0, -2823625.0, -2823875.0, -2824125.0, -2824375.0,\n              ...\n              -2923625.0, -2923875.0, -2924125.0, -2924375.0, -2924625.0,\n              -2924875.0, -2925125.0, -2925375.0, -2925625.0, -2925875.0],\n             dtype='float64', name='y', length=416))Attributes: (4)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0_FillValue :-200.0\n\n\nout_p = “../data/ghsl_sao_paulo.tif” ! rm $out_p ghsl_sp.rio.to_raster(out_p)\n\n\nConvert Raster to geometry\n\n# Read the raster data\nsurface = xarray.open_rasterio(\"../data/ghsl_sao_paulo.tif\")\n\n\n# Convert raster to geometry\nt_surface = surface.to_series()\n\n\nt_surface.head()\n\nband  y           x         \n1     -2822125.0  -4481875.0   -200.0\n                  -4481625.0   -200.0\n                  -4481375.0   -200.0\n                  -4481125.0   -200.0\n                  -4480875.0   -200.0\ndtype: float32\n\n\n\nt_surface = t_surface.reset_index().rename(columns={0: \"Value\"})\n\n\nt_surface.query(\"Value > 1000\").info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 7734 entries, 3785 to 181296\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   band    7734 non-null   int64  \n 1   y       7734 non-null   float64\n 2   x       7734 non-null   float64\n 3   Value   7734 non-null   float32\ndtypes: float32(1), float64(2), int64(1)\nmemory usage: 271.9 KB\n\n\n\ntype(t_surface)\n\npandas.core.frame.DataFrame\n\n\n\n# Calculate the polygon based on resolution values\ndef row2cell(row, res_xy):\n    res_x, res_y = res_xy  # Extract resolution for each dimension\n    # XY Coordinates are centered on the pixel\n    minX = row[\"x\"] - (res_x / 2)\n    maxX = row[\"x\"] + (res_x / 2)\n    minY = row[\"y\"] + (res_y / 2)\n    maxY = row[\"y\"] - (res_y / 2)\n    poly = geometry.box(\n        minX, minY, maxX, maxY\n    )  # Build squared polygon\n    return poly\n\n\n# Get the polygons\nmax_polys = (\n    t_surface.query(\n        \"Value > 1000\"\n    )  # Keep only cells with more than 1k people\n    .apply(  # Build polygons for selected cells\n        row2cell, res_xy=surface.attrs[\"res\"], axis=1\n    )\n    .pipe(  # Pipe result from apply to convert into a GeoSeries\n        geopandas.GeoSeries, crs=surface.attrs[\"crs\"]\n    )\n)\n\n\n# Plot polygons on the map\nax = max_polys.plot(edgecolor=\"red\", figsize=(9, 9))\n# Add basemap\ncx.add_basemap(\n    ax, crs=surface.attrs[\"crs\"], source=cx.providers.CartoDB.Voyager\n)\n\n\n\n\n\n\nConvert Geometry to Raster\n\nnew_da = xarray.DataArray.from_series(\n    t_surface.set_index([\"band\", \"y\", \"x\"])[\"Value\"]\n)\nnew_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'Value' (band: 1, y: 416, x: 468)>\narray([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)\nCoordinates:\n  * band     (band) int64 1\n  * y        (y) float64 -2.926e+06 -2.926e+06 ... -2.822e+06 -2.822e+06\n  * x        (x) float64 -4.482e+06 -4.482e+06 ... -4.365e+06 -4.365e+06xarray.DataArray'Value'band: 1y: 416x: 468-200.0 -200.0 -200.0 -200.0 -200.0 ... -200.0 -200.0 -200.0 -200.0array([[[-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        ...,\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.],\n        [-200., -200., -200., ..., -200., -200., -200.]]], dtype=float32)Coordinates: (3)band(band)int641array([1])y(y)float64-2.926e+06 ... -2.822e+06array([-2925875., -2925625., -2925375., ..., -2822625., -2822375., -2822125.])x(x)float64-4.482e+06 ... -4.365e+06array([-4481875., -4481625., -4481375., ..., -4365625., -4365375., -4365125.])Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))yPandasIndexPandasIndex(Float64Index([-2925875.0, -2925625.0, -2925375.0, -2925125.0, -2924875.0,\n              -2924625.0, -2924375.0, -2924125.0, -2923875.0, -2923625.0,\n              ...\n              -2824375.0, -2824125.0, -2823875.0, -2823625.0, -2823375.0,\n              -2823125.0, -2822875.0, -2822625.0, -2822375.0, -2822125.0],\n             dtype='float64', name='y', length=416))xPandasIndexPandasIndex(Float64Index([-4481875.0, -4481625.0, -4481375.0, -4481125.0, -4480875.0,\n              -4480625.0, -4480375.0, -4480125.0, -4479875.0, -4479625.0,\n              ...\n              -4367375.0, -4367125.0, -4366875.0, -4366625.0, -4366375.0,\n              -4366125.0, -4365875.0, -4365625.0, -4365375.0, -4365125.0],\n             dtype='float64', name='x', length=468))Attributes: (0)"
  },
  {
    "objectID": "research_methods/literature_review.html",
    "href": "research_methods/literature_review.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Use Nighttime light values (NLV) as a proxy for poverty\nDataset provided by Earth Observation Group and Images from Google Static Map at 2.5 Meters of resolution\nUsing nightlights as a proxy for development. Nightlights data cannot be directly used because there is less difference in luminosity between rich and poor regions in Africa\nUse Day images which capture more information and use night images as label data.\nUse transfer learning to learn features from day satellite images and NLV labels\nTrain the model and learn important features. These features are calculated for a new image.\nUse these learned features along with survey data at cluster level to train the model.\nAs the cluster level data is very less, use simple models\nUse the trained model for classifying new clusters or areas\n\n\n\n\nUsing CNN on Nightlight Images to learn features\n\n\n\n\n\n\n\n\n\nPublicly available satellite-based estimates of poverty are available\nThe estimation methods use deep learning models trained on Demographic and Health Surveys (DHS) data from neighbouring countries to estimate the average relative wealth of each 2.4km tile in Togo\nThis is used to do cluster level predictions\nIdentify the clusters\nUse phone CDR as independent variables and survey data as dependent variable to build models for each household\n\n\n\n\nUsing Phone CDR and Survey data for prediction\n\n\n\n\n\n\n\n\n\nLand use and the manufactured objects observed in a satellite image emphasize the wealthiness of an area\nCNN was trained on a land use detection and classification task\nThey used xView data consisting of very high resolution images annotated with bounding boxes defined over 10 main classes (building, fixed-wing aircraft, passenger vehicle, truck, railway vehicle, maritime vessel, engineering vehicle, helipad, vehicle lot, construction site) and 60 sub-classes.\nYolo V3 was used for object detection\n\n\n\n\n\n\n\n\nBased on Unsupervised learning\nThis method emphasizes the difference between two satellite images\nCluster homogeneous-looking areas and assume that some clusters will be specific to poor areas\n\n\n\n\nFields are often surronded by other fields\n\n\n\n\n\nContrastive learning between Anchor Neighbor and Distant tiles\n\n\n\n\n\n\n\n\n\ncomparison of different approaches"
  },
  {
    "objectID": "research_methods/nlp_wiki_wealth_pred.html",
    "href": "research_methods/nlp_wiki_wealth_pred.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Estimate socioeconomic indicators using open-source, geolocated textual information from wikipedia articles\nNLP techniques are used to predict community level asset wealth and education outcomes using nearby geolocated Wikipedia articles\nMany wikipedia articles are geolocated. Many developing regions of the world contain high concentrations of geolocated articles. These articles contain a rich textual information about locations and entities in an area\n\n\n\n\nAn Example of a geolocated wikipedia article\n\n\n\n\n\nGeolocated articles are mapped to a vector representation using Doc2vec method\nUse spatial distribution of the embeddings to predict socioeconomic indicators of poverty, as measured by ground-truth survey data collected by the world bank.\nThe model is further extended to include information about nightime light intensity as measured by satellites\nThis method is able to provide reliable predictions\n\n\n\n\n\nAsset ownership from DHS\nCorpus of geolocated wikipedia articles. For Africa there were roughly 50,000 such articles.\nNightlights Imagery from VIIRS\n\n\n\n\n\nWikipedia articles consist of a lot of bias in terms of information present, length of articles etc\nDoc2vec model is used to train the embeddings from the documents\n\n\n\n\nDoc2vec Model\n\n\n\n\n\nMulti-Modal architecture with Images and Text\n\n\n\n\n\n\nWikipedia embedding model outperformed the Nightlight-only model (train and tested within the same country)\nWikipedia embedding contributes positively towards the predictions\nMulti-modal model performs best in all the different situations\nResults suggest that wikipedia embeddings and nightlight images provide highly complementary information about poverty\n\n\n\n\nResearch Article"
  },
  {
    "objectID": "research_methods/Tile2Vec.html",
    "href": "research_methods/Tile2Vec.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Learning a lower-dimensional representation of the data that can be used for any number of downstream ML tasks\nLandscapes in remote sensing datasets are highly spatially correlated. The idea is to extract enough learning signal to reliably train deep neural networks\n\n\n\n\nThe distributional hypothesis in linguistics is the idea that a word is characterized by the company it keeps. words that appear in similar contexts should have similar meanings.\nIn natural language processing (NLP), this assumption that meaning can be derived from context is leveraged to learn continuous word vector representations like Word2vec and GloVe\n\n\n\n\n\nEverything is related to everything else, but near things are more related than distant things.\n\n\n\n\n\nTo extend the Word2vec analogy from NLP, an image tiles is used similar to be our “words” and spatial neighborhoods (defined by some radius) to define the context. The image tiles that are geographic neighbors (i.e. close spatially) should have similar semantics and therefore representations, while tiles far apart are likely to have dissimilar semantics and should therefore have dissimilar representations.\nTo learn a mapping from image tiles to low-dimensional embeddings, train a CNN on triplets of tiles, where each triplet consists of an anchor tile, neighbor tile and a distant tile.\nTiles from the same neighborhood are more likely to be similar than their more distant counterparts.\nA CNN is trained to minimize the distance between the anchor and neighbor embeddings, while maximizing the distance between the anchor and distant embeddings.\n\n\n\n\nContrastive Spatial Analysis\n\n\n\n\n\nA CNN is used to learn the embeddings\n\n\n\n\n\n\n\nTile2Vec CNN is a ResNet-18 architecture without the classification layer\n\n\n\n\n\nFor each Uganda cluster, the team extracted a median composite through Google Earth Engine of roughly 75 × 75 pixels (5 km2) centered at its location. They randomly sample 10 tiles from this patch and average their Tile2Vec embeddings. These embeddings are then input to a ridge regression to predict log consumption expenditures.\n\n\n\nBlog post on Tile2Vec Research Paper Github Repo"
  },
  {
    "objectID": "research_methods/new_directions.html",
    "href": "research_methods/new_directions.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Use Internet speeds for a location to estimate the development in the area (Ookla provides open source datasets for the same)\n\n\n\n\n\nFor a given location / area using Openstreet Map data it is possible to derive the following information, which can be used to estimate wealth\n\nNo. of buildings\nNo. of roads\nNo. of primary roads\nNo. of trunk roads\ncount of market places\nNumber of charging stations\nNo. of post offices\nSupermarket counts\nCar repair counts\nDepartment stores\nComputers\nPlaygrounds\nMonuments\n\n\n\n\n\n\n\n\n\nUsing explainable models - How much a variable is influencing the predictions\nExplainability along with good prediction accuracy. Example - Explainable boosting machines (EBMs)\nWe can build editable models using EBMs\n\n\n\n\n\nUse libraries like snorkel to generate labels\nLow confidence images can be routed to humans for labeling\n\n\n\n\n\nThere is correlation between wealth and spatial location\nA Gaussian process can be implemented on top of the model. Use the prior observations to get the posterior distribution from the priors (tried at stanford in 2016)\n\n\n\n\n\nUsing time series analysis to forecast the wealth from past surveys"
  },
  {
    "objectID": "research_methods/image_detection.html",
    "href": "research_methods/image_detection.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Poverty prediction using night lights and other methods are not interpretable\nThis method uses object detection on high resolution satellite images\nUse weighted counts of objects as features for predicting local-level consumption\nA satellite imagery object detector was trained on a publicly available, global scale object detection dataset, called xView, which avoids location specific training and provides a more general object detection model. The model is then applied to high resolution images taken over hundreds of villages across Uganda that were measured in an existing georeferenced household survey, and use extracted counts of detected objects as features in a final prediction of consumption expenditure.\nA linear regression model is used to predict consumption\n\n\n\n\nMethodology\n\n\n\n\n\n\nSurvey data from Living Standards Measurement Study (LSMS) for Uganda.\nUganda satellite imagery - High resolution images from DigitalGlobe satellites with three bands (RGB) and 30cm pixel resolution.\nAs object annotations were not available for Uganda, transfer learning was done by training an object detector on a different but related source dataset (xView dataset)\n\n\n\n\n\nYOLOv3 with single stage object detector with a DarkNet53 backbone architecture\nMean average precision and recall per class was used for evaluation\nxView dataset consist of parent and child classes. Two object detectors were trained using parent and child level classes\nFour types of features which was explored for this research paper\n\nCount of objects\nConfidence x counts - objects detected in Uganda was weighted by the confidence score\nEach detected object is weighted by its bounding box area\n(Confidence, size) X counts\n\nGiven the cluster level categorical feature vector, the poverty index is estimated with a regression model\nCount of objects was performing better than other methods\n\n\n\n\nFeature importance of the final model\n\n\nReference\n\nResearch Paper"
  },
  {
    "objectID": "research_methods/ph_social_media.html",
    "href": "research_methods/ph_social_media.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Satellite images are costly to acquire and training a deep learning model requires costly GPU resources\nDeep learning models do not provide interpretability\nIn this study these challenges are overcome by combining social media and geospatial data sources with cost efficient ML methods as an interpretable and inexpensive approach to poverty estimation\n\n\n\n\nGround truth data - DHS data for Philippines\nGeospatial covariates\n\nSocial Media advertising data\n\nFacebook users per DHS household cluster with a breakdown of user segments such as users with 4G access, 3G access, 2G access, WiFi access, Apple devices and mid-to-high valued goods consumer preferences\n\nRemote sensing data\n\nUsing Google Earth Engine (GEE) features were extracted from publicly accessible low-resolution saellite images\nNighttime luminosity data taken from the Visible Infrared Imaging Radiometer Suite provided by NASA\nDaytime and nighttime land surface temperature derived from MODIS Satellite 2017 data\nNormalized Difference Vegetation Index (NDVI) derived from Landsat 2017\nFor each satellite image, summary statistics was computed, i.e. the mean, maximum, minimum,skewness, variance, and kurtosis of all cloudless pixel values within each DHS cluster\n\nPoint of interest data\n\nUsing OSM volunteered geographic information was obtained for various points of interest like banks, hotels, convenience stores within each DHS household cluster.\n\nHealth data from Department of health\nPublic school information\n\n\n\n\n\n\nLinear Regression, Lasso Regression, Ridge Regression, Random Forest, and LightGBM. The models were trained on social media data, remote sensing data, and point of interest data, first separately then combined, with the hypothesis that integrating multiple data sources will lead to improved model performance over using any one data source alone.\n\n\n\n\n\nUsing multiple data sources provided better results than using a single data source\nImportant features in this study to predict wealth are night time light values, proportion of population with 4G access, presence of public schools\n\n\n\n\nResearch Paper"
  },
  {
    "objectID": "research_methods/poverty_mapping_deep_reinforcement.html",
    "href": "research_methods/poverty_mapping_deep_reinforcement.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Reinforcement learning approach in which free low-resolution imagery is used to dynamically identify where to acquire costly high-resolution images, prior to performing a deep learning task on high-resolution images\n\n\n\n\nLSMS survey conducted in Uganda\nHigh resolution images from DigitalGlobe satellites with 3 bands (RGB) and 30cm pixel resolution\nLow resolution satellite imagery from Sentinel-2 with 3 bands (RGB) with 10m pixel resolution\n\n\n\n\n\n\n\nDeep reinforcement learning method used\n\n\n\nIn the first step, High Resolution (HR) tiles are adaptively sampled and in the second step, pre-trained detector is used on the images\n\n\n\n\nThis framework finds tiles to sample, conditioned on the low spatial resolution image covering a cluster.\nA policy network is modelled to only choose tiles where there is desirable number of object counts\nThe reward function encourages dropping as many subtiles as possible while successfully approximating the classwise object counts (object detection was used)\n\n\n\n\n\n\nThe model achieved an R-squared of 0.62 and substantially outperforms results published from other studies, while using around 80% fewer satellite images.\nThe model is performing well when images of wet season is used instead of dry season\n\n\n\n\nDifference in image acquisition for dry and wet seasons\n\n\n\n\n\nResearch Paper"
  },
  {
    "objectID": "data_sources_detecting_poverty/multispectral_remote_sensing.html",
    "href": "data_sources_detecting_poverty/multispectral_remote_sensing.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Multispectral remote sensing is a passive remote sensing type. This means that the sensor is measuring light energy from an existing source - in this case the sun.\n\n\nThe electromagnetic spectrum is composed of a range of different wavelengths or “colors” of light energy. A spectral remote sensing instrument collects light energy within specific regions of the electromagnetic spectrum. Each region in the spectrum is referred to as a band.\n\n\n\nElectromagnetic Spectrum\n\n\n\n\n\nSpectral remote sensing data are collected by powerful camera-like instruments known as imaging spectrometers. Imaging spectrometers collect reflected light energy in “bands.”\nIn Multispectral dataset, the band information is reported as the center wavelength value. This value represents the center point value of the wavelengths represented in that band. Thus in a band spanning 800-850 nm, the center would be 825 nm.\n\n\n\nThe spectral resolution of a dataset that has more than one band, refers to the spectral width of each band in the dataset.\n\n\n\nThe spatial resolution of a raster represents the area on the ground that each pixel covers. If you have smaller pixels in a raster the data will appear more “detailed.” If you have large pixels in a raster, the data will appear more coarse or “fuzzy.”"
  },
  {
    "objectID": "data_sources_detecting_poverty/survey_data.html",
    "href": "data_sources_detecting_poverty/survey_data.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Demographic and Health Survey (DHS)\nHousehold expenditure and Income survery\nLiving standards measurement study (LSMS)\nSurveys conducted by National Statistics Office (NSO) of various countries\n\n\n\n\n\nVerifiability of the variable\nCorrelation betweent the variable and household consumption levels\nWe should not depend on any single or very few variables\nThe variables may differ based on the region. For example for a rural area, livestock is important but not for urban area\n\n\n\n\nHuman capital variables\n\nEducation of household head\nHighest level of education in household\nFemale literacy\nNumber of childern in school\n\nDemographic characteristics\n\nHousehold size\nNumber of children\nGender / Marital status\nAge of Household head\nDependency ratio\n\nHousehold assets\n\nOwn home\nType of wall construction\nType of roofing material\nType of latrine\nNumber of rooms per capita\nType of cooking fuel\nRadio, television and other forms of electronic or communication devices\nBicycle, car, motorcycle or other means of owned transport\nFurniture\nAccess to electricity\nCooker, heater, fan, air-conditioning\n\nProductive assets\n\nLandholding size\nLivestock\nuse of fertiliser\n\nLivelihood options\n\nAgricultural or nonfarm wage labour\nNon-farm independent business\nAgricultural production of cash or staple crops\nReceipt of foreign remittances\nSector of work\n\nCommunity variables\n\nPresence of midwife\nPopulation density\nAsphalt road\nBank in Community\nDivisional Secretariat in community\n\n\n\n\n\n\n\nProxies used in a PMT model\n\n\nReference:-\n\nWorld bank Document on PMT\nTargeting the Poorest\nExclusion by Design\nconsiderations in using PMT"
  },
  {
    "objectID": "data_sources_detecting_poverty/satellite_imagery.html",
    "href": "data_sources_detecting_poverty/satellite_imagery.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Satellite images are obtained from earth-observing systems. In general, the three main types of these systems, based on the altitude of their orbit, are geostationary (GEO) satellites, low Earth orbit (LEO) satellites, and medium Earth orbit satellites.\nGEO satellites stay positioned over the same spot on the Earth, with a highest altitude of about 36,000 km. This enables them to have greater Earth surface coverage, but with an increasingly skewed pixel towards the edge of the sensor coverage. GEO satellites were originally designed for meteorological use. An example of such a satellite is the HIMAWARI-8, which is positioned over Indonesia and can cover half the globe, having the highest spatial resolution of 500 meters (m) with images taken at 10-minute intervals.\nLEO satellites are positioned relatively close to the Earth’s surface at an altitude of 400 km to 800 km. These satellites can complete their rotation around the earth in about 90 minutes as they travel through a fixed orbit at around 28,000 km per hour. LEO satellites have wider coverage toward the poles, instead of at the equator. Being closer to earth allows these satellites to have higher spatial resolution. The resolution of LEO satellites can be as high as 30 centimeters per pixel for captured images in black and white or panchromatic, while commercially available images in color or multispectral bands can have about 1 m per pixel. Some popular publicly available LEO sensors are the Moderate Resolution Imaging Spectroradiometer (MODIS) and Landsat; with spatial resolutions of 250 m, 500 m, and 1000 m for MODIS, and 30 m for Landsat. These sensors have data applications that are well documented and have been covered by peer-reviewed literature. Meanwhile, the Sentinel-2A and 2B satellites, operated by the European Satellite Agency, have spatial resolutions of 10 m to 60 m, depending on the band.\nMedium earth orbit satellites are commonly used on navigation, communication, and geodetic or space satellites. They are positioned at approximately 20,000 km above the Earth, between GEO and LEO satellites.\n\n\n\n\nLandsat 8 Spectral bands and their purpose\n\n\n\n\n\n\nPrevious studies have used publicly available images from Landsat 8 (with 15 m resolution after pansharpening) and Sentinel 2 (with 10 m resolution). In identifying which specific area an image belonged to, its center was used as reference point.\n\n\n\n\n\nCollect cloud-free daytime images or with least amount of cloud cover\nPanshrapen the images to enhance the resolution of the images. Pansharpening produces a single high-resolution, color, multiband RGB image by combining high-resolution panchromatic images (black and white but sensitive to colors) with lower-resolution, multispectral band images. This is achieved by increasing the pixel-per-unit area of the multispectral band RGB image, transforming the RGB color scheme into a hue saturation value, and changing the value to the pixel intensity of the panchromatic image. The original Landsat images with 30 m resolution were converted to 15 m resolution after pansharpening\nWe should isolate the images that render highest loss, to prevent contamination of the input dataset. These images are very cloudy, with no recognizable land or urban areas, which could render the model inaccurate in predicting class and training incorrect features. Such images were caused either by weather disturbance or technical problems with the sensor’s camera, and should be isolated from further training.\nData augmentation methods should be used to prevent overfitting. Methods suitable to remote-sensing images are:-\n\nVertical and horizontal flipping\nRandom lighting\ncontrast change within a 10% probability\nDihedral and symmetric warping\n\n\n\n\n\n\nEarth Engine\nTranslator library for raster and vector geospatial data formats called Geospatial Data Abstraction Library (GDAL)\n\n\n\n\n\n\nImages from Visible Infrared Imaging Radiometer Suite (VIIRS) which provides publicly accessible earth observation images at night for the entire globe.\nData processing is required to ensure consistency of the resolution of night light data with the daytime satellite imagery in preparation for the CNN modelling\nFor a more effective training of the CNN model, actual values of intensity of lights were batched into discrete groups. Similarly, a Gaussian mixture model (GMM) for clustering the values of night light intensity was applied. The GMM assumes that the night light intensity distribution comes from the mixture of k underlying Gaussian or Normal distributions. A histogram of the radiance values was evaluated to arrive at the set of Normal distributions that best fit the data"
  },
  {
    "objectID": "telecom_churn_prediction/telecom_churn_prediction.html",
    "href": "telecom_churn_prediction/telecom_churn_prediction.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Telecom Churn Prediction\nThe objectives of this project are:-\n1. Perform exploratory analysis and extract insights from the dataset.\n2. Split the dataset into train/test sets and explain your reasoning.\n3. Build a predictive model to predict which customers are going to churn and discuss the reason why you choose a particular algorithm.\n4. Establish metrics to evaluate model performance.\n5. Discuss the potential issues with deploying the model into production\n\nImport the required libraries\n\n# python version # 3.8.2\nimport pandas as pd \nimport numpy as np \nfrom pandas_profiling import ProfileReport \nfrom pycaret.classification import * \nfrom sklearn import metrics \nimport os \nfrom sklearn.model_selection import train_test_split \n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# option to display all columns\npd.set_option('display.max_columns', None)\n\n\n# Read the data\ntelecom_churn = pd.read_csv('data science challenge.csv')\n\n\ntelecom_churn.head(10)\n\n\n\n\n\n  \n    \n      \n      state\n      account length\n      area code\n      phone number\n      international plan\n      voice mail plan\n      number vmail messages\n      total day minutes\n      total day calls\n      total day charge\n      total eve minutes\n      total eve calls\n      total eve charge\n      total night minutes\n      total night calls\n      total night charge\n      total intl minutes\n      total intl calls\n      total intl charge\n      customer service calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      5\n      AL\n      118\n      510\n      391-8027\n      yes\n      no\n      0\n      223.4\n      98\n      37.98\n      220.6\n      101\n      18.75\n      203.9\n      118\n      9.18\n      6.3\n      6\n      1.70\n      0\n      False\n    \n    \n      6\n      MA\n      121\n      510\n      355-9993\n      no\n      yes\n      24\n      218.2\n      88\n      37.09\n      348.5\n      108\n      29.62\n      212.6\n      118\n      9.57\n      7.5\n      7\n      2.03\n      3\n      False\n    \n    \n      7\n      MO\n      147\n      415\n      329-9001\n      yes\n      no\n      0\n      157.0\n      79\n      26.69\n      103.1\n      94\n      8.76\n      211.8\n      96\n      9.53\n      7.1\n      6\n      1.92\n      0\n      False\n    \n    \n      8\n      LA\n      117\n      408\n      335-4719\n      no\n      no\n      0\n      184.5\n      97\n      31.37\n      351.6\n      80\n      29.89\n      215.8\n      90\n      9.71\n      8.7\n      4\n      2.35\n      1\n      False\n    \n    \n      9\n      WV\n      141\n      415\n      330-8173\n      yes\n      yes\n      37\n      258.6\n      84\n      43.96\n      222.0\n      111\n      18.87\n      326.4\n      97\n      14.69\n      11.2\n      5\n      3.02\n      0\n      False\n    \n  \n\n\n\n\n\n\nCheck the Shape and Column types of the Dataframe\n\ntelecom_churn.shape\n\n(3333, 21)\n\n\n\ntelecom_churn.dtypes\n\nstate                      object\naccount length              int64\narea code                   int64\nphone number               object\ninternational plan         object\nvoice mail plan            object\nnumber vmail messages       int64\ntotal day minutes         float64\ntotal day calls             int64\ntotal day charge          float64\ntotal eve minutes         float64\ntotal eve calls             int64\ntotal eve charge          float64\ntotal night minutes       float64\ntotal night calls           int64\ntotal night charge        float64\ntotal intl minutes        float64\ntotal intl calls            int64\ntotal intl charge         float64\ncustomer service calls      int64\nchurn                        bool\ndtype: object\n\n\n\n\nExploratory Analysis\n\n# No missing values in the data.\n# Scaling of numeric columns is required\ntelecom_churn.describe()\n\n\n\n\n\n  \n    \n      \n      account length\n      area code\n      number vmail messages\n      total day minutes\n      total day calls\n      total day charge\n      total eve minutes\n      total eve calls\n      total eve charge\n      total night minutes\n      total night calls\n      total night charge\n      total intl minutes\n      total intl calls\n      total intl charge\n      customer service calls\n    \n  \n  \n    \n      count\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n      3333.000000\n    \n    \n      mean\n      101.064806\n      437.182418\n      8.099010\n      179.775098\n      100.435644\n      30.562307\n      200.980348\n      100.114311\n      17.083540\n      200.872037\n      100.107711\n      9.039325\n      10.237294\n      4.479448\n      2.764581\n      1.562856\n    \n    \n      std\n      39.822106\n      42.371290\n      13.688365\n      54.467389\n      20.069084\n      9.259435\n      50.713844\n      19.922625\n      4.310668\n      50.573847\n      19.568609\n      2.275873\n      2.791840\n      2.461214\n      0.753773\n      1.315491\n    \n    \n      min\n      1.000000\n      408.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      23.200000\n      33.000000\n      1.040000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      74.000000\n      408.000000\n      0.000000\n      143.700000\n      87.000000\n      24.430000\n      166.600000\n      87.000000\n      14.160000\n      167.000000\n      87.000000\n      7.520000\n      8.500000\n      3.000000\n      2.300000\n      1.000000\n    \n    \n      50%\n      101.000000\n      415.000000\n      0.000000\n      179.400000\n      101.000000\n      30.500000\n      201.400000\n      100.000000\n      17.120000\n      201.200000\n      100.000000\n      9.050000\n      10.300000\n      4.000000\n      2.780000\n      1.000000\n    \n    \n      75%\n      127.000000\n      510.000000\n      20.000000\n      216.400000\n      114.000000\n      36.790000\n      235.300000\n      114.000000\n      20.000000\n      235.300000\n      113.000000\n      10.590000\n      12.100000\n      6.000000\n      3.270000\n      2.000000\n    \n    \n      max\n      243.000000\n      510.000000\n      51.000000\n      350.800000\n      165.000000\n      59.640000\n      363.700000\n      170.000000\n      30.910000\n      395.000000\n      175.000000\n      17.770000\n      20.000000\n      20.000000\n      5.400000\n      9.000000\n    \n  \n\n\n\n\n\n# Format the column names, remove space and special characters in column names\ntelecom_churn.columns =  telecom_churn.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n\n\ntelecom_churn\n\n\n\n\n\n  \n    \n      \n      state\n      account_length\n      area_code\n      phone_number\n      international_plan\n      voice_mail_plan\n      number_vmail_messages\n      total_day_minutes\n      total_day_calls\n      total_day_charge\n      total_eve_minutes\n      total_eve_calls\n      total_eve_charge\n      total_night_minutes\n      total_night_calls\n      total_night_charge\n      total_intl_minutes\n      total_intl_calls\n      total_intl_charge\n      customer_service_calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3328\n      AZ\n      192\n      415\n      414-4276\n      no\n      yes\n      36\n      156.2\n      77\n      26.55\n      215.5\n      126\n      18.32\n      279.1\n      83\n      12.56\n      9.9\n      6\n      2.67\n      2\n      False\n    \n    \n      3329\n      WV\n      68\n      415\n      370-3271\n      no\n      no\n      0\n      231.1\n      57\n      39.29\n      153.4\n      55\n      13.04\n      191.3\n      123\n      8.61\n      9.6\n      4\n      2.59\n      3\n      False\n    \n    \n      3330\n      RI\n      28\n      510\n      328-8230\n      no\n      no\n      0\n      180.8\n      109\n      30.74\n      288.8\n      58\n      24.55\n      191.9\n      91\n      8.64\n      14.1\n      6\n      3.81\n      2\n      False\n    \n    \n      3331\n      CT\n      184\n      510\n      364-6381\n      yes\n      no\n      0\n      213.8\n      105\n      36.35\n      159.6\n      84\n      13.57\n      139.2\n      137\n      6.26\n      5.0\n      10\n      1.35\n      2\n      False\n    \n    \n      3332\n      TN\n      74\n      415\n      400-4344\n      no\n      yes\n      25\n      234.4\n      113\n      39.85\n      265.9\n      82\n      22.60\n      241.4\n      77\n      10.86\n      13.7\n      4\n      3.70\n      0\n      False\n    \n  \n\n3333 rows × 21 columns\n\n\n\n\n#telecom_churn[\"area_code\"] = telecom_churn[\"area_code\"].astype('category')\n\n\nprofile = ProfileReport(telecom_churn, title = \"Telecom Churn Report\")\n\n\n# create report for EDA\nprofile.to_widgets()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#save the profile report\nprofile.to_file(\"telecom_churn_eda.html\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[telecom_churn.churn.value_counts()]\n\n[False    2850\n True      483\n Name: churn, dtype: int64]\n\n\n\npd.crosstab(telecom_churn.churn, telecom_churn. customer_service_calls,margins=True, margins_name=\"Total\")\n\n\n\n\n\n  \n    \n      customer_service_calls\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      Total\n    \n    \n      churn\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      False\n      605\n      1059\n      672\n      385\n      90\n      26\n      8\n      4\n      1\n      0\n      2850\n    \n    \n      True\n      92\n      122\n      87\n      44\n      76\n      40\n      14\n      5\n      1\n      2\n      483\n    \n    \n      Total\n      697\n      1181\n      759\n      429\n      166\n      66\n      22\n      9\n      2\n      2\n      3333\n    \n  \n\n\n\n\n\nimport matplotlib.pyplot as plt\nct = pd.crosstab(telecom_churn.churn, telecom_churn.customer_service_calls)\nct.plot.bar(stacked=True)\nplt.legend(title='churn vs Number of calls')\nplt.show()\n\n\n\n\n\ntelecom_churn['area_code'].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntelecom_churn['state'].value_counts().head(10).plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntelecom_churn['international_plan'].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntelecom_churn['voice_mail_plan'].value_counts().plot.bar()\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nObservations from EDA are:-\n\nDataset is imbalanced - 85.5% customers did not churn and 14.5% customers churned\nState consist of 51 distinct values with high cardinality\nNumeric variables are in different ranges and needs to be scaled\nThree distinct area codes - Area code ‘415’ is 49.7%, rest two area codes are equally distributed\nDistinct values of phone number is equal to the length of the dataset. This will be equivalent to primary key of the dataset. Not be included in modelling\n72.3% customers did not activate their voicemail plan. This is verified by equal number of customers with zero number of voice messages\nTotal international calls and customer service calls data is skewed. This is verified by high kurtosis and skewness.\nAll the other numeric variables are following normal distribution as verified by kurtosis / skewness values and histogram\n\n\n\nSplit the Data for Training and Testing\nThe Machine learning algorithm should not be exposed to test data. The performance of the learning algorithm can only be measured by testing on unseen data. To achieve the same a train and test split with 95% and 5% is created. It is ensured that the sampling is stratified so that the proportion of churn and not churn customers are equal in train and test data. As the amount of data is very less, only 5% of the data is kept aside for testing.Further the train data is further split into train and validation set with 90% and 10%. Validation set is required for hyperparameter tuning.As validation set is also exposed to training algorithm, it is also should not be used for model validation. Model validation is done on test set only.\n\n# convert the target value to integers. \ntelecom_churn['churn'] = telecom_churn['churn'] * 1\n\n\ntrain, test = train_test_split(telecom_churn, test_size = 0.05, stratify = telecom_churn['churn']) \nprint('Data for Modeling: ' + str(train.shape))\nprint('Unseen Data For Predictions: ' + str(test.shape))\n\nData for Modeling: (3166, 21)\nUnseen Data For Predictions: (167, 21)\n\n\n\n# Test the proportion of churn in train and test sets\ntrain.churn.value_counts()\n\n0    2707\n1     459\nName: churn, dtype: int64\n\n\n\n# 16.5% of the customers churned in train data\n(459/2707)*100\n\n16.956039896564462\n\n\n\ntest.churn.value_counts()\n\n0    143\n1     24\nName: churn, dtype: int64\n\n\n\n# 16.7% of the customers churned in test data\n(24/143)*100\n# customers churned proportionally from train and test data\n\n16.783216783216783\n\n\n\n\nModelling with Pycaret\n\nTrain and validation sets are created with 90 % and 10 % data.\nThe random seed selected for the modeling is 786\nIn this step we are normalizing the data, ignoring the variable ‘phone number’ for analysis\nFixing the imbalance in the data using SMOTE method\nWe are transforming the features - Changing the distribution of variables to a normal or approximate normal distribution\nIgnorning features with low variance - This will ignore variables (multi level categorical) where a single level dominates and there is not much variation in the information provided by the feature\nThe setup is inferring the customer_service_calls as numeric (as there are only ten distinct values). Hence explicitly mentioning it as numeric\n\n\nexp_clf =    setup(data = train, target = 'churn', session_id = 786, \n                   train_size = 0.9,\n                   normalize = True,\n                   transformation = True,\n                   ignore_low_variance = True,               \n                   ignore_features = ['phone_number'],\n                   fix_imbalance = True,\n                   high_cardinality_features = ['state'],\n                   numeric_features = ['customer_service_calls'])               \n\nSetup Succesfully Completed!\n\n\n\n                    Description        Value    \n                \n                        0\n                        session_id\n                        786\n            \n            \n                        1\n                        Target Type\n                        Binary\n            \n            \n                        2\n                        Label Encoded\n                        None\n            \n            \n                        3\n                        Original Data\n                        (3166, 21)\n            \n            \n                        4\n                        Missing Values \n                        False\n            \n            \n                        5\n                        Numeric Features \n                        15\n            \n            \n                        6\n                        Categorical Features \n                        5\n            \n            \n                        7\n                        Ordinal Features \n                        False\n            \n            \n                        8\n                        High Cardinality Features \n                        True\n            \n            \n                        9\n                        High Cardinality Method \n                        frequency\n            \n            \n                        10\n                        Sampled Data\n                        (3166, 21)\n            \n            \n                        11\n                        Transformed Train Set\n                        (2849, 22)\n            \n            \n                        12\n                        Transformed Test Set\n                        (317, 22)\n            \n            \n                        13\n                        Numeric Imputer \n                        mean\n            \n            \n                        14\n                        Categorical Imputer \n                        constant\n            \n            \n                        15\n                        Normalize \n                        True\n            \n            \n                        16\n                        Normalize Method \n                        zscore\n            \n            \n                        17\n                        Transformation \n                        True\n            \n            \n                        18\n                        Transformation Method \n                        yeo-johnson\n            \n            \n                        19\n                        PCA \n                        False\n            \n            \n                        20\n                        PCA Method \n                        None\n            \n            \n                        21\n                        PCA Components \n                        None\n            \n            \n                        22\n                        Ignore Low Variance \n                        True\n            \n            \n                        23\n                        Combine Rare Levels \n                        False\n            \n            \n                        24\n                        Rare Level Threshold \n                        None\n            \n            \n                        25\n                        Numeric Binning \n                        False\n            \n            \n                        26\n                        Remove Outliers \n                        False\n            \n            \n                        27\n                        Outliers Threshold \n                        None\n            \n            \n                        28\n                        Remove Multicollinearity \n                        False\n            \n            \n                        29\n                        Multicollinearity Threshold \n                        None\n            \n            \n                        30\n                        Clustering \n                        False\n            \n            \n                        31\n                        Clustering Iteration \n                        None\n            \n            \n                        32\n                        Polynomial Features \n                        False\n            \n            \n                        33\n                        Polynomial Degree \n                        None\n            \n            \n                        34\n                        Trignometry Features \n                        False\n            \n            \n                        35\n                        Polynomial Threshold \n                        None\n            \n            \n                        36\n                        Group Features \n                        False\n            \n            \n                        37\n                        Feature Selection \n                        False\n            \n            \n                        38\n                        Features Selection Threshold \n                        None\n            \n            \n                        39\n                        Feature Interaction \n                        False\n            \n            \n                        40\n                        Feature Ratio \n                        False\n            \n            \n                        41\n                        Interaction Threshold \n                        None\n            \n            \n                        42\n                        Fix Imbalance\n                        True\n            \n            \n                        43\n                        Fix Imbalance Method\n                        SMOTE\n            \n    \n\n\n\ncompare_models(fold = 5)\n\n\n                    Model        Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC        TT (Sec)    \n                \n                        0\n                        Light Gradient Boosting Machine\n                        0.9512\n                        0.9127\n                        0.7771\n                        0.8732\n                        0.8213\n                        0.7932\n                        0.7957\n                        0.1908\n            \n            \n                        1\n                        Extreme Gradient Boosting\n                        0.9488\n                        0.9145\n                        0.7675\n                        0.8652\n                        0.8124\n                        0.7829\n                        0.7854\n                        0.2130\n            \n            \n                        2\n                        CatBoost Classifier\n                        0.9466\n                        0.9140\n                        0.7577\n                        0.8583\n                        0.8041\n                        0.7734\n                        0.7759\n                        4.3314\n            \n            \n                        3\n                        Extra Trees Classifier\n                        0.9333\n                        0.9032\n                        0.6535\n                        0.8544\n                        0.7392\n                        0.7018\n                        0.7109\n                        0.1456\n            \n            \n                        4\n                        Random Forest Classifier\n                        0.9330\n                        0.9082\n                        0.7045\n                        0.8147\n                        0.7529\n                        0.7145\n                        0.7186\n                        0.0366\n            \n            \n                        5\n                        Gradient Boosting Classifier\n                        0.9308\n                        0.9078\n                        0.7674\n                        0.7618\n                        0.7635\n                        0.7230\n                        0.7238\n                        1.1674\n            \n            \n                        6\n                        Decision Tree Classifier\n                        0.8929\n                        0.8227\n                        0.7238\n                        0.6106\n                        0.6616\n                        0.5986\n                        0.6022\n                        0.0356\n            \n            \n                        7\n                        Ada Boost Classifier\n                        0.8645\n                        0.8464\n                        0.6003\n                        0.5325\n                        0.5625\n                        0.4829\n                        0.4853\n                        0.2996\n            \n            \n                        8\n                        Naive Bayes\n                        0.8284\n                        0.7894\n                        0.6439\n                        0.4430\n                        0.5233\n                        0.4237\n                        0.4355\n                        0.0028\n            \n            \n                        9\n                        K Neighbors Classifier\n                        0.7715\n                        0.7897\n                        0.6878\n                        0.3531\n                        0.4664\n                        0.3398\n                        0.3705\n                        0.0160\n            \n            \n                        10\n                        Logistic Regression\n                        0.7375\n                        0.7886\n                        0.6852\n                        0.3152\n                        0.4314\n                        0.2904\n                        0.3273\n                        0.0316\n            \n            \n                        11\n                        Ridge Classifier\n                        0.7371\n                        0.0000\n                        0.6852\n                        0.3149\n                        0.4311\n                        0.2899\n                        0.3270\n                        0.0082\n            \n            \n                        12\n                        Linear Discriminant Analysis\n                        0.7336\n                        0.7844\n                        0.6779\n                        0.3103\n                        0.4252\n                        0.2824\n                        0.3190\n                        0.0144\n            \n            \n                        13\n                        SVM - Linear Kernel\n                        0.7329\n                        0.0000\n                        0.6079\n                        0.3163\n                        0.4021\n                        0.2621\n                        0.2910\n                        0.0198\n            \n            \n                        14\n                        Quadratic Discriminant Analysis\n                        0.5907\n                        0.6505\n                        0.6373\n                        0.2089\n                        0.3106\n                        0.1201\n                        0.1596\n                        0.0110\n            \n    \n\n\nLGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=786, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n\n\n\n\nCreating the models for top performing algorithms based on Precision and AUC. Tree based models are performing well on this dataset\n\nlightgbm = create_model('lightgbm', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9526\n                        0.9008\n                        0.8171\n                        0.8481\n                        0.8323\n                        0.8047\n                        0.8049\n            \n            \n                        1\n                        0.9561\n                        0.9271\n                        0.8193\n                        0.8718\n                        0.8447\n                        0.8192\n                        0.8197\n            \n            \n                        2\n                        0.9544\n                        0.9241\n                        0.8072\n                        0.8701\n                        0.8375\n                        0.8110\n                        0.8118\n            \n            \n                        3\n                        0.9544\n                        0.9278\n                        0.7470\n                        0.9254\n                        0.8267\n                        0.8007\n                        0.8068\n            \n            \n                        4\n                        0.9385\n                        0.8836\n                        0.6951\n                        0.8507\n                        0.7651\n                        0.7301\n                        0.7351\n            \n            \n                        Mean\n                        0.9512\n                        0.9127\n                        0.7771\n                        0.8732\n                        0.8213\n                        0.7932\n                        0.7957\n            \n            \n                        SD\n                        0.0065\n                        0.0176\n                        0.0488\n                        0.0278\n                        0.0287\n                        0.0321\n                        0.0307\n            \n    \n\n\n\ncatboost = create_model('catboost', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9421\n                        0.9161\n                        0.7439\n                        0.8356\n                        0.7871\n                        0.7537\n                        0.7554\n            \n            \n                        1\n                        0.9456\n                        0.9254\n                        0.7711\n                        0.8421\n                        0.8050\n                        0.7735\n                        0.7745\n            \n            \n                        2\n                        0.9561\n                        0.9198\n                        0.8313\n                        0.8625\n                        0.8466\n                        0.8210\n                        0.8212\n            \n            \n                        3\n                        0.9544\n                        0.9314\n                        0.7470\n                        0.9254\n                        0.8267\n                        0.8007\n                        0.8068\n            \n            \n                        4\n                        0.9350\n                        0.8775\n                        0.6951\n                        0.8261\n                        0.7550\n                        0.7178\n                        0.7214\n            \n            \n                        Mean\n                        0.9466\n                        0.9140\n                        0.7577\n                        0.8583\n                        0.8041\n                        0.7734\n                        0.7759\n            \n            \n                        SD\n                        0.0078\n                        0.0190\n                        0.0443\n                        0.0356\n                        0.0317\n                        0.0360\n                        0.0358\n            \n    \n\n\n\nxgboost = create_model('xgboost', fold = 5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9474\n                        0.9080\n                        0.8171\n                        0.8171\n                        0.8171\n                        0.7863\n                        0.7863\n            \n            \n                        1\n                        0.9561\n                        0.9270\n                        0.8072\n                        0.8816\n                        0.8428\n                        0.8173\n                        0.8184\n            \n            \n                        2\n                        0.9474\n                        0.9231\n                        0.7590\n                        0.8630\n                        0.8077\n                        0.7773\n                        0.7795\n            \n            \n                        3\n                        0.9509\n                        0.9393\n                        0.7470\n                        0.8986\n                        0.8158\n                        0.7877\n                        0.7922\n            \n            \n                        4\n                        0.9420\n                        0.8752\n                        0.7073\n                        0.8657\n                        0.7785\n                        0.7455\n                        0.7506\n            \n            \n                        Mean\n                        0.9488\n                        0.9145\n                        0.7675\n                        0.8652\n                        0.8124\n                        0.7829\n                        0.7854\n            \n            \n                        SD\n                        0.0047\n                        0.0220\n                        0.0404\n                        0.0272\n                        0.0206\n                        0.0230\n                        0.0218\n            \n    \n\n\n\nrf = create_model('rf', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9298\n                        0.8986\n                        0.7317\n                        0.7692\n                        0.7500\n                        0.7092\n                        0.7095\n            \n            \n                        1\n                        0.9263\n                        0.9227\n                        0.7349\n                        0.7531\n                        0.7439\n                        0.7009\n                        0.7009\n            \n            \n                        2\n                        0.9281\n                        0.9091\n                        0.6867\n                        0.7917\n                        0.7355\n                        0.6941\n                        0.6965\n            \n            \n                        3\n                        0.9421\n                        0.9291\n                        0.7349\n                        0.8472\n                        0.7871\n                        0.7538\n                        0.7563\n            \n            \n                        4\n                        0.9385\n                        0.8817\n                        0.6341\n                        0.9123\n                        0.7482\n                        0.7145\n                        0.7298\n            \n            \n                        Mean\n                        0.9330\n                        0.9082\n                        0.7045\n                        0.8147\n                        0.7529\n                        0.7145\n                        0.7186\n            \n            \n                        SD\n                        0.0062\n                        0.0170\n                        0.0396\n                        0.0583\n                        0.0178\n                        0.0208\n                        0.0221\n            \n    \n\n\n\net = create_model('et', fold =5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9175\n                        0.8940\n                        0.6463\n                        0.7465\n                        0.6928\n                        0.6455\n                        0.6477\n            \n            \n                        1\n                        0.9404\n                        0.9064\n                        0.6988\n                        0.8657\n                        0.7733\n                        0.7394\n                        0.7451\n            \n            \n                        2\n                        0.9404\n                        0.9096\n                        0.6747\n                        0.8889\n                        0.7671\n                        0.7337\n                        0.7428\n            \n            \n                        3\n                        0.9456\n                        0.9307\n                        0.6867\n                        0.9194\n                        0.7862\n                        0.7558\n                        0.7664\n            \n            \n                        4\n                        0.9227\n                        0.8754\n                        0.5610\n                        0.8519\n                        0.6765\n                        0.6347\n                        0.6525\n            \n            \n                        Mean\n                        0.9333\n                        0.9032\n                        0.6535\n                        0.8544\n                        0.7392\n                        0.7018\n                        0.7109\n            \n            \n                        SD\n                        0.0111\n                        0.0183\n                        0.0494\n                        0.0586\n                        0.0453\n                        0.0510\n                        0.0503\n            \n    \n\n\n\n\nTune the created models for selecting the best hyperparameters\n\ntuned_lightgbm = tune_model(lightgbm, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9404\n                        0.9034\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        1\n                        0.9439\n                        0.8894\n                        0.8049\n                        0.8049\n                        0.8049\n                        0.7721\n                        0.7721\n            \n            \n                        2\n                        0.9579\n                        0.9525\n                        0.8537\n                        0.8537\n                        0.8537\n                        0.8291\n                        0.8291\n            \n            \n                        3\n                        0.9649\n                        0.9000\n                        0.8049\n                        0.9429\n                        0.8684\n                        0.8483\n                        0.8519\n            \n            \n                        4\n                        0.9474\n                        0.9161\n                        0.8049\n                        0.8250\n                        0.8148\n                        0.7841\n                        0.7842\n            \n            \n                        5\n                        0.9614\n                        0.9198\n                        0.8049\n                        0.9167\n                        0.8571\n                        0.8349\n                        0.8373\n            \n            \n                        6\n                        0.9684\n                        0.9594\n                        0.8810\n                        0.9024\n                        0.8916\n                        0.8731\n                        0.8732\n            \n            \n                        7\n                        0.9649\n                        0.9162\n                        0.7619\n                        1.0000\n                        0.8649\n                        0.8451\n                        0.8554\n            \n            \n                        8\n                        0.9333\n                        0.8673\n                        0.7381\n                        0.7949\n                        0.7654\n                        0.7266\n                        0.7273\n            \n            \n                        9\n                        0.9437\n                        0.9092\n                        0.6829\n                        0.9032\n                        0.7778\n                        0.7462\n                        0.7558\n            \n            \n                        Mean\n                        0.9526\n                        0.9134\n                        0.7918\n                        0.8744\n                        0.8289\n                        0.8015\n                        0.8042\n            \n            \n                        SD\n                        0.0117\n                        0.0259\n                        0.0531\n                        0.0659\n                        0.0414\n                        0.0480\n                        0.0484\n            \n    \n\n\n\ntuned_catboost = tune_model(catboost, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9368\n                        0.8919\n                        0.7317\n                        0.8108\n                        0.7692\n                        0.7328\n                        0.7341\n            \n            \n                        1\n                        0.9474\n                        0.9042\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        2\n                        0.9509\n                        0.9510\n                        0.8049\n                        0.8462\n                        0.8250\n                        0.7964\n                        0.7968\n            \n            \n                        3\n                        0.9544\n                        0.8872\n                        0.7561\n                        0.9118\n                        0.8267\n                        0.8007\n                        0.8053\n            \n            \n                        4\n                        0.9474\n                        0.9258\n                        0.8049\n                        0.8250\n                        0.8148\n                        0.7841\n                        0.7842\n            \n            \n                        5\n                        0.9544\n                        0.9058\n                        0.8049\n                        0.8684\n                        0.8354\n                        0.8090\n                        0.8098\n            \n            \n                        6\n                        0.9579\n                        0.9607\n                        0.7619\n                        0.9412\n                        0.8421\n                        0.8181\n                        0.8242\n            \n            \n                        7\n                        0.9439\n                        0.9133\n                        0.6667\n                        0.9333\n                        0.7778\n                        0.7467\n                        0.7605\n            \n            \n                        8\n                        0.9193\n                        0.8823\n                        0.7143\n                        0.7317\n                        0.7229\n                        0.6757\n                        0.6757\n            \n            \n                        9\n                        0.9437\n                        0.8767\n                        0.7073\n                        0.8788\n                        0.7838\n                        0.7518\n                        0.7577\n            \n            \n                        Mean\n                        0.9456\n                        0.9099\n                        0.7533\n                        0.8589\n                        0.8008\n                        0.7695\n                        0.7729\n            \n            \n                        SD\n                        0.0106\n                        0.0270\n                        0.0452\n                        0.0597\n                        0.0351\n                        0.0411\n                        0.0414\n            \n    \n\n\n\ntuned_xgboost = tune_model(xgboost, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9474\n                        0.8786\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        1\n                        0.9474\n                        0.8737\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        2\n                        0.9509\n                        0.9396\n                        0.8293\n                        0.8293\n                        0.8293\n                        0.8006\n                        0.8006\n            \n            \n                        3\n                        0.9684\n                        0.9017\n                        0.8049\n                        0.9706\n                        0.8800\n                        0.8620\n                        0.8670\n            \n            \n                        4\n                        0.9509\n                        0.9386\n                        0.7805\n                        0.8649\n                        0.8205\n                        0.7921\n                        0.7935\n            \n            \n                        5\n                        0.9544\n                        0.9125\n                        0.7561\n                        0.9118\n                        0.8267\n                        0.8007\n                        0.8053\n            \n            \n                        6\n                        0.9439\n                        0.9686\n                        0.7381\n                        0.8611\n                        0.7949\n                        0.7626\n                        0.7656\n            \n            \n                        7\n                        0.9614\n                        0.9216\n                        0.7857\n                        0.9429\n                        0.8571\n                        0.8350\n                        0.8397\n            \n            \n                        8\n                        0.9263\n                        0.8634\n                        0.6905\n                        0.7838\n                        0.7342\n                        0.6916\n                        0.6935\n            \n            \n                        9\n                        0.9366\n                        0.9118\n                        0.6829\n                        0.8485\n                        0.7568\n                        0.7208\n                        0.7264\n            \n            \n                        Mean\n                        0.9487\n                        0.9110\n                        0.7629\n                        0.8697\n                        0.8120\n                        0.7825\n                        0.7852\n            \n            \n                        SD\n                        0.0113\n                        0.0313\n                        0.0446\n                        0.0533\n                        0.0408\n                        0.0472\n                        0.0476\n            \n    \n\n\n\ntuned_rf = tune_model(rf, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9368\n                        0.8803\n                        0.7561\n                        0.7949\n                        0.7750\n                        0.7383\n                        0.7386\n            \n            \n                        1\n                        0.9368\n                        0.9106\n                        0.8049\n                        0.7674\n                        0.7857\n                        0.7487\n                        0.7490\n            \n            \n                        2\n                        0.9439\n                        0.9287\n                        0.8049\n                        0.8049\n                        0.8049\n                        0.7721\n                        0.7721\n            \n            \n                        3\n                        0.9614\n                        0.9113\n                        0.7805\n                        0.9412\n                        0.8533\n                        0.8313\n                        0.8362\n            \n            \n                        4\n                        0.9439\n                        0.9427\n                        0.8049\n                        0.8049\n                        0.8049\n                        0.7721\n                        0.7721\n            \n            \n                        5\n                        0.9368\n                        0.8999\n                        0.7561\n                        0.7949\n                        0.7750\n                        0.7383\n                        0.7386\n            \n            \n                        6\n                        0.9439\n                        0.9418\n                        0.7143\n                        0.8824\n                        0.7895\n                        0.7575\n                        0.7631\n            \n            \n                        7\n                        0.9509\n                        0.9113\n                        0.7143\n                        0.9375\n                        0.8108\n                        0.7832\n                        0.7927\n            \n            \n                        8\n                        0.9263\n                        0.8532\n                        0.6905\n                        0.7838\n                        0.7342\n                        0.6916\n                        0.6935\n            \n            \n                        9\n                        0.9437\n                        0.8886\n                        0.6829\n                        0.9032\n                        0.7778\n                        0.7462\n                        0.7558\n            \n            \n                        Mean\n                        0.9424\n                        0.9068\n                        0.7509\n                        0.8415\n                        0.7911\n                        0.7579\n                        0.7612\n            \n            \n                        SD\n                        0.0089\n                        0.0265\n                        0.0454\n                        0.0636\n                        0.0293\n                        0.0344\n                        0.0356\n            \n    \n\n\n\ntuned_et = tune_model(et, optimize = 'F1' , n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9158\n                        0.8828\n                        0.6341\n                        0.7429\n                        0.6842\n                        0.6360\n                        0.6386\n            \n            \n                        1\n                        0.9298\n                        0.8971\n                        0.6829\n                        0.8000\n                        0.7368\n                        0.6966\n                        0.6996\n            \n            \n                        2\n                        0.9263\n                        0.9369\n                        0.6585\n                        0.7941\n                        0.7200\n                        0.6780\n                        0.6819\n            \n            \n                        3\n                        0.9509\n                        0.8955\n                        0.7317\n                        0.9091\n                        0.8108\n                        0.7830\n                        0.7891\n            \n            \n                        4\n                        0.9368\n                        0.9228\n                        0.6829\n                        0.8485\n                        0.7568\n                        0.7210\n                        0.7266\n            \n            \n                        5\n                        0.9439\n                        0.8852\n                        0.7073\n                        0.8788\n                        0.7838\n                        0.7520\n                        0.7578\n            \n            \n                        6\n                        0.9474\n                        0.9410\n                        0.7381\n                        0.8857\n                        0.8052\n                        0.7751\n                        0.7794\n            \n            \n                        7\n                        0.9439\n                        0.9394\n                        0.6429\n                        0.9643\n                        0.7714\n                        0.7409\n                        0.7607\n            \n            \n                        8\n                        0.9123\n                        0.8558\n                        0.5476\n                        0.7931\n                        0.6479\n                        0.5997\n                        0.6131\n            \n            \n                        9\n                        0.9120\n                        0.8913\n                        0.4878\n                        0.8333\n                        0.6154\n                        0.5695\n                        0.5956\n            \n            \n                        Mean\n                        0.9319\n                        0.9048\n                        0.6514\n                        0.8450\n                        0.7332\n                        0.6952\n                        0.7042\n            \n            \n                        SD\n                        0.0141\n                        0.0273\n                        0.0755\n                        0.0625\n                        0.0629\n                        0.0698\n                        0.0666\n            \n    \n\n\n\n\nCreate an Ensemble, Blended and Stack model to see the performance\n\ndt = create_model('dt' , fold = 5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8895\n                        0.8289\n                        0.7439\n                        0.5922\n                        0.6595\n                        0.5945\n                        0.6000\n            \n            \n                        1\n                        0.8912\n                        0.8314\n                        0.7470\n                        0.6019\n                        0.6667\n                        0.6026\n                        0.6076\n            \n            \n                        2\n                        0.8877\n                        0.8094\n                        0.6988\n                        0.5979\n                        0.6444\n                        0.5783\n                        0.5807\n            \n            \n                        3\n                        0.9035\n                        0.8536\n                        0.7831\n                        0.6373\n                        0.7027\n                        0.6458\n                        0.6507\n            \n            \n                        4\n                        0.8928\n                        0.7903\n                        0.6463\n                        0.6235\n                        0.6347\n                        0.5719\n                        0.5721\n            \n            \n                        Mean\n                        0.8929\n                        0.8227\n                        0.7238\n                        0.6106\n                        0.6616\n                        0.5986\n                        0.6022\n            \n            \n                        SD\n                        0.0055\n                        0.0214\n                        0.0471\n                        0.0170\n                        0.0234\n                        0.0260\n                        0.0274\n            \n    \n\n\n\ntuned_dt = tune_model(dt, optimize = 'F1', n_iter = 30)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9018\n                        0.8782\n                        0.7805\n                        0.6275\n                        0.6957\n                        0.6379\n                        0.6433\n            \n            \n                        1\n                        0.8842\n                        0.9037\n                        0.8049\n                        0.5690\n                        0.6667\n                        0.5991\n                        0.6123\n            \n            \n                        2\n                        0.8982\n                        0.9072\n                        0.8780\n                        0.6000\n                        0.7129\n                        0.6537\n                        0.6712\n            \n            \n                        3\n                        0.9053\n                        0.8717\n                        0.7805\n                        0.6400\n                        0.7033\n                        0.6476\n                        0.6521\n            \n            \n                        4\n                        0.9193\n                        0.8988\n                        0.8049\n                        0.6875\n                        0.7416\n                        0.6941\n                        0.6971\n            \n            \n                        5\n                        0.9018\n                        0.9168\n                        0.8537\n                        0.6140\n                        0.7143\n                        0.6569\n                        0.6699\n            \n            \n                        6\n                        0.9439\n                        0.9188\n                        0.8333\n                        0.7955\n                        0.8140\n                        0.7809\n                        0.7812\n            \n            \n                        7\n                        0.9298\n                        0.8626\n                        0.7381\n                        0.7750\n                        0.7561\n                        0.7151\n                        0.7154\n            \n            \n                        8\n                        0.9053\n                        0.8125\n                        0.6667\n                        0.6829\n                        0.6747\n                        0.6193\n                        0.6193\n            \n            \n                        9\n                        0.9190\n                        0.8680\n                        0.7073\n                        0.7250\n                        0.7160\n                        0.6688\n                        0.6689\n            \n            \n                        Mean\n                        0.9108\n                        0.8838\n                        0.7848\n                        0.6716\n                        0.7195\n                        0.6673\n                        0.6731\n            \n            \n                        SD\n                        0.0164\n                        0.0307\n                        0.0623\n                        0.0715\n                        0.0406\n                        0.0494\n                        0.0469\n            \n    \n\n\n\nbagged_dt = ensemble_model(tuned_dt, n_estimators = 200, optimize = 'F1')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8807\n                        0.8698\n                        0.7805\n                        0.5614\n                        0.6531\n                        0.5833\n                        0.5949\n            \n            \n                        1\n                        0.9298\n                        0.9043\n                        0.8293\n                        0.7234\n                        0.7727\n                        0.7315\n                        0.7338\n            \n            \n                        2\n                        0.9053\n                        0.9175\n                        0.9024\n                        0.6167\n                        0.7327\n                        0.6776\n                        0.6957\n            \n            \n                        3\n                        0.9404\n                        0.9077\n                        0.8049\n                        0.7857\n                        0.7952\n                        0.7603\n                        0.7604\n            \n            \n                        4\n                        0.9298\n                        0.9073\n                        0.8537\n                        0.7143\n                        0.7778\n                        0.7365\n                        0.7406\n            \n            \n                        5\n                        0.9123\n                        0.9220\n                        0.8293\n                        0.6538\n                        0.7312\n                        0.6796\n                        0.6865\n            \n            \n                        6\n                        0.9509\n                        0.9436\n                        0.8810\n                        0.8043\n                        0.8409\n                        0.8119\n                        0.8131\n            \n            \n                        7\n                        0.9509\n                        0.9128\n                        0.7857\n                        0.8684\n                        0.8250\n                        0.7965\n                        0.7979\n            \n            \n                        8\n                        0.9088\n                        0.8489\n                        0.7381\n                        0.6739\n                        0.7045\n                        0.6507\n                        0.6517\n            \n            \n                        9\n                        0.9401\n                        0.8935\n                        0.7561\n                        0.8158\n                        0.7848\n                        0.7501\n                        0.7508\n            \n            \n                        Mean\n                        0.9249\n                        0.9027\n                        0.8161\n                        0.7218\n                        0.7618\n                        0.7178\n                        0.7225\n            \n            \n                        SD\n                        0.0215\n                        0.0254\n                        0.0504\n                        0.0922\n                        0.0540\n                        0.0664\n                        0.0632\n            \n    \n\n\n\nboosted_dt = ensemble_model(tuned_dt, method = 'Boosting', n_estimators = 50, optimize = 'F1')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8877\n                        0.8717\n                        0.5610\n                        0.6216\n                        0.5897\n                        0.5249\n                        0.5258\n            \n            \n                        1\n                        0.9123\n                        0.8352\n                        0.6341\n                        0.7222\n                        0.6753\n                        0.6249\n                        0.6266\n            \n            \n                        2\n                        0.9333\n                        0.9076\n                        0.7073\n                        0.8056\n                        0.7532\n                        0.7149\n                        0.7169\n            \n            \n                        3\n                        0.9404\n                        0.8308\n                        0.6829\n                        0.8750\n                        0.7671\n                        0.7335\n                        0.7409\n            \n            \n                        4\n                        0.9053\n                        0.8260\n                        0.6098\n                        0.6944\n                        0.6494\n                        0.5949\n                        0.5965\n            \n            \n                        5\n                        0.9333\n                        0.9336\n                        0.7317\n                        0.7895\n                        0.7595\n                        0.7209\n                        0.7216\n            \n            \n                        6\n                        0.9228\n                        0.8915\n                        0.6429\n                        0.7941\n                        0.7105\n                        0.6666\n                        0.6715\n            \n            \n                        7\n                        0.9018\n                        0.8379\n                        0.4524\n                        0.7917\n                        0.5758\n                        0.5248\n                        0.5512\n            \n            \n                        8\n                        0.9263\n                        0.8179\n                        0.6190\n                        0.8387\n                        0.7123\n                        0.6712\n                        0.6814\n            \n            \n                        9\n                        0.9085\n                        0.8018\n                        0.5610\n                        0.7419\n                        0.6389\n                        0.5876\n                        0.5952\n            \n            \n                        Mean\n                        0.9172\n                        0.8554\n                        0.6202\n                        0.7675\n                        0.6832\n                        0.6364\n                        0.6428\n            \n            \n                        SD\n                        0.0159\n                        0.0411\n                        0.0774\n                        0.0701\n                        0.0654\n                        0.0735\n                        0.0710\n            \n    \n\n\n\n# Train a voting classifier with all models in the library\nblender = blend_models()\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.8912\n                        0.0000\n                        0.7561\n                        0.5962\n                        0.6667\n                        0.6028\n                        0.6088\n            \n            \n                        1\n                        0.9404\n                        0.0000\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        2\n                        0.9158\n                        0.0000\n                        0.6829\n                        0.7179\n                        0.7000\n                        0.6511\n                        0.6513\n            \n            \n                        3\n                        0.9474\n                        0.0000\n                        0.7805\n                        0.8421\n                        0.8101\n                        0.7796\n                        0.7804\n            \n            \n                        4\n                        0.9263\n                        0.0000\n                        0.7561\n                        0.7381\n                        0.7470\n                        0.7039\n                        0.7039\n            \n            \n                        5\n                        0.9158\n                        0.0000\n                        0.7073\n                        0.7073\n                        0.7073\n                        0.6581\n                        0.6581\n            \n            \n                        6\n                        0.9404\n                        0.0000\n                        0.7857\n                        0.8049\n                        0.7952\n                        0.7603\n                        0.7604\n            \n            \n                        7\n                        0.9474\n                        0.0000\n                        0.7619\n                        0.8649\n                        0.8101\n                        0.7797\n                        0.7818\n            \n            \n                        8\n                        0.9018\n                        0.0000\n                        0.7143\n                        0.6522\n                        0.6818\n                        0.6239\n                        0.6248\n            \n            \n                        9\n                        0.9190\n                        0.0000\n                        0.7317\n                        0.7143\n                        0.7229\n                        0.6755\n                        0.6755\n            \n            \n                        Mean\n                        0.9245\n                        0.0000\n                        0.7457\n                        0.7438\n                        0.7431\n                        0.6990\n                        0.7001\n            \n            \n                        SD\n                        0.0183\n                        0.0000\n                        0.0333\n                        0.0802\n                        0.0520\n                        0.0628\n                        0.0621\n            \n    \n\n\n\nblender_specific = blend_models(estimator_list = [tuned_lightgbm,tuned_xgboost,\n                                                 tuned_rf, tuned_et, tuned_dt], method = 'soft')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9404\n                        0.8833\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        1\n                        0.9404\n                        0.9079\n                        0.7805\n                        0.8000\n                        0.7901\n                        0.7554\n                        0.7554\n            \n            \n                        2\n                        0.9474\n                        0.9345\n                        0.8293\n                        0.8095\n                        0.8193\n                        0.7885\n                        0.7886\n            \n            \n                        3\n                        0.9684\n                        0.8996\n                        0.8049\n                        0.9706\n                        0.8800\n                        0.8620\n                        0.8670\n            \n            \n                        4\n                        0.9509\n                        0.9361\n                        0.8049\n                        0.8462\n                        0.8250\n                        0.7964\n                        0.7968\n            \n            \n                        5\n                        0.9474\n                        0.9149\n                        0.8293\n                        0.8095\n                        0.8193\n                        0.7885\n                        0.7886\n            \n            \n                        6\n                        0.9544\n                        0.9541\n                        0.7857\n                        0.8919\n                        0.8354\n                        0.8091\n                        0.8113\n            \n            \n                        7\n                        0.9579\n                        0.9318\n                        0.7381\n                        0.9688\n                        0.8378\n                        0.8142\n                        0.8241\n            \n            \n                        8\n                        0.9298\n                        0.8511\n                        0.6905\n                        0.8056\n                        0.7436\n                        0.7032\n                        0.7060\n            \n            \n                        9\n                        0.9437\n                        0.8926\n                        0.7073\n                        0.8788\n                        0.7838\n                        0.7518\n                        0.7577\n            \n            \n                        Mean\n                        0.9481\n                        0.9106\n                        0.7751\n                        0.8581\n                        0.8124\n                        0.7824\n                        0.7851\n            \n            \n                        SD\n                        0.0102\n                        0.0289\n                        0.0458\n                        0.0639\n                        0.0354\n                        0.0412\n                        0.0422\n            \n    \n\n\n\nstacked_models = stack_models(estimator_list = [tuned_lightgbm,tuned_catboost,tuned_xgboost,\n                                                 tuned_rf, tuned_et, tuned_dt], meta_model = None, optimize = 'F1', fold = 5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9368\n                        0.9035\n                        0.8293\n                        0.7556\n                        0.7907\n                        0.7536\n                        0.7547\n            \n            \n                        1\n                        0.9579\n                        0.9200\n                        0.8434\n                        0.8642\n                        0.8537\n                        0.8291\n                        0.8292\n            \n            \n                        2\n                        0.9509\n                        0.9184\n                        0.8434\n                        0.8235\n                        0.8333\n                        0.8045\n                        0.8046\n            \n            \n                        3\n                        0.9614\n                        0.9347\n                        0.8434\n                        0.8861\n                        0.8642\n                        0.8417\n                        0.8421\n            \n            \n                        4\n                        0.9315\n                        0.8787\n                        0.7561\n                        0.7654\n                        0.7607\n                        0.7207\n                        0.7208\n            \n            \n                        Mean\n                        0.9477\n                        0.9111\n                        0.8231\n                        0.8190\n                        0.8205\n                        0.7899\n                        0.7903\n            \n            \n                        SD\n                        0.0117\n                        0.0189\n                        0.0339\n                        0.0519\n                        0.0391\n                        0.0459\n                        0.0458\n            \n    \n\n\n\n\nOut of all the models created tuned_lightgbm is performing better on validation data\n\nevaluate_model(tuned_lightgbm)\n\n\n\n\n\n\nTest the model on the test data and choose the best performing model\n\n\nEvaluate Model\n\n# create funtion to return evaluation metrics\ndef evaluation_metrics(model):\n    check_model = predict_model(model, data = test)\n    print(metrics.confusion_matrix(check_model.churn,check_model.Label))\n    tn, fp, fn, tp = metrics.confusion_matrix(check_model.churn,check_model.Label).ravel()\n    Accuracy = round((tp+tn)/(tp+tn+fp+fn),3)\n    precision = round(tp/(tp+fp),3)\n    specificity = round(tn/(tn+fp),3)\n    recall = round(tp/(tp+fn),3)\n    print( f\"Accuracy:{Accuracy} , Specificity:{specificity}, Precision:{precision} , Recall:{recall}\")\n\n\ncheck tuned_lightgbm\n\nevaluation_metrics(tuned_lightgbm)\n\n[[142   1]\n [  5  19]]\nAccuracy:0.964 , Specificity:0.993, Precision:0.95 , Recall:0.792\n\n\n\n\ncheck tuned_catboost\n\nevaluation_metrics(tuned_catboost)\n\n[[141   2]\n [  5  19]]\nAccuracy:0.958 , Specificity:0.986, Precision:0.905 , Recall:0.792\n\n\n\n\ncheck tuned_xgboost\n\nevaluation_metrics(tuned_xgboost)\n\n[[141   2]\n [  5  19]]\nAccuracy:0.958 , Specificity:0.986, Precision:0.905 , Recall:0.792\n\n\n\n\ncheck tuned_rf\n\nevaluation_metrics(tuned_rf)\n\n[[141   2]\n [  6  18]]\nAccuracy:0.952 , Specificity:0.986, Precision:0.9 , Recall:0.75\n\n\n\n\ncheck tuned_et\n\nevaluation_metrics(tuned_et)\n\n[[143   0]\n [ 11  13]]\nAccuracy:0.934 , Specificity:1.0, Precision:1.0 , Recall:0.542\n\n\n\n\ncheck tuned_dt\n\nevaluation_metrics(tuned_dt)\n\n[[141   2]\n [  6  18]]\nAccuracy:0.952 , Specificity:0.986, Precision:0.9 , Recall:0.75\n\n\n\n\ncheck boosted_dt\n\nevaluation_metrics(boosted_dt)\n\n[[141   2]\n [ 10  14]]\nAccuracy:0.928 , Specificity:0.986, Precision:0.875 , Recall:0.583\n\n\n\n\ncheck bagged_dt\n\nevaluation_metrics(bagged_dt)\n\n[[139   4]\n [  6  18]]\nAccuracy:0.94 , Specificity:0.972, Precision:0.818 , Recall:0.75\n\n\n\n\ncheck blender\n\nevaluation_metrics(blender)\n\n[[143   0]\n [ 11  13]]\nAccuracy:0.934 , Specificity:1.0, Precision:1.0 , Recall:0.542\n\n\n\n\ncheck blender_specific\n\nevaluation_metrics(blender_specific)\n\n[[142   1]\n [  5  19]]\nAccuracy:0.964 , Specificity:0.993, Precision:0.95 , Recall:0.792\n\n\n\n\ncheck stacked_models\n\nevaluation_metrics(stacked_models)\n\n[[139   4]\n [  4  20]]\nAccuracy:0.952 , Specificity:0.972, Precision:0.833 , Recall:0.833\n\n\n\n\n\nFinalizing the Model and Metrics\n\nCompared multiple models to examine which algorithm is suitable for this dataset\nChose the five best performing algorithms and created models for them\nHyper parameter tuning was done to further improve the model performance\nEnsemble of models were created to check their performance on test data\nAll the tuned and esemble models were tested out on unseen data to finalize a model\n\n\n\nModel finalization\nMy recommendation for the final model is tuned_lightgbm. This is because the models predictions for churned customers is very high. From my experience in media Industry, it was observed that business users usually request for model explainability. This model’s Recall is slightly lower than stacked_models. stacked_model was not selected because it does not provide model interpretation. Also given similar performance it is better to go for simpler model.\n\n# Finalize the model\nfinal_model = finalize_model(tuned_lightgbm)\n\n\n# Feature importance using decision tree models\nplot_model(final_model, plot = 'feature')\n\n\n\n\n\n# Feature importance using Shap\ninterpret_model(final_model, plot = 'summary')\n\n\n\n\n\n# local interpretation\ninterpret_model(final_model, plot = 'reason', observation = 14)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n# save the model\nsave_model(final_model,'tuned_lightgbm_dt_save_20201017')\n\nTransformation Pipeline and Model Succesfully Saved\n\n\n\n# load the model\nloaded_model = load_model('bagged_dt_save_20201017')\n\nTransformation Pipeline and Model Sucessfully Loaded\n\n\n\n\nPotential issues with deploying the model into production are:-\n\nData and model versioning As the amount of data is increasing every day, a mechanism to version data along with code needs to be established. This needs to be done without any cost overhead for storing multiple copies of the same data.\nTracking and storing of experiment results and artifacts efficiently. Data scientist should be able to tell which version of model is presently in production, data used for training, what are the evaluation metrics of the model at any given time.\nMonitoring of models in production for data drift - The behaviour of incoming data may change and will may differ from the data on which it was trained\nTaking care of CI/CD in production - As soon as a better performing model is finalized and commited, it should go to production in an automated fashion\nInstead of full-deployment of the models - Canary or Blue-Green deployment should be done. If model should be exposed to 10% to 15% of the population. If it performs well on a small population, then it should be rolled out for everyone.\nFeedback loops - The data used by the model for prediction going again into the training set\n\nThe following tools which can be used for the production issues mentioned above:- 1. Data and model versioning - Data Version Control (DVC) and MLOps by DVC 2. Tracking experiments - mlflow python package 3. Monitoring of models in production - Bi tools 4. Containerization - Docker and Kubernetes 5. CI/CD - Github, CircleCI, MLops 6. Deployment - Seldon core, Heroku 7. Canary / Bluegreen Deployment - AWS Sagemaker\n\n\nAppendix\nIf the business priorirty is to predict both churners and non-churners accurately then Accuracy. If the business priority is to identify churners then precision and recall. If the business priority is to predict non-churners (which is a very rare scenario) then it is specificity. The final model will be chosen as per business priority. If the business priority is :- 1. Predicting both churn and non-churning customers accurately then the model with highest accuracy will be chosen i.e - tuned_dt 2. Maximizing the proportion of churner identifications which are actually correct, then model with highest precision - tuned_dt 3. Identifying the Maximum proportion of actual churners then model with highest recall - bagged_dt (as this is simpler than blender_specific)\n\n# Data for DOE\ndoe = predict_model(loaded_model, data = telecom_churn)\nprint(metrics.confusion_matrix(doe.churn,doe.Label))\n\n[[2821   29]\n [ 135  348]]\n\n\n\ntn, fp, fn, tp = metrics.confusion_matrix(doe.churn,doe.Label).ravel()\nAccuracy = round((tp+tn)/(tp+tn+fp+fn),3)\nprecision = round(tp/(tp+fp),3)\nspecificity = round(tn/(tn+fp),3)\nrecall = round(tp/(tp+fn),3)\nprint( f\"Accuracy:{Accuracy} , Specificity:{specificity}, Precision:{precision} , Recall:{recall}\")\n\nAccuracy:0.951 , Specificity:0.99, Precision:0.923 , Recall:0.72\n\n\n\ndoe.shape\n\n(3333, 23)\n\n\n\ndoe.churn.value_counts()\n\n0    2850\n1     483\nName: churn, dtype: int64\n\n\n\n93/483\n\n0.19254658385093168"
  },
  {
    "objectID": "telecom_churn_prediction/telecom_churn_eda.html",
    "href": "telecom_churn_prediction/telecom_churn_eda.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "EDA on Telecom Churn Data\nThe objectives of this project are:-\n1. Perform exploratory analysis and extract insights from the dataset.\n2. Split the dataset into train/test sets and explain your reasoning.\n3. Build a predictive model to predict which customers are going to churn and discuss the reason why you choose a particular algorithm.\n4. Establish metrics to evaluate model performance.\n5. Discuss the potential issues with deploying the model into production\n\nImport the required libraries\n\n# python version # 3.8.2\nimport pandas as pd \nimport numpy as np \nimport os \nfrom pandas_profiling import ProfileReport\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# option to display all columns\npd.set_option('display.max_columns', None)\n\n\n# Read the data\ntelecom_churn = pd.read_csv('../data/telecom_data/telecom.csv')\n\n\ntelecom_churn.head(10)\n\n\n\n\n\n  \n    \n      \n      state\n      account length\n      area code\n      phone number\n      international plan\n      voice mail plan\n      number vmail messages\n      total day minutes\n      total day calls\n      total day charge\n      total eve minutes\n      total eve calls\n      total eve charge\n      total night minutes\n      total night calls\n      total night charge\n      total intl minutes\n      total intl calls\n      total intl charge\n      customer service calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      5\n      AL\n      118\n      510\n      391-8027\n      yes\n      no\n      0\n      223.4\n      98\n      37.98\n      220.6\n      101\n      18.75\n      203.9\n      118\n      9.18\n      6.3\n      6\n      1.70\n      0\n      False\n    \n    \n      6\n      MA\n      121\n      510\n      355-9993\n      no\n      yes\n      24\n      218.2\n      88\n      37.09\n      348.5\n      108\n      29.62\n      212.6\n      118\n      9.57\n      7.5\n      7\n      2.03\n      3\n      False\n    \n    \n      7\n      MO\n      147\n      415\n      329-9001\n      yes\n      no\n      0\n      157.0\n      79\n      26.69\n      103.1\n      94\n      8.76\n      211.8\n      96\n      9.53\n      7.1\n      6\n      1.92\n      0\n      False\n    \n    \n      8\n      LA\n      117\n      408\n      335-4719\n      no\n      no\n      0\n      184.5\n      97\n      31.37\n      351.6\n      80\n      29.89\n      215.8\n      90\n      9.71\n      8.7\n      4\n      2.35\n      1\n      False\n    \n    \n      9\n      WV\n      141\n      415\n      330-8173\n      yes\n      yes\n      37\n      258.6\n      84\n      43.96\n      222.0\n      111\n      18.87\n      326.4\n      97\n      14.69\n      11.2\n      5\n      3.02\n      0\n      False\n    \n  \n\n\n\n\n\n\nCheck the Shape and Column types of the Dataframe\n\ntelecom_churn.shape\n\n(3333, 21)\n\n\n\ntelecom_churn.dtypes\n\nstate                      object\naccount length              int64\narea code                   int64\nphone number               object\ninternational plan         object\nvoice mail plan            object\nnumber vmail messages       int64\ntotal day minutes         float64\ntotal day calls             int64\ntotal day charge          float64\ntotal eve minutes         float64\ntotal eve calls             int64\ntotal eve charge          float64\ntotal night minutes       float64\ntotal night calls           int64\ntotal night charge        float64\ntotal intl minutes        float64\ntotal intl calls            int64\ntotal intl charge         float64\ncustomer service calls      int64\nchurn                        bool\ndtype: object\n\n\n\n\nExploratory Analysis\n\n# Format the column names, remove space and special characters in column names\ntelecom_churn.columns =  telecom_churn.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n\n\ntelecom_churn\n\n\n\n\n\n  \n    \n      \n      state\n      account_length\n      area_code\n      phone_number\n      international_plan\n      voice_mail_plan\n      number_vmail_messages\n      total_day_minutes\n      total_day_calls\n      total_day_charge\n      total_eve_minutes\n      total_eve_calls\n      total_eve_charge\n      total_night_minutes\n      total_night_calls\n      total_night_charge\n      total_intl_minutes\n      total_intl_calls\n      total_intl_charge\n      customer_service_calls\n      churn\n    \n  \n  \n    \n      0\n      KS\n      128\n      415\n      382-4657\n      no\n      yes\n      25\n      265.1\n      110\n      45.07\n      197.4\n      99\n      16.78\n      244.7\n      91\n      11.01\n      10.0\n      3\n      2.70\n      1\n      False\n    \n    \n      1\n      OH\n      107\n      415\n      371-7191\n      no\n      yes\n      26\n      161.6\n      123\n      27.47\n      195.5\n      103\n      16.62\n      254.4\n      103\n      11.45\n      13.7\n      3\n      3.70\n      1\n      False\n    \n    \n      2\n      NJ\n      137\n      415\n      358-1921\n      no\n      no\n      0\n      243.4\n      114\n      41.38\n      121.2\n      110\n      10.30\n      162.6\n      104\n      7.32\n      12.2\n      5\n      3.29\n      0\n      False\n    \n    \n      3\n      OH\n      84\n      408\n      375-9999\n      yes\n      no\n      0\n      299.4\n      71\n      50.90\n      61.9\n      88\n      5.26\n      196.9\n      89\n      8.86\n      6.6\n      7\n      1.78\n      2\n      False\n    \n    \n      4\n      OK\n      75\n      415\n      330-6626\n      yes\n      no\n      0\n      166.7\n      113\n      28.34\n      148.3\n      122\n      12.61\n      186.9\n      121\n      8.41\n      10.1\n      3\n      2.73\n      3\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3328\n      AZ\n      192\n      415\n      414-4276\n      no\n      yes\n      36\n      156.2\n      77\n      26.55\n      215.5\n      126\n      18.32\n      279.1\n      83\n      12.56\n      9.9\n      6\n      2.67\n      2\n      False\n    \n    \n      3329\n      WV\n      68\n      415\n      370-3271\n      no\n      no\n      0\n      231.1\n      57\n      39.29\n      153.4\n      55\n      13.04\n      191.3\n      123\n      8.61\n      9.6\n      4\n      2.59\n      3\n      False\n    \n    \n      3330\n      RI\n      28\n      510\n      328-8230\n      no\n      no\n      0\n      180.8\n      109\n      30.74\n      288.8\n      58\n      24.55\n      191.9\n      91\n      8.64\n      14.1\n      6\n      3.81\n      2\n      False\n    \n    \n      3331\n      CT\n      184\n      510\n      364-6381\n      yes\n      no\n      0\n      213.8\n      105\n      36.35\n      159.6\n      84\n      13.57\n      139.2\n      137\n      6.26\n      5.0\n      10\n      1.35\n      2\n      False\n    \n    \n      3332\n      TN\n      74\n      415\n      400-4344\n      no\n      yes\n      25\n      234.4\n      113\n      39.85\n      265.9\n      82\n      22.60\n      241.4\n      77\n      10.86\n      13.7\n      4\n      3.70\n      0\n      False\n    \n  \n\n3333 rows × 21 columns\n\n\n\n\nprofile = ProfileReport(telecom_churn, title = \"Telecom Churn Report\")\n\n\nprofile.to_notebook_iframe()"
  },
  {
    "objectID": "cdr_methods/cdr_features.html",
    "href": "cdr_methods/cdr_features.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "active_days\nnumber_of_contacts\nnumber_of_interactions\ncall_duration\npercent_nocturnal\npercent_initiated_conversations\nresponse_delay_text\nresponse_rate_text\nentropy_of_contacts\nInteractions_per_contact\npercent_pareto_interactions (percentage of user’s contact that account for 80% of its interactions)\npercent_pareto_durations\n\n\n\n\n\nNumber of unique places (antennas) visited\nEntropy of antennas\npercent at home\nradius of gyration (the equivalent distance of the mass from the center of gravity, for all visited places)\nfrequent_antennas - location that accounts for 80% of the locations the user was\nchurn_rate - Computes the frequency spent at every towers each week, and returns the distribution of the cosine similarity between two consecutives week\n\n\n\n\n\nDirected, weighted matrix for call, text etc\nDirected, Unweighted matrix\nUndirected, weighted matrix\nUndirected, Unweighted matrix\nClustering coefficient - Measure of the degree to which nodes in a graph tend to cluster together\nclustering coefficient unweighted of users weighted undirected network\nclustering coefficient weighted (undirected)\nassortativity of indicators(The extent to which nodes of a graphlink to others of the same degree)\nassortativity of attributes\n\n\n\n\n\nRecharge amounts\nTime between recharges\npercent pareto recharges\nNumber of recharges\nAverage daily balance estimated from all recharges\n\n\n\n\n\n\nMobile data will not be uniform across different networks. A different model may be required for different network."
  },
  {
    "objectID": "cdr_methods/resources.html",
    "href": "cdr_methods/resources.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Resources for analyzing CDR\n\nPython packages\nBandicoot open source library by MIT\nCellyzer\n\n\nBlogpost\nCDR data analysis using Neo4j"
  },
  {
    "objectID": "cdr_methods/phone_metadata.html",
    "href": "cdr_methods/phone_metadata.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Predicting poverty and wealth from Mobile phone metadata\n\nResearch paper by prof. Joshua Blumenstock\n\nMobile phone use reflects the structure of individual’s social network, patterns of travel and location choice and histories of consumption and expenditure\nSurvey on asset ownership, housing characteristics and other welfare indicators.\nConstructed a composite wealth index\nMobile phone data is used to predict the wealth index calculated from survey data\nFeatures constructed are:-\n\nTotal volume\nIntensity\nTiming\nDirection of communication etc\nStructure of the individual’s contact network\nPatterns of mobility based on geospatial markers\n\nElastic Net regularization was used in modelling\nGeospatial markers in the phone data enabled to study the geographic distribution of subscriber of wealth at an extremely fine degree of spatial granularity\nThere was a strong correlation between the mobile metadata predictions and the DHS survey data at district and village levels. Correlations persisted even for comparing clusters within urban and rural areas\nThis approach can be used to predict other metrics as well. Rates of district electrification estimated from phone records are comparable to those reported in the DHS survey\n\n\n\n\nPredicted Vs Actual wealth of Mobile users\n\n\n\n\n\nWealth Prediction for Rawanda"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Poverty Eradication Using Machine Learning",
    "section": "",
    "text": "A Study undertook to predict poverty using Data science and Machine learning"
  },
  {
    "objectID": "misc/datasets.html",
    "href": "misc/datasets.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Satellites\n\nLandsat\nDigitalGlobe\nSentinel\nMODIS\n\n\n\nWeb services\n\nplanet (website)\nGoogle Earth static map API"
  },
  {
    "objectID": "misc/challenges.html",
    "href": "misc/challenges.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "* Model Interpretability * Privacy of the mobile users * Fairness of the algorithms *"
  },
  {
    "objectID": "satellite_image_classification/train.html",
    "href": "satellite_image_classification/train.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Satellite Images Classification\n\nImport the required libraries\n\nimport torch\nimport argparse\nimport torch.nn as nn\nimport torch.optim as optim\nimport argparse\nimport cv2\nfrom matplotlib import pyplot as plt\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom model import build_model\nfrom utils import save_model, save_plots\nfrom datasets import train_loader, valid_loader, dataset\nfrom tqdm.notebook import tqdm\n\nClasses: ['cloudy', 'desert', 'green_area', 'water']\nTotal number of images: 5631\nTotal training images: 4505\nTotal valid_images: 1126\n\n\n\n\nLoad the weights for Reset Model\n\nlr = 0.001\nepochs = 20\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"computation device: {device}\\n\")\n\ncomputation device: cuda\n\n\n\n\nmodel = build_model(\n    pretrained=True, fine_tune=False, num_classes=len(dataset.classes)).to(device)\n   \n# total parameters and trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\n\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\\n\")\n\n[INFO]: Loading pre-trained weights\n[INFO]: Freezing hidden layers...\n21,286,724 total parameters.\n2,052 training parameters.\n\n\n\n\n# optimizer\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# loss function\ncriterion = nn.CrossEntropyLoss()\n\n\n\nTraining and Validation Functions\n\ndef train(model, trainloader, optimizer, criterion):\n    model.train()\n    print('Training')\n    train_running_loss = 0.0\n    train_running_correct = 0\n    counter = 0\n    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n        counter += 1\n        image, labels = data\n        image = image.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        # forward pass\n        outputs = model(image)\n        # calculate the loss\n        loss = criterion(outputs, labels)\n        train_running_loss += loss.item()\n        # calculate the accuracy\n        _, preds = torch.max(outputs.data, 1)\n        train_running_correct += (preds == labels).sum().item()\n        # backpropagation\n        loss.backward()\n        # update the optimizer parameters\n        optimizer.step()\n    \n    # loss and accuracy for the complete epoch\n    epoch_loss = train_running_loss / counter\n    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n    return epoch_loss, epoch_acc\n\n\ndef validate(model, testloader, criterion, class_names):\n    model.eval()\n    print('Validation')\n    valid_running_loss = 0.0\n    valid_running_correct = 0\n    counter = 0\n    \n    # we need two lists to keep track of class-wise accuracy\n    class_correct = list(0. for i in range(len(class_names)))\n    class_total = list(0. for i in range(len(class_names)))\n    \n    with torch.no_grad():\n        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n            counter += 1\n            \n            image, labels = data\n            image = image.to(device)\n            labels = labels.to(device)\n            # forward pass\n            outputs = model(image)\n            # calculate the loss\n            loss = criterion(outputs, labels)\n            valid_running_loss += loss.item()\n            # calculate the accuracy\n            _, preds = torch.max(outputs.data, 1)\n            valid_running_correct += (preds == labels).sum().item()\n            \n            # calculate the accuracy for each class\n            correct  = (preds == labels).squeeze()\n            for i in range(len(preds)):\n                label = labels[i]\n                class_correct[label] += correct[i].item()\n                class_total[label] += 1\n        \n    # loss and accuracy for the complete epoch\n    epoch_loss = valid_running_loss / counter\n    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n    \n    # print the accuracy for each class after every epoch\n    print('\\n')\n    for i in range(len(class_names)):\n        print(f\"Accuracy of class {class_names[i]}: {100*class_correct[i]/class_total[i]}\")\n    print('\\n')\n        \n    return epoch_loss, epoch_acc\n\n\n\nTrain for 20 Epochs\n\n# lists to keep track of losses and accuracies\ntrain_loss, valid_loss = [], []\ntrain_acc, valid_acc = [], []\n# start the training\nfor epoch in range(epochs):\n    #print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n    train_epoch_loss, train_epoch_acc = train(model, train_loader, \n                                              optimizer, criterion)\n    valid_epoch_loss, valid_epoch_acc = validate(model, valid_loader,  \n                                                 criterion, dataset.classes)\n    train_loss.append(train_epoch_loss)\n    valid_loss.append(valid_epoch_loss)\n    train_acc.append(train_epoch_acc)\n    valid_acc.append(valid_epoch_acc)\n    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n    print('-'*50)\n# save the trained model weights\nsave_model(epochs, model, optimizer, criterion)\n# save the loss and accuracy plots\nsave_plots(train_acc, valid_acc, train_loss, valid_loss)\nprint('TRAINING COMPLETE')\n\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 95.57522123893806\nAccuracy of class green_area: 96.53979238754326\nAccuracy of class water: 93.8566552901024\n\n\nTraining loss: 0.057, training acc: 98.180\nValidation loss: 0.142, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.37106918238993\nAccuracy of class desert: 92.92035398230088\nAccuracy of class green_area: 97.57785467128028\nAccuracy of class water: 83.2764505119454\n\n\nTraining loss: 0.049, training acc: 98.557\nValidation loss: 0.208, validation acc: 93.428\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 96.88581314878893\nAccuracy of class water: 84.98293515358361\n\n\nTraining loss: 0.048, training acc: 98.424\nValidation loss: 0.189, validation acc: 94.405\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.23008849557522\nAccuracy of class green_area: 95.50173010380622\nAccuracy of class water: 91.46757679180887\n\n\nTraining loss: 0.047, training acc: 98.690\nValidation loss: 0.162, validation acc: 96.004\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.74213836477988\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 89.96539792387543\nAccuracy of class water: 96.24573378839591\n\n\nTraining loss: 0.060, training acc: 98.091\nValidation loss: 0.161, validation acc: 95.648\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.11320754716981\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 96.88581314878893\nAccuracy of class water: 88.39590443686006\n\n\nTraining loss: 0.049, training acc: 98.313\nValidation loss: 0.159, validation acc: 95.382\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.34513274336283\nAccuracy of class green_area: 97.92387543252595\nAccuracy of class water: 91.12627986348123\n\n\nTraining loss: 0.049, training acc: 98.402\nValidation loss: 0.143, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 93.77162629757785\nAccuracy of class water: 93.8566552901024\n\n\nTraining loss: 0.040, training acc: 98.912\nValidation loss: 0.161, validation acc: 96.092\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 92.47787610619469\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 91.80887372013652\n\n\nTraining loss: 0.039, training acc: 98.690\nValidation loss: 0.163, validation acc: 95.115\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 89.419795221843\n\n\nTraining loss: 0.044, training acc: 98.468\nValidation loss: 0.162, validation acc: 95.382\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 97.48427672955975\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 87.5432525951557\nAccuracy of class water: 93.51535836177474\n\n\nTraining loss: 0.043, training acc: 98.513\nValidation loss: 0.178, validation acc: 94.139\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.42767295597484\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 88.23529411764706\nAccuracy of class water: 96.24573378839591\n\n\nTraining loss: 0.041, training acc: 98.602\nValidation loss: 0.162, validation acc: 95.293\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.23008849557522\nAccuracy of class green_area: 86.85121107266436\nAccuracy of class water: 92.83276450511946\n\n\nTraining loss: 0.044, training acc: 98.513\nValidation loss: 0.183, validation acc: 94.139\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.34513274336283\nAccuracy of class green_area: 94.80968858131487\nAccuracy of class water: 92.15017064846417\n\n\nTraining loss: 0.042, training acc: 98.668\nValidation loss: 0.157, validation acc: 95.826\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 92.38754325259515\nAccuracy of class water: 95.56313993174061\n\n\nTraining loss: 0.044, training acc: 98.446\nValidation loss: 0.143, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 96.53979238754326\nAccuracy of class water: 93.51535836177474\n\n\nTraining loss: 0.039, training acc: 98.468\nValidation loss: 0.125, validation acc: 96.714\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 95.15570934256056\nAccuracy of class water: 92.83276450511946\n\n\nTraining loss: 0.038, training acc: 98.713\nValidation loss: 0.149, validation acc: 96.004\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.74213836477988\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 89.27335640138408\nAccuracy of class water: 94.53924914675768\n\n\nTraining loss: 0.041, training acc: 98.713\nValidation loss: 0.154, validation acc: 95.204\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.37106918238993\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 88.73720136518772\n\n\nTraining loss: 0.040, training acc: 98.602\nValidation loss: 0.163, validation acc: 95.471\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 97.2318339100346\nAccuracy of class water: 91.46757679180887\n\n\nTraining loss: 0.042, training acc: 98.557\nValidation loss: 0.133, validation acc: 96.359\n--------------------------------------------------\nTRAINING COMPLETE\n\n\n\n\n\n\n\n\n\n\nInference\n\nimport torch\nimport cv2\nimport torchvision.transforms as transforms\nfrom model import build_model\n\n\ndevice = 'cpu'\n\n\n# list containing all the labels\nlabels = ['cloudy', 'desert', 'green_area', 'water']\n# initialize the model and load the trained weights\nmodel = build_model(\n    pretrained=False, fine_tune=False, num_classes=4\n).to(device)\n\nprint('[INFO]: Loading custom-trained weights...')\ncheckpoint = torch.load('outputs/model.pth', map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# define preprocess transforms\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n]) \n\n[INFO]: Not loading pre-trained weights\n[INFO]: Freezing hidden layers...\n[INFO]: Loading custom-trained weights...\n\n\n\ndef inference(input):\n# read and preprocess the image\n    image = cv2.imread(input)\n    # get the ground truth class\n    gt_class = input.split('/')[-1].split('.')[0]\n    orig_image = image.copy()\n    # convert to RGB format\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = transform(image)\n    # add batch dimension\n    image = torch.unsqueeze(image, 0)\n    with torch.no_grad():\n        outputs = model(image.to(device))\n    output_label = torch.topk(outputs, 1)\n    pred_class = labels[int(output_label.indices)]\n    cv2.putText(orig_image, \n        f\"GT: {gt_class}\",\n        (10, 25),\n        cv2.FONT_HERSHEY_SIMPLEX, \n        1, (0, 255, 0), 2, cv2.LINE_AA\n    )\n    cv2.putText(orig_image, \n        f\"Pred: {pred_class}\",\n        (10, 55),\n        cv2.FONT_HERSHEY_SIMPLEX, \n        1, (0, 0, 255), 2, cv2.LINE_AA\n    )\n    print(f\"GT: {gt_class}, pred: {pred_class}\")\n    #image = cv2.imshow('Result', orig_image)\n    rgb_image = cv2.cvtColor(orig_image,cv2.COLOR_BGR2RGB)\n    fig = plt.figure()\n    plt.axis('off')\n    plt.grid(b=None)\n    plt.imshow(rgb_image)\n    cv2.imwrite(f\"outputs/{gt_class}.png\",\n        orig_image)\n\n\ninference(input='input/test_data/cloudy.jpeg')\n\nGT: cloudy, pred: cloudy\n\n\n\n\n\n\ninference(input='input/test_data/desert.jpeg')\n\nGT: desert, pred: desert"
  },
  {
    "objectID": "deeplearning_implementations/satellite_images_classification.html",
    "href": "deeplearning_implementations/satellite_images_classification.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Satellite Images Classification\n\nImport the required libraries\n\nimport torch\nimport argparse\nimport torch.nn as nn\nimport torch.optim as optim\nimport argparse\nimport cv2\nfrom matplotlib import pyplot as plt\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\nfrom model import build_model\nfrom utils import save_model, save_plots\nfrom datasets import train_loader, valid_loader, dataset\nfrom tqdm.notebook import tqdm\n\nClasses: ['cloudy', 'desert', 'green_area', 'water']\nTotal number of images: 5631\nTotal training images: 4505\nTotal valid_images: 1126\n\n\n\n\nLoad the weights for Reset Model\n\nlr = 0.001\nepochs = 20\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"computation device: {device}\\n\")\n\ncomputation device: cuda\n\n\n\n\nmodel = build_model(\n    pretrained=True, fine_tune=False, num_classes=len(dataset.classes)).to(device)\n   \n# total parameters and trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\n\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\\n\")\n\n[INFO]: Loading pre-trained weights\n[INFO]: Freezing hidden layers...\n21,286,724 total parameters.\n2,052 training parameters.\n\n\n\n\n# optimizer\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# loss function\ncriterion = nn.CrossEntropyLoss()\n\n\n\nTraining and Validation Functions\n\ndef train(model, trainloader, optimizer, criterion):\n    model.train()\n    print('Training')\n    train_running_loss = 0.0\n    train_running_correct = 0\n    counter = 0\n    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n        counter += 1\n        image, labels = data\n        image = image.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        # forward pass\n        outputs = model(image)\n        # calculate the loss\n        loss = criterion(outputs, labels)\n        train_running_loss += loss.item()\n        # calculate the accuracy\n        _, preds = torch.max(outputs.data, 1)\n        train_running_correct += (preds == labels).sum().item()\n        # backpropagation\n        loss.backward()\n        # update the optimizer parameters\n        optimizer.step()\n    \n    # loss and accuracy for the complete epoch\n    epoch_loss = train_running_loss / counter\n    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n    return epoch_loss, epoch_acc\n\n\ndef validate(model, testloader, criterion, class_names):\n    model.eval()\n    print('Validation')\n    valid_running_loss = 0.0\n    valid_running_correct = 0\n    counter = 0\n    \n    # we need two lists to keep track of class-wise accuracy\n    class_correct = list(0. for i in range(len(class_names)))\n    class_total = list(0. for i in range(len(class_names)))\n    \n    with torch.no_grad():\n        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n            counter += 1\n            \n            image, labels = data\n            image = image.to(device)\n            labels = labels.to(device)\n            # forward pass\n            outputs = model(image)\n            # calculate the loss\n            loss = criterion(outputs, labels)\n            valid_running_loss += loss.item()\n            # calculate the accuracy\n            _, preds = torch.max(outputs.data, 1)\n            valid_running_correct += (preds == labels).sum().item()\n            \n            # calculate the accuracy for each class\n            correct  = (preds == labels).squeeze()\n            for i in range(len(preds)):\n                label = labels[i]\n                class_correct[label] += correct[i].item()\n                class_total[label] += 1\n        \n    # loss and accuracy for the complete epoch\n    epoch_loss = valid_running_loss / counter\n    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n    \n    # print the accuracy for each class after every epoch\n    print('\\n')\n    for i in range(len(class_names)):\n        print(f\"Accuracy of class {class_names[i]}: {100*class_correct[i]/class_total[i]}\")\n    print('\\n')\n        \n    return epoch_loss, epoch_acc\n\n\n\nTrain for 20 Epochs\n\n# lists to keep track of losses and accuracies\ntrain_loss, valid_loss = [], []\ntrain_acc, valid_acc = [], []\n# start the training\nfor epoch in range(epochs):\n    #print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n    train_epoch_loss, train_epoch_acc = train(model, train_loader, \n                                              optimizer, criterion)\n    valid_epoch_loss, valid_epoch_acc = validate(model, valid_loader,  \n                                                 criterion, dataset.classes)\n    train_loss.append(train_epoch_loss)\n    valid_loss.append(valid_epoch_loss)\n    train_acc.append(train_epoch_acc)\n    valid_acc.append(valid_epoch_acc)\n    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n    print('-'*50)\n# save the trained model weights\nsave_model(epochs, model, optimizer, criterion)\n# save the loss and accuracy plots\nsave_plots(train_acc, valid_acc, train_loss, valid_loss)\nprint('TRAINING COMPLETE')\n\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 95.57522123893806\nAccuracy of class green_area: 96.53979238754326\nAccuracy of class water: 93.8566552901024\n\n\nTraining loss: 0.057, training acc: 98.180\nValidation loss: 0.142, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.37106918238993\nAccuracy of class desert: 92.92035398230088\nAccuracy of class green_area: 97.57785467128028\nAccuracy of class water: 83.2764505119454\n\n\nTraining loss: 0.049, training acc: 98.557\nValidation loss: 0.208, validation acc: 93.428\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 96.88581314878893\nAccuracy of class water: 84.98293515358361\n\n\nTraining loss: 0.048, training acc: 98.424\nValidation loss: 0.189, validation acc: 94.405\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.23008849557522\nAccuracy of class green_area: 95.50173010380622\nAccuracy of class water: 91.46757679180887\n\n\nTraining loss: 0.047, training acc: 98.690\nValidation loss: 0.162, validation acc: 96.004\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.74213836477988\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 89.96539792387543\nAccuracy of class water: 96.24573378839591\n\n\nTraining loss: 0.060, training acc: 98.091\nValidation loss: 0.161, validation acc: 95.648\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.11320754716981\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 96.88581314878893\nAccuracy of class water: 88.39590443686006\n\n\nTraining loss: 0.049, training acc: 98.313\nValidation loss: 0.159, validation acc: 95.382\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.34513274336283\nAccuracy of class green_area: 97.92387543252595\nAccuracy of class water: 91.12627986348123\n\n\nTraining loss: 0.049, training acc: 98.402\nValidation loss: 0.143, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 93.77162629757785\nAccuracy of class water: 93.8566552901024\n\n\nTraining loss: 0.040, training acc: 98.912\nValidation loss: 0.161, validation acc: 96.092\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 92.47787610619469\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 91.80887372013652\n\n\nTraining loss: 0.039, training acc: 98.690\nValidation loss: 0.163, validation acc: 95.115\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 89.419795221843\n\n\nTraining loss: 0.044, training acc: 98.468\nValidation loss: 0.162, validation acc: 95.382\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 97.48427672955975\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 87.5432525951557\nAccuracy of class water: 93.51535836177474\n\n\nTraining loss: 0.043, training acc: 98.513\nValidation loss: 0.178, validation acc: 94.139\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.42767295597484\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 88.23529411764706\nAccuracy of class water: 96.24573378839591\n\n\nTraining loss: 0.041, training acc: 98.602\nValidation loss: 0.162, validation acc: 95.293\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.23008849557522\nAccuracy of class green_area: 86.85121107266436\nAccuracy of class water: 92.83276450511946\n\n\nTraining loss: 0.044, training acc: 98.513\nValidation loss: 0.183, validation acc: 94.139\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.34513274336283\nAccuracy of class green_area: 94.80968858131487\nAccuracy of class water: 92.15017064846417\n\n\nTraining loss: 0.042, training acc: 98.668\nValidation loss: 0.157, validation acc: 95.826\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 92.38754325259515\nAccuracy of class water: 95.56313993174061\n\n\nTraining loss: 0.044, training acc: 98.446\nValidation loss: 0.143, validation acc: 96.359\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 96.53979238754326\nAccuracy of class water: 93.51535836177474\n\n\nTraining loss: 0.039, training acc: 98.468\nValidation loss: 0.125, validation acc: 96.714\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 96.90265486725664\nAccuracy of class green_area: 95.15570934256056\nAccuracy of class water: 92.83276450511946\n\n\nTraining loss: 0.038, training acc: 98.713\nValidation loss: 0.149, validation acc: 96.004\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 98.74213836477988\nAccuracy of class desert: 98.67256637168141\nAccuracy of class green_area: 89.27335640138408\nAccuracy of class water: 94.53924914675768\n\n\nTraining loss: 0.041, training acc: 98.713\nValidation loss: 0.154, validation acc: 95.204\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.37106918238993\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 96.19377162629758\nAccuracy of class water: 88.73720136518772\n\n\nTraining loss: 0.040, training acc: 98.602\nValidation loss: 0.163, validation acc: 95.471\n--------------------------------------------------\nTraining\n\n\n\n\n\nValidation\n\n\n\n\n\n\n\nAccuracy of class cloudy: 99.05660377358491\nAccuracy of class desert: 97.78761061946902\nAccuracy of class green_area: 97.2318339100346\nAccuracy of class water: 91.46757679180887\n\n\nTraining loss: 0.042, training acc: 98.557\nValidation loss: 0.133, validation acc: 96.359\n--------------------------------------------------\nTRAINING COMPLETE\n\n\n\n\n\n\n\n\n\n\nInference\n\nimport torch\nimport cv2\nimport torchvision.transforms as transforms\nfrom model import build_model\n\n\ndevice = 'cpu'\n\n\n# list containing all the labels\nlabels = ['cloudy', 'desert', 'green_area', 'water']\n# initialize the model and load the trained weights\nmodel = build_model(\n    pretrained=False, fine_tune=False, num_classes=4\n).to(device)\n\nprint('[INFO]: Loading custom-trained weights...')\ncheckpoint = torch.load('outputs/model.pth', map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# define preprocess transforms\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n]) \n\n[INFO]: Not loading pre-trained weights\n[INFO]: Freezing hidden layers...\n[INFO]: Loading custom-trained weights...\n\n\n\ndef inference(input):\n# read and preprocess the image\n    image = cv2.imread(input)\n    # get the ground truth class\n    gt_class = input.split('/')[-1].split('.')[0]\n    orig_image = image.copy()\n    # convert to RGB format\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = transform(image)\n    # add batch dimension\n    image = torch.unsqueeze(image, 0)\n    with torch.no_grad():\n        outputs = model(image.to(device))\n    output_label = torch.topk(outputs, 1)\n    pred_class = labels[int(output_label.indices)]\n    cv2.putText(orig_image, \n        f\"GT: {gt_class}\",\n        (10, 25),\n        cv2.FONT_HERSHEY_SIMPLEX, \n        1, (0, 255, 0), 2, cv2.LINE_AA\n    )\n    cv2.putText(orig_image, \n        f\"Pred: {pred_class}\",\n        (10, 55),\n        cv2.FONT_HERSHEY_SIMPLEX, \n        1, (0, 0, 255), 2, cv2.LINE_AA\n    )\n    print(f\"GT: {gt_class}, pred: {pred_class}\")\n    #image = cv2.imshow('Result', orig_image)\n    rgb_image = cv2.cvtColor(orig_image,cv2.COLOR_BGR2RGB)\n    fig = plt.figure()\n    plt.axis('off')\n    plt.grid(b=None)\n    plt.imshow(rgb_image)\n    cv2.imwrite(f\"outputs/{gt_class}.png\",\n        orig_image)\n\n\ninference(input='input/test_data/cloudy.jpeg')\n\nGT: cloudy, pred: cloudy\n\n\n\n\n\n\ninference(input='input/test_data/desert.jpeg')\n\nGT: desert, pred: desert"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A Study on predicting poverty using Data Science and Machine learning"
  },
  {
    "objectID": "how_to_measure_poverty/approches.html",
    "href": "how_to_measure_poverty/approches.html",
    "title": "Poverty Eradication Using ML",
    "section": "",
    "text": "Approaches to measuring poverty - Notes from the book\n\n\nAccording to the Book Measuring Poverty Around the World by Prof. Anthony B Atkinson, the following are different ways to measure poverty are - Basic Needs based approach, Capabilities based approach, Rights based approach and Subjective and perception based approach\n\n\n\nThis approach is based on either consumption or Income.\nIn this approach there are three steps involved. They are\n\nEstimating the nutritional requirements\nConverting the nutritional requirements into food budget\nMaking an allowance for nonfood items\n\nAbove three steps will decide the threshold for the poverty line\nThe drawback of this approach is that a lot of judegment is required at every step of the process making comparisions difficult across time and regions\n\n\n\n\n\nIndividual well-being should be judged in terms of the functionings achieved by the individual and of the capabilities open to them\nIn terms of measuring poverty this should be seen as a deprivation of capabilities, where the deprivation limits the capabilties of the individual to purse their goals in life.\nPoverty is measured by asking the individual about his/her current income and his unmet aspirations like taking a vacation, purchasing a bike etc. There is no clarity on what threshold to select for the poverty in this case.\nTheir is judgement involved in this approach as well\n\n\n\n\nCapability Approach to Absolute and Relative poverty\n\n\n\n\n\n\nEqual rights are provided to all individuals in this approach. In Basic needs approach the poverty line for men and women was different due to differences in estimation for nutritional needs. In Rights approach all humans are considered equal. Along with the food needs Rights approach also considers health, education, and other dimensions as well as a basic human right\n\n\n\n\n\nIn subjective approach each individual classifies himself if he is poor or not. There was lot of criticism that institutions measuring poverty is not considering the views of the people who are suffering from poverty. This method adds considerable value by recording the view of the individual. The drawback of this method is that there are multiple poverty values existing in parallel and this method is subjective and requires a objective measure to make it more actionable.\n\n\n\n\n\nIncome and expenditure are commonly used as metrics to be compared with a determined poverty threshold\nSurveys on living standards and household income and expenditure are generally the sources of data used for deriving expenditure and income\nIn most developed countries, the national poverty line is based on relative standards. This relative poverty line considers the median income of an individual or family to maintain an average living standard as a point of comparison to those who might be considered poor.\nThe World Bank has international poverty lines including $3.20 per day, which is based on 2011 purchasing power parity. However, many countries adopt the cost of basic needs approach in measuring absolute poverty. This estimates severe deprivation of basic human needs such as food, safe drinking water, sanitation facilities, health, shelter, education, and information (UN 1995). The approach determines a food basket that meets the minimum nutritional requirements set by the World Health Organization and Food and Agricultural Organization of the United Nations.\nIncorporating “equivalence scale adjustments” is another common practice in poverty estimation. An equivalence scale indicates that households with the same income or expenditure do not necessarily have the same economic capacity, since this capacity will depend on the number of dependent members in the household. Economic status is therefore usually determined by dividing the household income or expenditure by the family or household size,then determining whether the resulting value is above or below the poverty line. Some NSOs also assign index weights based on the age of the family members to estimate poverty"
  }
]